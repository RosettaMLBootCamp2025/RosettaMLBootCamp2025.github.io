{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3D83f_eeERMz"
   },
   "source": [
    "We're going to illustrate some practical differences between CPU and GPU operations using Python and PyTorch. First, we'll import some modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "NW4PE1ZjEQ0H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1SCD6zoEmp-"
   },
   "source": [
    "Next, we'll make sure that we have a GPU. If you don't have one, then click \"Runtime\" at the top of the notebook and then \"Change runtime type\". Select the \"T4 GPU\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bhStX9ocE4DL",
    "outputId": "2c1d3471-e6b4-482d-bcc2-d7849ae7bd5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_best_device():\n",
    "    # 1. CUDA (NVIDIA)\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "\n",
    "    # 2. MPS (Apple Silicon / Apple GPU)\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "\n",
    "    # 3. ROCm (AMD GPUs on Linux)\n",
    "    if torch.version.hip is not None and torch.cuda.is_available():\n",
    "        # ROCm exposes itself via torch.cuda\n",
    "        return torch.device(\"cuda\")\n",
    "\n",
    "    # 4. CPU fallback\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFQgou1ZFN2T"
   },
   "source": [
    "Now, we're going to create a function that we'll use for timing our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "hYm3zUv6FSg2"
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_and_time(func, *args, **kwargs):\n",
    "    times = {}\n",
    "\n",
    "    # ---- CPU ----\n",
    "    device = torch.device(\"cpu\")\n",
    "    start = time.time()\n",
    "    _ = func(*args, **kwargs, device=device)\n",
    "    times[\"cpu\"] = time.time() - start\n",
    "\n",
    "    # ---- CUDA GPU ----\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        torch.cuda.synchronize()               # important for accurate timing\n",
    "        start = time.time()\n",
    "        _ = func(*args, **kwargs, device=device)\n",
    "        torch.cuda.synchronize()\n",
    "        times[\"gpu\"] = time.time() - start\n",
    "        times[\"gpu_type\"] = \"cuda\"\n",
    "        return times\n",
    "\n",
    "    # ---- MPS (Apple Metal) ----\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        torch.mps.synchronize()\n",
    "        start = time.time()\n",
    "        _ = func(*args, **kwargs, device=device)\n",
    "        torch.mps.synchronize()\n",
    "        times[\"gpu\"] = time.time() - start\n",
    "        times[\"gpu_type\"] = \"mps\"\n",
    "        return times\n",
    "\n",
    "    # ---- No GPU ----\n",
    "    times[\"gpu\"] = None\n",
    "    times[\"gpu_type\"] = None\n",
    "    return times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnNo_--YGFxZ"
   },
   "source": [
    "We'll be performing dot products on vectors of various sizes. Specifically, we'll be computing the dot product between two vectors `A` and `B` both with shape `Nx1`. `N` will be one of `[10, 50, 100, 500, 1000, 2000, 5000]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "IPTT1uYZGsOi"
   },
   "outputs": [],
   "source": [
    "# Varying data sizes\n",
    "data_sizes = [10, 10, 50, 100, 500, 1000, 2000, 5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3wZ0UaVGw7M"
   },
   "source": [
    "Let's create three functions that'll both perform the dot product. The first of which will naively use `for` loops to perform the computation; the second will use \"vectorization\" to accelerate the computation. These functions will use another function to create random tensors for the computation. We'll also be comparing the times for our custom functions to the built-in `torch.dot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "C-0ZbhLiHIQj"
   },
   "outputs": [],
   "source": [
    "def get_random_tensor(size, device):\n",
    "    return torch.randn(size, device=device)\n",
    "\n",
    "\n",
    "def dot_product_for_loop(size, device):\n",
    "    \"\"\"\n",
    "    Calculates the dot product of two PyTorch tensors using a for loop.\n",
    "\n",
    "    Args:\n",
    "        size: The size of the tensors.\n",
    "        device: The device (CPU or GPU) to perform the computation on.\n",
    "\n",
    "    Returns:\n",
    "        The dot product (float).\n",
    "    \"\"\"\n",
    "    # Create random tensors\n",
    "    a = get_random_tensor(size, device)\n",
    "    b = get_random_tensor(size, device)\n",
    "\n",
    "    # Initialize dot product\n",
    "    dot_product = 0\n",
    "\n",
    "    # Iterate through elements and accumulate dot product\n",
    "    for i in range(a.shape[0]):\n",
    "        dot_product += a[i] * b[i]\n",
    "\n",
    "    return dot_product\n",
    "\n",
    "\n",
    "def dot_product_vectorized(size, device):\n",
    "    \"\"\"\n",
    "    Calculates the dot product of two PyTorch tensors using vectorization.\n",
    "\n",
    "    Args:\n",
    "        size: The size of the tensors.\n",
    "        device: The device (CPU or GPU) to perform the computation on.\n",
    "\n",
    "    Returns:\n",
    "        The dot product (float).\n",
    "    \"\"\"\n",
    "    # Create random tensors\n",
    "    a = get_random_tensor(size, device)\n",
    "    b = get_random_tensor(size, device)\n",
    "\n",
    "    # Calculate dot product using element-wise multiplication and sum\n",
    "    dot_product = (a * b).sum()\n",
    "\n",
    "    return dot_product\n",
    "\n",
    "\n",
    "def dot_product_torch(size, device):\n",
    "    \"\"\"\n",
    "    Calculates the dot product of two PyTorch tensors using torch.dot.\n",
    "\n",
    "    Args:\n",
    "        size: The size of the tensors.\n",
    "        device: The device (CPU or GPU) to perform the computation on.\n",
    "\n",
    "    Returns:\n",
    "        The dot product (float).\n",
    "    \"\"\"\n",
    "    # Create random tensors\n",
    "    a = get_random_tensor(size, device)\n",
    "    b = get_random_tensor(size, device)\n",
    "\n",
    "    # Calculate dot product using torch.dot\n",
    "    dot_product = torch.dot(a, b)\n",
    "\n",
    "    return dot_product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbWafqLGJScQ"
   },
   "source": [
    "For each of our tensor sizes, let's use the functions we just defined to compute the dot product and time them on the CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PCtHQdHJRhO"
   },
   "outputs": [],
   "source": "# Perform calculations and measure execution time for CPU and GPU\ncpu_times_for_loop = []\ngpu_times_for_loop = []\ncpu_times_vectorized = []\ngpu_times_vectorized = []\ncpu_times_torch_dot = []\ngpu_times_torch_dot = []\n\nfor size in data_sizes:\n    times = calculate_and_time(dot_product_for_loop, size)\n    cpu_times_for_loop.append(times[\"cpu\"])\n    gpu_times_for_loop.append(times[\"gpu\"])\n\n    times = calculate_and_time(dot_product_vectorized, size)\n    cpu_times_vectorized.append(times[\"cpu\"])\n    gpu_times_vectorized.append(times[\"gpu\"])\n\n    times = calculate_and_time(dot_product_torch, size)\n    cpu_times_torch_dot.append(times[\"cpu\"])\n    gpu_times_torch_dot.append(times[\"gpu\"])"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RDUQjCPLckq"
   },
   "source": [
    "Now let's format and print the timing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WambBZIOLfqa",
    "outputId": "84f0ae4f-aad6-49b7-fd9a-921c50916712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Loop\n",
      "Data Size\tCPU Time (s)\tGPU Time (s)\n",
      "10\t\t0.001137\t\tN/A\n",
      "10\t\t0.000077\t\tN/A\n",
      "50\t\t0.000313\t\tN/A\n",
      "100\t\t0.000451\t\tN/A\n",
      "500\t\t0.002362\t\tN/A\n",
      "1000\t\t0.004707\t\tN/A\n",
      "2000\t\t0.006913\t\tN/A\n",
      "5000\t\t0.011325\t\tN/A\n",
      "\n",
      "Vectorized\n",
      "Data Size\tCPU Time (s)\tGPU Time (s)\n",
      "10\t\t0.000295\t\tN/A\n",
      "10\t\t0.000014\t\tN/A\n",
      "50\t\t0.000024\t\tN/A\n",
      "100\t\t0.000017\t\tN/A\n",
      "500\t\t0.000048\t\tN/A\n",
      "1000\t\t0.000088\t\tN/A\n",
      "2000\t\t0.000070\t\tN/A\n",
      "5000\t\t0.000108\t\tN/A\n",
      "\n",
      "Torch Dot\n",
      "Data Size\tCPU Time (s)\tGPU Time (s)\n",
      "10\t\t0.000032\t\tN/A\n",
      "10\t\t0.000009\t\tN/A\n",
      "50\t\t0.000013\t\tN/A\n",
      "100\t\t0.000011\t\tN/A\n",
      "500\t\t0.000028\t\tN/A\n",
      "1000\t\t0.000037\t\tN/A\n",
      "2000\t\t0.000067\t\tN/A\n",
      "5000\t\t0.000074\t\tN/A\n"
     ]
    }
   ],
   "source": [
    "# Print timing results\n",
    "print(\"For Loop\")\n",
    "print(\"Data Size\\tCPU Time (s)\\tGPU Time (s)\")\n",
    "for i, size in enumerate(data_sizes):\n",
    "    print(f\"{size}\\t\\t{cpu_times_for_loop[i]:.6f}\\t\\t{gpu_times_for_loop[i] if gpu_times_for_loop[i] is not None else 'N/A'}\")\n",
    "\n",
    "print(\"\\nVectorized\")\n",
    "print(\"Data Size\\tCPU Time (s)\\tGPU Time (s)\")\n",
    "for i, size in enumerate(data_sizes):\n",
    "    print(f\"{size}\\t\\t{cpu_times_vectorized[i]:.6f}\\t\\t{gpu_times_vectorized[i] if gpu_times_vectorized[i] is not None else 'N/A'}\")\n",
    "\n",
    "print(\"\\nTorch Dot\")\n",
    "print(\"Data Size\\tCPU Time (s)\\tGPU Time (s)\")\n",
    "for i, size in enumerate(data_sizes):\n",
    "    print(f\"{size}\\t\\t{cpu_times_torch_dot[i]:.6f}\\t\\t{gpu_times_torch_dot[i] if gpu_times_torch_dot[i] is not None else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7Cyac8RMJiX"
   },
   "source": [
    "#### For Loop\n",
    "**CPU Time:** As the data size (vector length) increases, the CPU time increases linearly. This is expected since the for loop iterates through each element of the vectors, and the number of operations grows proportionally with the vector size.\n",
    "\n",
    "**GPU Time:** The GPU times are actually higher than the CPU times for smaller sizes. There's an overhead associated with transferring data to and from the GPU. This overhead dominates for smaller data sizes, making it slower than the CPU in this case. However, as data size increases, the GPU leverages its parallel processing capabilities to outperform the CPU significantly, reducing computation time.\n",
    "\n",
    "#### Vectorized\n",
    "**CPU Time:** The CPU times remain relatively consistent across different data sizes. This highlights the efficiency of vectorization, as it leverages low-level, optimized operations that are less sensitive to vector size. The small increases in CPU time with larger data sizes may be due to the additional memory and computation required.\n",
    "\n",
    "**GPU Time:** Similar to the for loop case, the GPU times are higher than CPU times for smaller sizes. The overhead of data transfer is more significant than any parallel processing advantage for small tasks. However, as data size increases, we see more speed benefits from the parallel processing capabilities of the GPU. For the largest size, 5000, the GPU time is significantly lower than the CPU time and also lower than the GPU time from the for loop.\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "- Vectorization is essential for efficiency: Vectorized operations are significantly faster than for loops, especially when dealing with large datasets.\n",
    "- GPU vs. CPU: The overhead of data transfer to the GPU might mean slower performance for smaller tasks. But for large tasks, the advantages of parallel processing and optimized computations of the GPU become apparent.\n",
    "- PyTorch Considerations: It's likely that PyTorch has internal optimizations that reduce the overhead of transferring data between CPU and GPU, which explains some of the nuances observed in the timings. The built-in torch.dot function tends to be the fastest overall, particularly on the GPU. It is likely also optimized specifically for efficient dot product calculations.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}