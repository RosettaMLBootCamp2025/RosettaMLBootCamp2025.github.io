[
  {
    "objectID": "wednesday/index.html",
    "href": "wednesday/index.html",
    "title": "Wednesday",
    "section": "",
    "text": "Wednesday content coming soon."
  },
  {
    "objectID": "wednesday/index.html#overview",
    "href": "wednesday/index.html#overview",
    "title": "Wednesday",
    "section": "",
    "text": "Wednesday content coming soon."
  },
  {
    "objectID": "wednesday/index.html#modules",
    "href": "wednesday/index.html#modules",
    "title": "Wednesday",
    "section": "Modules",
    "text": "Modules\n\n\n\nModule\nTopic\nStatus\n\n\n\n\n3.1\nPlaceholder Module\nComing soon\n\n\n\nMore modules will be added here.\n\n\n\n← Tuesday\n\n\nBack to Home\n\n\nThursday →"
  },
  {
    "objectID": "thursday/index.html",
    "href": "thursday/index.html",
    "title": "Thursday",
    "section": "",
    "text": "Thursday content coming soon."
  },
  {
    "objectID": "thursday/index.html#overview",
    "href": "thursday/index.html#overview",
    "title": "Thursday",
    "section": "",
    "text": "Thursday content coming soon."
  },
  {
    "objectID": "thursday/index.html#modules",
    "href": "thursday/index.html#modules",
    "title": "Thursday",
    "section": "Modules",
    "text": "Modules\n\n\n\nModule\nTopic\nStatus\n\n\n\n\n4.1\nPlaceholder Module\nComing soon\n\n\n\nMore modules will be added here.\n\n\n\n← Wednesday\n\n\nBack to Home\n\n\nCapstone →"
  },
  {
    "objectID": "tuesday/index.html",
    "href": "tuesday/index.html",
    "title": "Tuesday",
    "section": "",
    "text": "Tuesday content coming soon."
  },
  {
    "objectID": "tuesday/index.html#overview",
    "href": "tuesday/index.html#overview",
    "title": "Tuesday",
    "section": "",
    "text": "Tuesday content coming soon."
  },
  {
    "objectID": "tuesday/index.html#modules",
    "href": "tuesday/index.html#modules",
    "title": "Tuesday",
    "section": "Modules",
    "text": "Modules\n\n\n\nModule\nTopic\nStatus\n\n\n\n\n2.1\nPlaceholder Module\nComing soon\n\n\n\nMore modules will be added here.\n\n\n\n← Monday\n\n\nBack to Home\n\n\nWednesday →"
  },
  {
    "objectID": "capstone/index.html",
    "href": "capstone/index.html",
    "title": "Capstone",
    "section": "",
    "text": "Capstone project details coming soon.\nThe capstone project brings together everything you’ve learned throughout the bootcamp."
  },
  {
    "objectID": "capstone/index.html#overview",
    "href": "capstone/index.html#overview",
    "title": "Capstone",
    "section": "",
    "text": "Capstone project details coming soon.\nThe capstone project brings together everything you’ve learned throughout the bootcamp."
  },
  {
    "objectID": "capstone/index.html#project-description",
    "href": "capstone/index.html#project-description",
    "title": "Capstone",
    "section": "Project Description",
    "text": "Project Description\nComing soon.\n\n\n\n← Thursday\n\n\nBack to Home"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "",
    "text": "Welcome back! You were last on:   Resume where you left off\nWelcome to the ML Protein Design Bootcamp 2025! This self-paced course covers machine learning tools for protein structure prediction and design."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "Course Overview",
    "text": "Course Overview\nThis bootcamp is organized into 4 days of content plus a capstone project. Each day contains sequential modules that build on each other.\n\n\nMonday\nComing soon\nIntroduction and setup modules.\n\n\nTuesday\nComing soon\nModules for Tuesday.\n\n\nWednesday\nComing soon\nModules for Wednesday.\n\n\nThursday\nComing soon\nModules for Thursday.\n\n\nCapstone\nComing soon\nCapstone project bringing together everything you’ve learned."
  },
  {
    "objectID": "index.html#your-progress",
    "href": "index.html#your-progress",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "Your Progress",
    "text": "Your Progress\n\n\nCompleted modules: 0\n\n\n&lt;div id=\"progress-bar\" class=\"progress-bar\" role=\"progressbar\" style=\"width: 0%;\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"&gt;0%&lt;/div&gt;\n\n\n\nReset Progress"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "Getting Started",
    "text": "Getting Started\n\nChoose a day from the navigation above or the cards below\nWork through modules in order - each builds on the previous\nCheck off sections as you complete them - your progress is saved automatically\nReturn anytime - use the “Resume” button to pick up where you left off\n\n\nReport an Issue / Ask a Question"
  },
  {
    "objectID": "monday/10-bindcraft.html",
    "href": "monday/10-bindcraft.html",
    "title": "10. BindCraft",
    "section": "",
    "text": "BindCraft (paper, code) is an end-to-end binder design pipeline that combines AlphaFold2 backpropagation, ProteinMPNN, and PyRosetta to design protein binders against target proteins.\nBindCraft is useful for de novo binder design, allowing you to select a target protein and automatically generate, optimize, and validate designed binders. It integrates multiple ML tools into a complete workflow, making it ideal for learning how protein design pipelines work in practice.\nThe goal of this module is to install BindCraft on your HPC cluster."
  },
  {
    "objectID": "monday/10-bindcraft.html#preparation-work",
    "href": "monday/10-bindcraft.html#preparation-work",
    "title": "10. BindCraft",
    "section": "Preparation Work",
    "text": "Preparation Work\n Mark as complete\nThis assignment assumes that you have access to a computing cluster that allows installation of new software.\nRequirements:\n\nCUDA-compatible Nvidia GPU (recommended: at least 32 GB GPU memory)\nStorage space: ~2 MB for code + ~5.3 GB for AlphaFold2 weights\nConda or Mamba package manager\n\nImportant: BindCraft requires a PyRosetta license for commercial use. Academic use is typically covered by standard PyRosetta licensing."
  },
  {
    "objectID": "monday/10-bindcraft.html#installation-steps",
    "href": "monday/10-bindcraft.html#installation-steps",
    "title": "10. BindCraft",
    "section": "Installation Steps",
    "text": "Installation Steps\n Mark as complete\n\nClone the BindCraft repository (replace [install_folder] with your desired path):\n\ngit clone https://github.com/martinpacesa/BindCraft [install_folder]\n\nNavigate into the install folder:\n\ncd [install_folder]\n\nRun the installation script with your CUDA version:\n\nbash install_bindcraft.sh --cuda '12.4' --pkg_manager 'conda'\nNotes:\n\nReplace 12.4 with your cluster’s CUDA version (check with nvcc --version)\nUse --pkg_manager 'mamba' if you prefer mamba (faster)\nIf you leave --cuda blank, the installer will try to detect the version automatically (may not always work correctly)\n\nThe installation creates a conda environment called BindCraft with all dependencies."
  },
  {
    "objectID": "monday/10-bindcraft.html#testing-the-installation",
    "href": "monday/10-bindcraft.html#testing-the-installation",
    "title": "10. BindCraft",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nBindCraft comes with example settings. To test:\n\nActivate the BindCraft environment:\n\nconda activate BindCraft\n\nNavigate to your BindCraft folder and run a test design:\n\ncd /path/to/bindcraft/folder/\npython -u ./bindcraft.py \\\n    --settings './settings_target/PDL1.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'\nThis runs binder design against the PDL1 example target. If it starts generating trajectories without errors, your installation was successful!\nNote: A complete binder design run can take hours to days depending on settings. For testing, you can stop it after a few trajectories complete."
  },
  {
    "objectID": "monday/10-bindcraft.html#hpc-job-submission",
    "href": "monday/10-bindcraft.html#hpc-job-submission",
    "title": "10. BindCraft",
    "section": "HPC Job Submission",
    "text": "HPC Job Submission\nFor SLURM clusters, BindCraft provides a template job script:\nsbatch ./bindcraft.slurm \\\n    --settings './settings_target/PDL1.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'\nOr create your own job script:\n#!/bin/bash\n#SBATCH --job-name=bindcraft\n#SBATCH --gpus=1\n#SBATCH --mem=64G\n#SBATCH --time=24:00:00\n\nconda activate BindCraft\ncd /path/to/bindcraft/folder/\n\npython -u ./bindcraft.py \\\n    --settings './settings_target/my_target.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'"
  },
  {
    "objectID": "monday/10-bindcraft.html#basic-usage-workflow",
    "href": "monday/10-bindcraft.html#basic-usage-workflow",
    "title": "10. BindCraft",
    "section": "Basic Usage Workflow",
    "text": "Basic Usage Workflow\n\nPrepare your target: Place your target protein PDB file in the BindCraft folder\nConfigure target settings: Edit or create a JSON file in settings_target/:\n\n{\n    \"design_path\": \"./my_binder_designs\",\n    \"binder_name\": \"my_binder\",\n    \"starting_pdb\": \"./my_target.pdb\",\n    \"chains\": \"A\",\n    \"target_hotspot_residues\": \"A10-20\",\n    \"lengths\": \"50-100\",\n    \"number_of_final_designs\": 100\n}\n\nRun the pipeline: Submit the job to design binders\nAnalyze results: BindCraft generates statistics and filtered designs in the output folder"
  },
  {
    "objectID": "monday/10-bindcraft.html#key-settings-explained",
    "href": "monday/10-bindcraft.html#key-settings-explained",
    "title": "10. BindCraft",
    "section": "Key Settings Explained",
    "text": "Key Settings Explained\nTarget Settings (settings_target/*.json):\n\nstarting_pdb: Your target protein structure\nchains: Which chains to target\ntarget_hotspot_residues: Specific residues to target (or null for automatic selection)\nlengths: Range of binder lengths to design (e.g., 50-100)\nnumber_of_final_designs: How many designs passing filters to generate before stopping\n\nFilters (settings_filters/*.json):\n\nControl which designs to keep based on confidence scores (pLDDT, pTM, i_pTM)\nInterface quality metrics (shape complementarity, energy, etc.)\nDefault filters are usually a good starting point\n\nAdvanced Settings (settings_advanced/*.json):\n\nDesign algorithm (default: 4stage)\nNumber of iterations for each design stage\nAF2 and MPNN parameters\nDesign weights for various objectives"
  },
  {
    "objectID": "monday/10-bindcraft.html#tips-for-success",
    "href": "monday/10-bindcraft.html#tips-for-success",
    "title": "10. BindCraft",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nTrim your target PDB: Remove unnecessary chains/residues to reduce memory requirements and speed up design\nStart with defaults: Use the default filter and advanced settings initially, then adjust based on results\nGenerate enough designs: Aim for at least 100 final designs passing filters (the script recommends ordering top 5-20 for experiments)\nBe patient: Expect to generate hundreds to thousands of trajectories to get enough accepted binders, especially for difficult targets\nMonitor acceptance rate: If very few trajectories pass filters, you may need to adjust design weights or filters"
  },
  {
    "objectID": "monday/10-bindcraft.html#understanding-bindcrafts-integration",
    "href": "monday/10-bindcraft.html#understanding-bindcrafts-integration",
    "title": "10. BindCraft",
    "section": "Understanding BindCraft’s Integration",
    "text": "Understanding BindCraft’s Integration\nBindCraft demonstrates a complete protein design workflow:\n\nDesign: Uses AlphaFold2 backpropagation to generate binder backbones\nOptimize: Uses ProteinMPNN to design sequences for the backbones\nValidate: Uses AlphaFold2 to predict the designed complex\nScore: Uses PyRosetta to evaluate interface quality\n\nThis integration showcases how real-world protein design combines multiple tools into a pipeline."
  },
  {
    "objectID": "monday/10-bindcraft.html#troubleshooting",
    "href": "monday/10-bindcraft.html#troubleshooting",
    "title": "10. BindCraft",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nGPU memory errors: Reduce the size of your target PDB or request more GPU memory. BindCraft recommends at least 32 GB GPU memory for larger complexes.\nCUDA version mismatch: Make sure you specified the correct CUDA version during installation. Check with nvcc --version.\nLow acceptance rate: If very few designs pass filters, you may need to:\n\nAdjust design weights in advanced settings\nRelax filter thresholds\nChange target hotspot selection\nIncrease the target site area\n\nWiki for detailed help: Before posting issues, check the complete wiki at https://github.com/martinpacesa/BindCraft/wiki/De-novo-binder-design-with-BindCraft\n\nNavigation: ← PLACER | Monday Overview | Next: ESM3 (Optional) →"
  },
  {
    "objectID": "monday/6-chai1.html",
    "href": "monday/6-chai1.html",
    "title": "6. Chai-1",
    "section": "",
    "text": "Chai-1 (paper, code) is a multi-modal foundation model for molecular structure prediction that achieves state-of-the-art performance across diverse benchmarks. Chai-1 enables unified prediction of proteins, small molecules, DNA, RNA, glycosylations, and more.\nChai-1 is useful for predicting complex biomolecular structures including protein-ligand complexes, protein-nucleic acid complexes, and multi-component assemblies. It can handle modified residues and incorporate experimental restraints.\nThe goal of this module is to install Chai-1 on your HPC cluster."
  },
  {
    "objectID": "monday/6-chai1.html#preparation-work",
    "href": "monday/6-chai1.html#preparation-work",
    "title": "6. Chai-1",
    "section": "Preparation Work",
    "text": "Preparation Work\n Mark as complete\nRequirements:\n\nLinux operating system\nPython 3.10 or later\nGPU with CUDA and bfloat16 support\nRecommended: A100 80GB, H100 80GB, or L40S 48GB\nAlso works on: A10, A30, RTX 4090"
  },
  {
    "objectID": "monday/6-chai1.html#installation-steps",
    "href": "monday/6-chai1.html#installation-steps",
    "title": "6. Chai-1",
    "section": "Installation Steps",
    "text": "Installation Steps\n Mark as complete\n\nCreate a conda environment with Python 3.10+:\n\nmamba create -n chailab python=3.11\nmamba activate chailab\n\nInstall Chai-1:\n\npip install chai_lab==0.6.1\nOr for the latest development version:\npip install git+https://github.com/chaidiscovery/chai-lab.git"
  },
  {
    "objectID": "monday/6-chai1.html#testing-the-installation",
    "href": "monday/6-chai1.html#testing-the-installation",
    "title": "6. Chai-1",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test FASTA file test.fasta:\n&gt;protein|name=example\nMKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\nRun prediction:\nchai-lab fold test.fasta output_folder/\nBy default, this generates 5 sample predictions using embeddings without MSAs."
  },
  {
    "objectID": "monday/6-chai1.html#using-msas-for-better-performance",
    "href": "monday/6-chai1.html#using-msas-for-better-performance",
    "title": "6. Chai-1",
    "section": "Using MSAs for Better Performance",
    "text": "Using MSAs for Better Performance\nFor improved performance, use MSAs via the ColabFold server:\nchai-lab fold --use-msa-server --use-templates-server input.fasta output_folder/\nNote: This uses the public ColabFold MMseqs2 server, which is a shared resource. Please be considerate of usage."
  },
  {
    "objectID": "monday/6-chai1.html#for-hpc-internal-colabfold-server",
    "href": "monday/6-chai1.html#for-hpc-internal-colabfold-server",
    "title": "6. Chai-1",
    "section": "For HPC Internal ColabFold Server",
    "text": "For HPC Internal ColabFold Server\nIf your HPC has an internal ColabFold server:\nchai-lab fold --use-msa-server \\\n    --msa-server-url \"https://internal.colabserver.edu\" \\\n    input.fasta output_folder/"
  },
  {
    "objectID": "monday/6-chai1.html#hpc-job-script-example",
    "href": "monday/6-chai1.html#hpc-job-script-example",
    "title": "6. Chai-1",
    "section": "HPC Job Script Example",
    "text": "HPC Job Script Example\nFor SLURM clusters:\n#!/bin/bash\n#SBATCH --job-name=chai\n#SBATCH --gpus=1\n#SBATCH --time=04:00:00\n#SBATCH --mem=64G\n\nmodule load cuda/12.1\n\nmamba activate chailab\n\n# Set custom download directory if needed\nexport CHAI_DOWNLOADS_DIR=/scratch/chai_models\n\n# Run prediction with MSAs\nchai-lab fold --use-msa-server --use-templates-server \\\n    my_complex.fasta \\\n    predictions/"
  },
  {
    "objectID": "monday/6-chai1.html#python-api-usage",
    "href": "monday/6-chai1.html#python-api-usage",
    "title": "6. Chai-1",
    "section": "Python API Usage",
    "text": "Python API Usage\nYou can also use Chai-1 programmatically:\nfrom chai_lab.chai1 import run_inference\n\n# Run inference\nresults = run_inference(\n    fasta_file=\"input.fasta\",\n    output_dir=\"output/\",\n    num_trunk_recycles=3,\n    num_diffn_timesteps=200,\n    seed=42\n)\nSee examples/predict_structure.py in the repository for more details."
  },
  {
    "objectID": "monday/6-chai1.html#advanced-features",
    "href": "monday/6-chai1.html#advanced-features",
    "title": "6. Chai-1",
    "section": "Advanced Features",
    "text": "Advanced Features\nCustom Templates: Provide your own structure templates\nExperimental Restraints: Specify inter-chain contacts or covalent bonds\nCustom MSAs: Provide MSAs in aligned.pqt format\nSee the restraints documentation and covalent bonds documentation for advanced usage."
  },
  {
    "objectID": "monday/6-chai1.html#web-server",
    "href": "monday/6-chai1.html#web-server",
    "title": "6. Chai-1",
    "section": "Web Server",
    "text": "Web Server\nYou can also test Chai-1 via the web interface: https://lab.chaidiscovery.com"
  },
  {
    "objectID": "monday/6-chai1.html#troubleshooting",
    "href": "monday/6-chai1.html#troubleshooting",
    "title": "6. Chai-1",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nGPU compatibility: Chai-1 requires bfloat16 support. Older GPUs may not work.\nOut of memory: Use smaller complexes or request more GPU memory.\nDownload location: Control where models are downloaded:\nexport CHAI_DOWNLOADS_DIR=/path/to/storage\n\nNavigation: ← OpenFold | Monday Overview | Next: Boltz-2 →"
  },
  {
    "objectID": "monday/2-ligandmpnn.html",
    "href": "monday/2-ligandmpnn.html",
    "title": "2. LigandMPNN",
    "section": "",
    "text": "LigandMPNN (paper, code) is a deep learning model for context-aware protein sequence design. It extends ProteinMPNN to handle small molecules, metal ions, and other non-protein components in protein design tasks.\nLigandMPNN is useful for designing protein sequences that interact with ligands, cofactors, or other small molecules. It can also be used for side chain packing and evaluating sequence-structure compatibility.\nThe goal of this module is to install LigandMPNN on your HPC cluster."
  },
  {
    "objectID": "monday/2-ligandmpnn.html#preparation-work",
    "href": "monday/2-ligandmpnn.html#preparation-work",
    "title": "2. LigandMPNN",
    "section": "Preparation Work",
    "text": "Preparation Work\n Mark as complete\nThis assignment assumes that you have access to a computing cluster that allows installation of new software. We’ll be creating virtual environments using Conda/Mamba.\n(If you don’t have Mamba or prefer using Conda, just swap mamba for conda in the install commands below)"
  },
  {
    "objectID": "monday/2-ligandmpnn.html#installation-steps",
    "href": "monday/2-ligandmpnn.html#installation-steps",
    "title": "2. LigandMPNN",
    "section": "Installation Steps",
    "text": "Installation Steps\n Mark as complete\n\nClone the LigandMPNN repository:\n\ngit clone https://github.com/dauparas/LigandMPNN.git\ncd LigandMPNN\n\nDownload the model parameters:\n\nbash get_model_params.sh \"./model_params\"\n\nCreate a new conda environment:\n\nmamba create -n ligandmpnn_env python=3.11\n\nActivate the environment:\n\nmamba activate ligandmpnn_env\n\nInstall dependencies:\n\npip3 install -r requirements.txt\nThe requirements include PyTorch, NumPy, and ProDy for reading/writing PDB files."
  },
  {
    "objectID": "monday/2-ligandmpnn.html#testing-the-installation",
    "href": "monday/2-ligandmpnn.html#testing-the-installation",
    "title": "2. LigandMPNN",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a test design on the provided example structure:\npython run.py \\\n    --seed 111 \\\n    --pdb_path \"./inputs/1BC8.pdb\" \\\n    --out_folder \"./outputs/test_output\"\nIf this completes successfully and generates output files in ./outputs/test_output/, your installation was successful!"
  },
  {
    "objectID": "monday/2-ligandmpnn.html#hpc-specific-notes",
    "href": "monday/2-ligandmpnn.html#hpc-specific-notes",
    "title": "2. LigandMPNN",
    "section": "HPC-Specific Notes",
    "text": "HPC-Specific Notes\nGPU Requirements: LigandMPNN can run on both CPU and GPU. GPU is recommended for faster processing. Make sure to request GPU resources when submitting jobs on your cluster.\nBatch Processing: For designing multiple structures, use the --pdb_path_multi option with a JSON file listing all input PDBs. This is more efficient since the model is loaded only once."
  },
  {
    "objectID": "monday/2-ligandmpnn.html#example-hpc-job-script",
    "href": "monday/2-ligandmpnn.html#example-hpc-job-script",
    "title": "2. LigandMPNN",
    "section": "Example HPC Job Script",
    "text": "Example HPC Job Script\nFor SLURM clusters:\n#!/bin/bash\n#SBATCH --job-name=ligandmpnn\n#SBATCH --gpus=1\n#SBATCH --time=02:00:00\n#SBATCH --mem=16G\n\nmodule load cuda/12.1  # Adjust to your cluster\n\nmamba activate ligandmpnn_env\n\npython run.py \\\n    --model_type \"ligand_mpnn\" \\\n    --seed 111 \\\n    --pdb_path \"./inputs/my_protein.pdb\" \\\n    --out_folder \"./outputs/my_design\" \\\n    --number_of_batches 5"
  },
  {
    "objectID": "monday/2-ligandmpnn.html#common-use-cases",
    "href": "monday/2-ligandmpnn.html#common-use-cases",
    "title": "2. LigandMPNN",
    "section": "Common Use Cases",
    "text": "Common Use Cases\nDesign with ligand context:\npython run.py \\\n    --model_type \"ligand_mpnn\" \\\n    --pdb_path \"./input.pdb\" \\\n    --ligand \"HEM\" \\\n    --out_folder \"./output\"\nFix specific residues:\npython run.py \\\n    --pdb_path \"./input.pdb\" \\\n    --fixed_residues \"A10 A20 A30\" \\\n    --out_folder \"./output\"\n\nNavigation: ← LocalColabFold | Monday Overview | Next: RFdiffusion2 →"
  },
  {
    "objectID": "monday/7-boltz2.html",
    "href": "monday/7-boltz2.html",
    "title": "7. Boltz-2",
    "section": "",
    "text": "Boltz-2 (paper, code) is a biomolecular foundation model that jointly models complex structures and binding affinities. It’s the first deep learning model to approach the accuracy of physics-based free-energy perturbation (FEP) methods while running 1000x faster.\nBoltz-2 is useful for structure prediction of biomolecular complexes AND binding affinity prediction, making it valuable for drug discovery and protein design. It can predict structures for proteins, nucleic acids, small molecules, and covalent modifications.\nThe goal of this module is to install Boltz-2 on your HPC cluster."
  },
  {
    "objectID": "monday/7-boltz2.html#preparation-work",
    "href": "monday/7-boltz2.html#preparation-work",
    "title": "7. Boltz-2",
    "section": "Preparation Work",
    "text": "Preparation Work\n Mark as complete\nThis assignment assumes that you have access to a computing cluster that allows installation of new software. We’ll be creating virtual environments using Conda/Mamba.\nNote: It’s recommended to install Boltz in a fresh Python environment."
  },
  {
    "objectID": "monday/7-boltz2.html#installation-steps",
    "href": "monday/7-boltz2.html#installation-steps",
    "title": "7. Boltz-2",
    "section": "Installation Steps",
    "text": "Installation Steps\n Mark as complete\nOption 1: Install from PyPI (recommended):\npip install boltz[cuda] -U\nOption 2: Install from GitHub for daily updates:\ngit clone https://github.com/jwohlwend/boltz.git\ncd boltz\npip install -e .[cuda]\nFor CPU-only or non-CUDA GPUs: Remove [cuda] from the commands above (note: CPU version is significantly slower)."
  },
  {
    "objectID": "monday/7-boltz2.html#testing-the-installation",
    "href": "monday/7-boltz2.html#testing-the-installation",
    "title": "7. Boltz-2",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test YAML file test_input.yaml:\nversion: 1\nsequences:\n  - protein:\n      id: [A, B]\n      sequence: MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\nRun prediction:\nboltz predict test_input.yaml --use_msa_server\nThis will predict the structure and save results in the output directory."
  },
  {
    "objectID": "monday/7-boltz2.html#input-format",
    "href": "monday/7-boltz2.html#input-format",
    "title": "7. Boltz-2",
    "section": "Input Format",
    "text": "Input Format\nBoltz uses YAML files to describe biomolecules and properties to predict. For detailed input format information, see the prediction instructions."
  },
  {
    "objectID": "monday/7-boltz2.html#binding-affinity-prediction",
    "href": "monday/7-boltz2.html#binding-affinity-prediction",
    "title": "7. Boltz-2",
    "section": "Binding Affinity Prediction",
    "text": "Binding Affinity Prediction\nBoltz-2 provides two affinity predictions:\n\naffinity_probability_binary (0-1 scale):\n\nUse for: Hit discovery, detecting binders from decoys\nRepresents: Probability that ligand is a binder\n\naffinity_pred_value (log10(IC50) in μM):\n\nUse for: Hit-to-lead and lead optimization\nRepresents: Specific binding affinity for comparison"
  },
  {
    "objectID": "monday/7-boltz2.html#msa-server-authentication",
    "href": "monday/7-boltz2.html#msa-server-authentication",
    "title": "7. Boltz-2",
    "section": "MSA Server Authentication",
    "text": "MSA Server Authentication\nWhen using --use_msa_server with servers requiring authentication, provide credentials:\nexport BOLTZ_MSA_TOKEN=\"your_token_here\"\nboltz predict input.yaml --use_msa_server\nSee the prediction documentation for more details."
  },
  {
    "objectID": "monday/7-boltz2.html#hpc-job-script-example",
    "href": "monday/7-boltz2.html#hpc-job-script-example",
    "title": "7. Boltz-2",
    "section": "HPC Job Script Example",
    "text": "HPC Job Script Example\nFor SLURM clusters:\n#!/bin/bash\n#SBATCH --job-name=boltz\n#SBATCH --gpus=1\n#SBATCH --time=04:00:00\n#SBATCH --mem=32G\n\nmodule load cuda/12.1\n\n# Activate environment\nsource ~/.bashrc\n# or: mamba activate boltz_env\n\n# Run prediction\nboltz predict my_complex.yaml --use_msa_server --out_dir results/"
  },
  {
    "objectID": "monday/7-boltz2.html#performance-notes",
    "href": "monday/7-boltz2.html#performance-notes",
    "title": "7. Boltz-2",
    "section": "Performance Notes",
    "text": "Performance Notes\n\nGPU: Boltz runs efficiently on recent NVIDIA GPUs\nCPU: Functional but significantly slower than GPU\nSpeed: Approximately 1000x faster than FEP methods for affinity prediction"
  },
  {
    "objectID": "monday/7-boltz2.html#example-use-cases",
    "href": "monday/7-boltz2.html#example-use-cases",
    "title": "7. Boltz-2",
    "section": "Example Use Cases",
    "text": "Example Use Cases\nStructure prediction only:\nboltz predict structure.yaml\nStructure + Affinity prediction:\n# In your YAML file, specify affinity prediction\nversion: 1\nsequences:\n  - protein:\n      id: A\n      sequence: MKTVRQERLK...\n  - ligand:\n      id: L\n      smiles: \"CC(C)CC1=CC=C(C=C1)C(C)C\"\nproperties:\n  - affinity"
  },
  {
    "objectID": "monday/7-boltz2.html#troubleshooting",
    "href": "monday/7-boltz2.html#troubleshooting",
    "title": "7. Boltz-2",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nInstallation issues: Ensure you’re in a fresh environment and have compatible CUDA version.\nMSA server errors: Check authentication credentials and server availability.\n\nNavigation: ← Chai-1 | Monday Overview | Next: DiffDock-PP →"
  },
  {
    "objectID": "monday/9-placer.html",
    "href": "monday/9-placer.html",
    "title": "9. PLACER",
    "section": "",
    "text": "PLACER (paper, code) stands for Protein-Ligand Atomistic Conformational Ensemble Resolver. It’s a graph neural network that operates entirely at the atomic level to generate conformational ensembles of protein-ligand complexes.\nPLACER is useful for:\nThe goal of this module is to install PLACER on your HPC cluster."
  },
  {
    "objectID": "monday/9-placer.html#preparation-work",
    "href": "monday/9-placer.html#preparation-work",
    "title": "9. PLACER",
    "section": "Preparation Work",
    "text": "Preparation Work\n Mark as complete\nThis assignment assumes that you have access to a computing cluster that allows installation of new software. We’ll be creating virtual environments using Conda/Mamba."
  },
  {
    "objectID": "monday/9-placer.html#installation-steps",
    "href": "monday/9-placer.html#installation-steps",
    "title": "9. PLACER",
    "section": "Installation Steps",
    "text": "Installation Steps\n Mark as complete\n\nClone the repository:\n\ngit clone https://github.com/baker-laboratory/PLACER.git\ncd PLACER\nThe repository contains model weights and is ready to run after environment setup.\n\nCreate the conda environment from the provided file:\n\nmamba env create -f envs/placer_env.yml\n\nActivate the environment:\n\nmamba activate placer_env"
  },
  {
    "objectID": "monday/9-placer.html#testing-the-installation",
    "href": "monday/9-placer.html#testing-the-installation",
    "title": "9. PLACER",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a simple heme docking prediction:\npython run_PLACER.py \\\n    --ifile examples/inputs/dnHEM1.pdb \\\n    --odir test_output \\\n    --rerank prmsd \\\n    -n 10 \\\n    --ligand_file HEM:examples/ligands/HEM.mol2\nIf this completes successfully and creates output files in test_output/, your installation was successful!"
  },
  {
    "objectID": "monday/9-placer.html#command-line-usage",
    "href": "monday/9-placer.html#command-line-usage",
    "title": "9. PLACER",
    "section": "Command Line Usage",
    "text": "Command Line Usage\nBasic syntax:\npython run_PLACER.py -f INPUT.pdb -o OUTPUT_DIR -n NUM_SAMPLES\nKey parameters:\n\n-f or --ifile: Input PDB/mmCIF file\n-o or --odir: Output directory\n-n or --nsamples: Number of ensemble samples (50-100 recommended for docking)\n--rerank: Rank outputs by confidence metric (prmsd, plddt, or plddt_pde)\n--predict_ligand: Specify which ligand(s) to predict\n--fixed_ligand: Keep certain ligands fixed in place\n--ligand_file: Provide SDF/MOL2 files for correct atom typing"
  },
  {
    "objectID": "monday/9-placer.html#hpc-job-script-example",
    "href": "monday/9-placer.html#hpc-job-script-example",
    "title": "9. PLACER",
    "section": "HPC Job Script Example",
    "text": "HPC Job Script Example\nFor SLURM clusters:\n#!/bin/bash\n#SBATCH --job-name=placer\n#SBATCH --gpus=1\n#SBATCH --time=04:00:00\n#SBATCH --mem=32G\n\nmodule load cuda/12.1\n\nmamba activate placer_env\n\n# Predict ligand binding with 100 samples\npython run_PLACER.py \\\n    --ifile my_complex.pdb \\\n    --odir results/ \\\n    --predict_ligand LIG-501 \\\n    --rerank prmsd \\\n    -n 100 \\\n    --ligand_file LIG:ligand.sdf"
  },
  {
    "objectID": "monday/9-placer.html#understanding-confidence-scores",
    "href": "monday/9-placer.html#understanding-confidence-scores",
    "title": "9. PLACER",
    "section": "Understanding Confidence Scores",
    "text": "Understanding Confidence Scores\nPLACER provides several confidence metrics:\n\nprmsd: Predicted RMSD (recommended for docking). Good: &lt;2.0, Acceptable: &lt;4.0\nplddt: Predicted lDDT from 1D track. Good: &gt;0.8\nplddt_pde: Predicted lDDT from 2D track. Good: &gt;0.8\nfape: All-atom FAPE loss\nrmsd: Actual RMSD to reference (if available)\nkabsch: Superimposed RMSD (conformation accuracy)"
  },
  {
    "objectID": "monday/9-placer.html#example-use-cases",
    "href": "monday/9-placer.html#example-use-cases",
    "title": "9. PLACER",
    "section": "Example Use Cases",
    "text": "Example Use Cases\nLigand docking with cofactor fixed:\npython run_PLACER.py \\\n    --ifile 4dtz.cif \\\n    --odir output/ \\\n    --predict_ligand D-LDP-501 \\\n    --fixed_ligand C-HEM-500 \\\n    -n 100 \\\n    --rerank prmsd\nSide chain prediction:\npython run_PLACER.py \\\n    --ifile protein.pdb \\\n    --odir output/ \\\n    --target_res A-149 \\\n    -n 50 \\\n    --no-use_sm  # apo mode, no small molecule\nMultiple ligands simultaneously:\npython run_PLACER.py \\\n    --ifile complex.pdb \\\n    --odir output/ \\\n    --predict_multi \\\n    --predict_ligand LIG1 LIG2 \\\n    -n 100"
  },
  {
    "objectID": "monday/9-placer.html#python-api",
    "href": "monday/9-placer.html#python-api",
    "title": "9. PLACER",
    "section": "Python API",
    "text": "Python API\nPLACER can also be imported as a Python module:\nimport sys\nsys.path.append(\"/path/to/PLACER\")\nimport PLACER\n\nplacer = PLACER.PLACER()  # Load model\n\npl_input = PLACER.PLACERinput()\npl_input.pdb(\"complex.pdb\")\npl_input.name(\"my_prediction\")\npl_input.ligand_reference({\"HEM\": \"heme.mol2\"})\n\n# Run 50 predictions\noutputs = placer.run(pl_input, 50)"
  },
  {
    "objectID": "monday/9-placer.html#performance-notes",
    "href": "monday/9-placer.html#performance-notes",
    "title": "9. PLACER",
    "section": "Performance Notes",
    "text": "Performance Notes\nGPU: 1-3 seconds per model (typical ligand-protein complex)\nCPU (1 core): ~7 minutes per model\nCPU (8 cores): ~1 minute per model\nLigands with many symmetric groups take longer."
  },
  {
    "objectID": "monday/9-placer.html#troubleshooting",
    "href": "monday/9-placer.html#troubleshooting",
    "title": "9. PLACER",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nNon-planar aromatic rings: Provide SDF/MOL2 file with --ligand_file for correct bonding information.\nMissing ligands in PDB: Currently, ligands must be in the input PDB. SMILES-only input not supported.\nCustom residues: Use --residue_json to define non-canonical amino acids.\n\nNavigation: ← DiffDock-PP | Monday Overview | Next: BindCraft →"
  },
  {
    "objectID": "monday/5-openfold.html",
    "href": "monday/5-openfold.html",
    "title": "5. OpenFold",
    "section": "",
    "text": "OpenFold (paper, code) is a faithful, trainable PyTorch reproduction of DeepMind’s AlphaFold2. It achieves performance comparable to AlphaFold2 and provides a fully open-source implementation for protein structure prediction.\nOpenFold is useful for protein structure prediction with full transparency into the model architecture and training process. It’s particularly valuable for researchers who want to understand, modify, or retrain structure prediction models.\nThe goal of this module is to install OpenFold on your HPC cluster."
  },
  {
    "objectID": "monday/5-openfold.html#preparation-work",
    "href": "monday/5-openfold.html#preparation-work",
    "title": "5. OpenFold",
    "section": "Preparation Work",
    "text": "Preparation Work\n Mark as complete\nThis assignment assumes that you have access to a computing cluster that allows installation of new software.\nNote: OpenFold installation can be complex. Please refer to the official documentation at openfold.readthedocs.io for the most up-to-date installation instructions."
  },
  {
    "objectID": "monday/5-openfold.html#recommended-installation-method",
    "href": "monday/5-openfold.html#recommended-installation-method",
    "title": "5. OpenFold",
    "section": "Recommended Installation Method",
    "text": "Recommended Installation Method\n Mark as complete\nThe easiest way to install OpenFold on an HPC is to follow the official documentation:\n\nVisit the OpenFold documentation: https://openfold.readthedocs.io\nFollow the installation instructions for your system\nThe documentation provides detailed guidance for:\n\nEnvironment setup\nDependency installation\nModel weight downloads\nRunning inference\nTraining (if desired)"
  },
  {
    "objectID": "monday/5-openfold.html#key-requirements",
    "href": "monday/5-openfold.html#key-requirements",
    "title": "5. OpenFold",
    "section": "Key Requirements",
    "text": "Key Requirements\n\nPython 3.7+\nPyTorch\nCUDA-capable GPU\nSignificant disk space for databases (if using MSAs)"
  },
  {
    "objectID": "monday/5-openfold.html#basic-installation-overview",
    "href": "monday/5-openfold.html#basic-installation-overview",
    "title": "5. OpenFold",
    "section": "Basic Installation Overview",
    "text": "Basic Installation Overview\nWhile exact commands may change, the general process involves:\n\nClone the repository:\n\ngit clone https://github.com/aqlaboratory/openfold.git\ncd openfold\n\nSet up the conda environment:\n\n# Follow instructions from official docs\nconda env create -f environment.yml\nconda activate openfold\n\nInstall OpenFold:\n\n# Follow instructions from official docs\npython setup.py install\n\nDownload model weights:\n\n# Scripts provided in the repository\nbash scripts/download_openfold_params.sh openfold/resources"
  },
  {
    "objectID": "monday/5-openfold.html#hpc-specific-notes",
    "href": "monday/5-openfold.html#hpc-specific-notes",
    "title": "5. OpenFold",
    "section": "HPC-Specific Notes",
    "text": "HPC-Specific Notes\nDatabase Requirements: If using MSAs, OpenFold requires access to large sequence databases (similar to AlphaFold2). These may already be available on your HPC system. Check with your system administrator.\nGPU Requirements: Structure prediction requires CUDA-capable GPUs. Request appropriate GPU resources when submitting jobs.\nDocker/Singularity: If OpenFold provides Docker containers, remember that most academic HPCs use Singularity/Apptainer instead of Docker."
  },
  {
    "objectID": "monday/5-openfold.html#testing-your-installation",
    "href": "monday/5-openfold.html#testing-your-installation",
    "title": "5. OpenFold",
    "section": "Testing Your Installation",
    "text": "Testing Your Installation\n Mark as complete\nOnce installed, test with a simple prediction:\n# Example command (exact syntax from official docs)\npython run_pretrained_openfold.py \\\n    --fasta_paths example.fasta \\\n    --output_dir predictions/\nConsult the official documentation for exact command syntax and options."
  },
  {
    "objectID": "monday/5-openfold.html#why-openfold",
    "href": "monday/5-openfold.html#why-openfold",
    "title": "5. OpenFold",
    "section": "Why OpenFold?",
    "text": "Why OpenFold?\n\nTransparency: Full access to model architecture and training code\nReproducibility: Can retrain models with custom data\nResearch: Understand how structure prediction models work\nCustomization: Modify model architecture for specific applications"
  },
  {
    "objectID": "monday/5-openfold.html#documentation-and-support",
    "href": "monday/5-openfold.html#documentation-and-support",
    "title": "5. OpenFold",
    "section": "Documentation and Support",
    "text": "Documentation and Support\nFor the most accurate and up-to-date information:\n\nOfficial Documentation: https://openfold.readthedocs.io\nGitHub Repository: https://github.com/aqlaboratory/openfold\nOriginal README: Available in the repository\n\n\nNavigation: ← ESMFold | Monday Overview | Next: Chai-1 →"
  },
  {
    "objectID": "monday/8-diffdock-pp.html",
    "href": "monday/8-diffdock-pp.html",
    "title": "8. DiffDock-PP",
    "section": "",
    "text": "DiffDock-PP (paper, code) is a graph neural network trained to complete de-noising of rigid transformations (rotation and translation) that parameterize the docking orientation between two rigid protein subunits.\nDiffDock-PP is useful for predicting binding orientations between protein chains and can be used to orthogonally validate structure predictions, particularly for protein-protein docking.\nThe goal of this module is to install DiffDock-PP on your HPC cluster."
  },
  {
    "objectID": "monday/8-diffdock-pp.html#preparation-work",
    "href": "monday/8-diffdock-pp.html#preparation-work",
    "title": "8. DiffDock-PP",
    "section": "Preparation Work",
    "text": "Preparation Work\n Mark as complete\nThis assignment assumes that you have access to a computing cluster that allows installation of new software. We’ll be creating virtual environments using Conda/Mamba.\n(If you don’t have Mamba or prefer using Conda, just swap mamba for conda in the install commands below)"
  },
  {
    "objectID": "monday/8-diffdock-pp.html#installation-steps",
    "href": "monday/8-diffdock-pp.html#installation-steps",
    "title": "8. DiffDock-PP",
    "section": "Installation Steps",
    "text": "Installation Steps\n Mark as complete\n\nClone the DiffDock-PP repository:\n\ngit clone https://github.com/ketatam/DiffDock-PP.git\ncd DiffDock-PP\n\nCreate a new environment called “diffdock_pp”:\n\nmamba create -n diffdock_pp\n\nActivate the new environment:\n\nmamba activate diffdock_pp\n\nInstall PyTorch:\n\nmamba install pytorch=1.13.0 pytorch-cuda=11.6 -c pytorch -c nvidia\n\nInstall PyG (PyTorch Geometric) packages:\n\nmamba install pytorch-scatter pytorch-sparse pytorch-cluster pytorch-spline-conv pyg -c pyg\n\nInstall the remaining dependencies:\n\nmamba install mkl=2024.0 \"numpy&lt;2.0\" dill tqdm pyyaml pandas biopandas scikit-learn biopython e3nn wandb tensorboard tensorboardX matplotlib"
  },
  {
    "objectID": "monday/8-diffdock-pp.html#testing-the-installation",
    "href": "monday/8-diffdock-pp.html#testing-the-installation",
    "title": "8. DiffDock-PP",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\n\nCreate required directories:\n\nmkdir storage\n\nRun the test script on the DB5 benchmark:\n\nbash DiffDock-PP/src/db5_inference.sh\nIf the command works without errors and the output folder visualization/epoch-0/ contains PDB files of docked complexes, your installation was successful!"
  },
  {
    "objectID": "monday/8-diffdock-pp.html#hpc-job-script-example",
    "href": "monday/8-diffdock-pp.html#hpc-job-script-example",
    "title": "8. DiffDock-PP",
    "section": "HPC Job Script Example",
    "text": "HPC Job Script Example\nFor SLURM clusters:\n#!/bin/bash\n#SBATCH --job-name=diffdock_pp\n#SBATCH --gpus=1\n#SBATCH --time=02:00:00\n#SBATCH --mem=16G\n\nmodule load cuda/11.6  # Adjust to match PyTorch CUDA version\n\nmamba activate diffdock_pp\n\n# Create output directory\nmkdir -p storage\n\n# Run inference\nbash DiffDock-PP/src/db5_inference.sh"
  },
  {
    "objectID": "monday/8-diffdock-pp.html#hpc-specific-notes",
    "href": "monday/8-diffdock-pp.html#hpc-specific-notes",
    "title": "8. DiffDock-PP",
    "section": "HPC-Specific Notes",
    "text": "HPC-Specific Notes\nCUDA Version: DiffDock-PP was tested with PyTorch 1.13.0 and CUDA 11.6. Make sure your HPC has compatible CUDA modules:\nmodule avail cuda\nGPU Requirements: Docking predictions benefit from GPU acceleration. Request GPU resources when submitting jobs."
  },
  {
    "objectID": "monday/8-diffdock-pp.html#understanding-the-output",
    "href": "monday/8-diffdock-pp.html#understanding-the-output",
    "title": "8. DiffDock-PP",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nThe script generates:\n\nPDB files of predicted protein-protein complexes in visualization/epoch-0/\nEach PDB file represents a predicted docking pose\nMultiple poses may be generated for ensemble predictions"
  },
  {
    "objectID": "monday/8-diffdock-pp.html#use-cases",
    "href": "monday/8-diffdock-pp.html#use-cases",
    "title": "8. DiffDock-PP",
    "section": "Use Cases",
    "text": "Use Cases\n\nProtein-Protein Docking: Predict binding orientations between protein chains\nComplex Validation: Validate predicted protein-protein interfaces\nEnsemble Generation: Generate multiple docking poses to capture uncertainty"
  },
  {
    "objectID": "monday/8-diffdock-pp.html#troubleshooting",
    "href": "monday/8-diffdock-pp.html#troubleshooting",
    "title": "8. DiffDock-PP",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nPyG installation failures: Ensure CUDA toolkit is available and PyTorch is installed first.\nCUDA version mismatch: Make sure the CUDA version for PyTorch matches your system’s CUDA:\nnvcc --version\nNumPy version: The environment specifies numpy&lt;2.0 for compatibility. Don’t upgrade NumPy beyond 2.0.\n\nNavigation: ← Boltz-2 | Monday Overview | Next: PLACER →"
  },
  {
    "objectID": "monday/3-rfdiffusion2.html",
    "href": "monday/3-rfdiffusion2.html",
    "title": "3. RFdiffusion2",
    "section": "",
    "text": "RFdiffusion2 (paper, code) is a protein design model capable of atom-level active site scaffolding. It extends the original RFdiffusion to enable more precise control over protein-ligand interactions at the atomic level.\nRFdiffusion2 is useful for designing proteins with specific active site geometries, enzyme design with atomic-level motif constraints, and designing binders to small molecules with precise interaction patterns.\nThe goal of this module is to install RFdiffusion2 on your HPC cluster."
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#preparation-work",
    "href": "monday/3-rfdiffusion2.html#preparation-work",
    "title": "3. RFdiffusion2",
    "section": "Preparation Work",
    "text": "Preparation Work\n Mark as complete\nThis assignment assumes that you have access to a computing cluster that allows installation of new software.\nIMPORTANT: RFdiffusion2 uses Apptainer/Singularity containers. Most academic HPCs support Singularity/Apptainer, but verify with your system administrator if unsure."
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#installation-steps",
    "href": "monday/3-rfdiffusion2.html#installation-steps",
    "title": "3. RFdiffusion2",
    "section": "Installation Steps",
    "text": "Installation Steps\n Mark as complete\n\nClone the repository:\n\ngit clone https://github.com/RosettaCommons/RFdiffusion2.git\ncd RFdiffusion2\n\nAdd the repo to your PYTHONPATH (add this to your ~/.bashrc):\n\nexport PYTHONPATH=\"/path/to/your/RFdiffusion2:$PYTHONPATH\"\n\nDownload the model weights and containers:\n\npython setup.py\nNote: This downloads large files (~several GB) and can take 30+ minutes. If the download is interrupted, run python setup.py overwrite to resume.\n\nInstall Apptainer/Singularity\n\nFor Apptainer on Ubuntu/Debian:\nsudo add-apt-repository -y ppa:apptainer/ppa\nsudo apt update\nsudo apt install -y apptainer\nFor HPC clusters: Apptainer/Singularity is usually pre-installed. Load the module:\nmodule load apptainer\n# or\nmodule load singularity"
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#docker-alternative-for-hpc",
    "href": "monday/3-rfdiffusion2.html#docker-alternative-for-hpc",
    "title": "3. RFdiffusion2",
    "section": "Docker Alternative for HPC",
    "text": "Docker Alternative for HPC\nIMPORTANT HPC NOTE: Most academic HPCs do NOT support Docker for security reasons. If the official documentation mentions Docker, use Singularity/Apptainer instead. The downloaded .sif file in rf_diffusion/exec/ is a Singularity/Apptainer container that works on HPCs."
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#testing-the-installation",
    "href": "monday/3-rfdiffusion2.html#testing-the-installation",
    "title": "3. RFdiffusion2",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a demo case:\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=open_source_demo \\\n    sweep.benchmarks=active_site_unindexed_atomic_partial_ligand\nNote: Omit the --nv flag if running without GPU.\nThis will generate a protein design with an atomized active site motif. The output will be in:\npipeline_outputs/&lt;timestamp&gt;_open_source_demo"
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#hpc-specific-notes",
    "href": "monday/3-rfdiffusion2.html#hpc-specific-notes",
    "title": "3. RFdiffusion2",
    "section": "HPC-Specific Notes",
    "text": "HPC-Specific Notes\nGPU Requirements: RFdiffusion2 requires CUDA-capable GPUs for reasonable performance. Request GPU nodes when submitting jobs.\nContainer Execution: Use apptainer (or singularity) instead of docker:\n# NOT: docker run ...\n# INSTEAD:\napptainer exec --nv &lt;container.sif&gt; &lt;command&gt;"
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#example-hpc-job-script",
    "href": "monday/3-rfdiffusion2.html#example-hpc-job-script",
    "title": "3. RFdiffusion2",
    "section": "Example HPC Job Script",
    "text": "Example HPC Job Script\nFor SLURM clusters:\n#!/bin/bash\n#SBATCH --job-name=rfd2\n#SBATCH --gpus=1\n#SBATCH --time=04:00:00\n#SBATCH --mem=32G\n\nmodule load apptainer\nmodule load cuda/12.1\n\ncd /path/to/RFdiffusion2\n\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=open_source_demo"
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#troubleshooting",
    "href": "monday/3-rfdiffusion2.html#troubleshooting",
    "title": "3. RFdiffusion2",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nNo GPU available: The demo will be extremely slow on CPU (30+ minutes per case). Always use GPUs for production runs.\nContainer permission errors: Ensure you have read/execute permissions on the .sif file.\n\nNavigation: ← LigandMPNN | Monday Overview | Next: ESMFold →"
  },
  {
    "objectID": "monday/index.html",
    "href": "monday/index.html",
    "title": "Monday: Tool Installation",
    "section": "",
    "text": "Monday is dedicated to installing the essential ML-based protein design and structure prediction tools on your HPC cluster. Each module guides you through installing a specific tool, from structure predictors like LocalColabFold and ESMFold to design tools like LigandMPNN and BindCraft.\nGoal: By the end of Monday, you should have all major tools installed and tested on your HPC cluster."
  },
  {
    "objectID": "monday/index.html#overview",
    "href": "monday/index.html#overview",
    "title": "Monday: Tool Installation",
    "section": "",
    "text": "Monday is dedicated to installing the essential ML-based protein design and structure prediction tools on your HPC cluster. Each module guides you through installing a specific tool, from structure predictors like LocalColabFold and ESMFold to design tools like LigandMPNN and BindCraft.\nGoal: By the end of Monday, you should have all major tools installed and tested on your HPC cluster."
  },
  {
    "objectID": "monday/index.html#modules",
    "href": "monday/index.html#modules",
    "title": "Monday: Tool Installation",
    "section": "Modules",
    "text": "Modules\n\n\n\n#\nTool\nDescription\nStatus\n\n\n\n\n1\nLocalColabFold\nFast AlphaFold2 structure prediction\nRequired\n\n\n2\nLigandMPNN\nContext-aware protein sequence design\nRequired\n\n\n3\nRFdiffusion2\nAtom-level active site scaffolding\nRequired\n\n\n4\nESMFold\nSingle-sequence structure prediction\nRequired\n\n\n5\nOpenFold\nOpen-source AlphaFold2 reproduction\nRequired\n\n\n6\nChai-1\nMulti-modal biomolecular structure prediction\nRequired\n\n\n7\nBoltz-2\nStructure + binding affinity prediction\nRequired\n\n\n8\nDiffDock-PP\nProtein-protein docking\nRequired\n\n\n9\nPLACER\nProtein-ligand conformational ensemble prediction\nRequired\n\n\n10\nBindCraft\nEnd-to-end binder design pipeline\nRequired\n\n\n11\nESM3\nMultimodal protein generation\nOptional\n\n\n12\nRFdiffusion All Atom\nAll-atom protein design (predecessor to RFd2)\nOptional"
  },
  {
    "objectID": "monday/index.html#tips-for-success",
    "href": "monday/index.html#tips-for-success",
    "title": "Monday: Tool Installation",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nWork sequentially - Some tools share dependencies, so installing in order may help avoid conflicts\nUse separate conda environments - Each tool should have its own environment to avoid dependency conflicts\nCheck GPU availability - Most tools require GPU access; make sure you can request GPU nodes on your cluster\nNote your paths - Keep track of where you install each tool; you’ll need these paths later\nTest each installation - Don’t move on until you’ve verified each tool works"
  },
  {
    "objectID": "monday/index.html#getting-help",
    "href": "monday/index.html#getting-help",
    "title": "Monday: Tool Installation",
    "section": "Getting Help",
    "text": "Getting Help\nIf you encounter issues:\n\nCheck the tool’s official documentation (linked in each module)\nSearch for existing GitHub issues on the tool’s repository\nReport an issue on the bootcamp site\n\n\n\n\n← Back to Home\n\n\nTuesday →"
  },
  {
    "objectID": "monday/4-esmfold.html",
    "href": "monday/4-esmfold.html",
    "title": "4. ESMFold",
    "section": "",
    "text": "ESMFold (paper, code) is an end-to-end single sequence structure predictor that uses the ESM-2 language model to generate accurate 3D protein structures directly from sequence.\nESMFold is useful for fast, accurate protein structure prediction without requiring MSAs (though MSAs can be used to improve performance). It’s significantly faster than AlphaFold2 while maintaining competitive accuracy.\nThe goal of this module is to install ESMFold on your HPC cluster."
  },
  {
    "objectID": "monday/4-esmfold.html#preparation-work",
    "href": "monday/4-esmfold.html#preparation-work",
    "title": "4. ESMFold",
    "section": "Preparation Work",
    "text": "Preparation Work\n Mark as complete\nThis assignment assumes that you have access to a computing cluster that allows installation of new software. We’ll be creating virtual environments using Conda/Mamba.\nRequirements:\n\nPython ≤ 3.9\nPyTorch installed\nnvcc available (for compiling OpenFold dependencies)\nCUDA-capable GPU recommended"
  },
  {
    "objectID": "monday/4-esmfold.html#installation-steps",
    "href": "monday/4-esmfold.html#installation-steps",
    "title": "4. ESMFold",
    "section": "Installation Steps",
    "text": "Installation Steps\n Mark as complete\n\nCreate a conda environment with Python 3.9:\n\nmamba create -n esmfold python=3.9\nmamba activate esmfold\n\nInstall PyTorch (adjust CUDA version as needed):\n\nmamba install pytorch pytorch-cuda=12.1 -c pytorch -c nvidia\n\nInstall ESM with ESMFold dependencies:\n\npip install \"fair-esm[esmfold]\"\n\nInstall remaining OpenFold dependencies:\n\npip install 'dllogger @ git+https://github.com/NVIDIA/dllogger.git'\npip install 'openfold @ git+https://github.com/aqlaboratory/openfold.git@4b41059694619831a7db195b7e0988fc4ff3a307'\nNote: If OpenFold installation fails, verify that nvcc is available and a CUDA-compatible PyTorch is installed."
  },
  {
    "objectID": "monday/4-esmfold.html#alternative-use-conda-environment-file",
    "href": "monday/4-esmfold.html#alternative-use-conda-environment-file",
    "title": "4. ESMFold",
    "section": "Alternative: Use Conda Environment File",
    "text": "Alternative: Use Conda Environment File\nAlternatively, you can use a pre-configured environment:\nwget https://raw.githubusercontent.com/facebookresearch/esm/main/environment.yml\nmamba env create -f environment.yml\nmamba activate esmfold"
  },
  {
    "objectID": "monday/4-esmfold.html#testing-the-installation",
    "href": "monday/4-esmfold.html#testing-the-installation",
    "title": "4. ESMFold",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test script test_esmfold.py:\nimport torch\nimport esm\n\n# Load ESMFold model\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()  # Remove .cuda() if using CPU\n\n# Test sequence\nsequence = \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"\n\n# Run prediction\nwith torch.no_grad():\n    output = model.infer_pdb(sequence)\n\n# Save output\nwith open(\"test_result.pdb\", \"w\") as f:\n    f.write(output)\n\nprint(\"Structure prediction successful! Output saved to test_result.pdb\")\nRun the test:\npython test_esmfold.py\nIf this creates test_result.pdb, your installation was successful!"
  },
  {
    "objectID": "monday/4-esmfold.html#command-line-interface",
    "href": "monday/4-esmfold.html#command-line-interface",
    "title": "4. ESMFold",
    "section": "Command Line Interface",
    "text": "Command Line Interface\nESM also provides a command line tool for batch prediction:\nesm-fold -i sequences.fasta -o output_pdbs/\nAdditional options:\n\n--num-recycles: Number of recycles (default: 4)\n--max-tokens-per-batch: Batch shorter sequences together\n--chunk-size: Reduce memory usage (values: 128, 64, 32)\n--cpu-only: Run on CPU only\n--cpu-offload: Offload to CPU RAM for longer sequences"
  },
  {
    "objectID": "monday/4-esmfold.html#hpc-job-script-example",
    "href": "monday/4-esmfold.html#hpc-job-script-example",
    "title": "4. ESMFold",
    "section": "HPC Job Script Example",
    "text": "HPC Job Script Example\nFor SLURM clusters:\n#!/bin/bash\n#SBATCH --job-name=esmfold\n#SBATCH --gpus=1\n#SBATCH --time=04:00:00\n#SBATCH --mem=32G\n\nmodule load cuda/12.1\n\nmamba activate esmfold\n\n# Predict structures for all sequences in FASTA file\nesm-fold -i my_proteins.fasta -o predictions/ \\\n    --num-recycles 4 \\\n    --max-tokens-per-batch 1024"
  },
  {
    "objectID": "monday/4-esmfold.html#troubleshooting",
    "href": "monday/4-esmfold.html#troubleshooting",
    "title": "4. ESMFold",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nOpenFold installation fails: Double-check that nvcc is available:\nnvcc --version\nLoad CUDA modules if needed:\nmodule load cuda\nOut of memory errors: Use --chunk-size to reduce memory:\nesm-fold -i input.fasta -o output/ --chunk-size 64\nFor very long sequences: Use CPU offloading:\nesm-fold -i input.fasta -o output/ --cpu-offload\n\nNavigation: ← RFdiffusion2 | Monday Overview | Next: OpenFold →"
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html",
    "href": "monday/12-rfdiffusion-aa.html",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "",
    "text": "RFdiffusion All Atom (code) is the predecessor to RFdiffusion2 and enables all-atom protein design with small molecule binding. It can design protein binders to ligands with all-atom precision.\nNote: This tool is marked as OPTIONAL because RFdiffusion2 is the newer, more capable version. Install this only if you’re interested in exploring the earlier methodology or if specific features are needed.\nRFdiffusion All Atom is useful for designing small molecule binders and incorporating protein motifs in the design process.\nThe goal of this module is to install RFdiffusion All Atom on your HPC cluster (optional)."
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#preparation-work",
    "href": "monday/12-rfdiffusion-aa.html#preparation-work",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Preparation Work",
    "text": "Preparation Work\n Mark as complete\nThis assignment assumes that you have access to a computing cluster that allows installation of new software.\nIMPORTANT: Like RFdiffusion2, this uses Apptainer/Singularity containers, not Docker (which most academic HPCs don’t support)."
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#installation-steps",
    "href": "monday/12-rfdiffusion-aa.html#installation-steps",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Installation Steps",
    "text": "Installation Steps\n Mark as complete\n\nClone the repository:\n\ngit clone https://github.com/baker-laboratory/rf_diffusion_all_atom.git\ncd rf_diffusion_all_atom\n\nDownload the Singularity container:\n\nwget http://files.ipd.uw.edu/pub/RF-All-Atom/containers/rf_se3_diffusion.sif\n\nDownload the model weights:\n\nwget http://files.ipd.uw.edu/pub/RF-All-Atom/weights/RFDiffusionAA_paper_weights.pt\n\nInitialize git submodules:\n\ngit submodule init\ngit submodule update\n\nInstall Apptainer (if not already available):\n\nFor HPC clusters:\nmodule load apptainer\n# or\nmodule load singularity"
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#docker-vs-singularityapptainer-for-hpc",
    "href": "monday/12-rfdiffusion-aa.html#docker-vs-singularityapptainer-for-hpc",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Docker vs Singularity/Apptainer for HPC",
    "text": "Docker vs Singularity/Apptainer for HPC\nIMPORTANT: The official documentation uses Docker commands, but most academic HPCs do NOT support Docker. Replace Docker commands with Apptainer/Singularity:\n# ORIGINAL (Docker - won't work on most HPCs):\n# docker run --gpus all -v $(pwd):/workspace rf_se3_diffusion.sif ...\n\n# CORRECTED (Apptainer - works on HPCs):\napptainer run --nv rf_se3_diffusion.sif ..."
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#testing-the-installation",
    "href": "monday/12-rfdiffusion-aa.html#testing-the-installation",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a ligand binder design example:\napptainer run --nv rf_se3_diffusion.sif -u run_inference.py \\\n    inference.deterministic=True \\\n    diffuser.T=100 \\\n    inference.output_prefix=output/ligand_test/sample \\\n    inference.input_pdb=input/7v11.pdb \\\n    contigmap.contigs=\"['150-150']\" \\\n    inference.ligand=OQO \\\n    inference.num_designs=1 \\\n    inference.design_startnum=0\nNote: Omit --nv flag if running without GPU.\nExpected outputs:\n\noutput/ligand_test/sample_0.pdb - The design PDB\noutput/ligand_test/sample_0_Xt-1_traj.pdb - Denoising trajectory\noutput/ligand_test/sample_0_X0-1_traj.pdb - Predicted ground truth at each step"
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#hpc-job-script-example",
    "href": "monday/12-rfdiffusion-aa.html#hpc-job-script-example",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "HPC Job Script Example",
    "text": "HPC Job Script Example\nFor SLURM clusters:\n#!/bin/bash\n#SBATCH --job-name=rfdaa\n#SBATCH --gpus=1\n#SBATCH --time=02:00:00\n#SBATCH --mem=16G\n\nmodule load apptainer\nmodule load cuda/12.1\n\ncd /path/to/rf_diffusion_all_atom\n\napptainer run --nv rf_se3_diffusion.sif -u run_inference.py \\\n    inference.deterministic=True \\\n    diffuser.T=100 \\\n    inference.output_prefix=output/my_design/sample \\\n    inference.input_pdb=input/my_protein.pdb \\\n    contigmap.contigs=\"['150-150']\" \\\n    inference.ligand=HEM \\\n    inference.num_designs=10"
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#troubleshooting",
    "href": "monday/12-rfdiffusion-aa.html#troubleshooting",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nContainer not found: Ensure the .sif file is in the current directory or provide the full path.\nGPU errors: Make sure CUDA modules are loaded and GPU is available in your allocation.\n\nNavigation: ← ESM3 (Optional) | Monday Overview"
  },
  {
    "objectID": "monday/11-esm3.html",
    "href": "monday/11-esm3.html",
    "title": "11. ESM3 (Optional)",
    "section": "",
    "text": "ESM3 (paper, code) is a frontier generative model for biology that can jointly reason across three fundamental biological properties of proteins: sequence, structure, and function. It represents a multimodal generative masked language model.\nNote: This tool is marked as OPTIONAL as it’s primarily a research/generative model. Install if you’re interested in protein generation and design beyond structure prediction.\nESM3 is useful for protein generation, function prediction, and multimodal protein design tasks. It can generate novel proteins with desired properties.\nThe goal of this module is to install ESM3 on your HPC cluster (optional)."
  },
  {
    "objectID": "monday/11-esm3.html#preparation-work",
    "href": "monday/11-esm3.html#preparation-work",
    "title": "11. ESM3 (Optional)",
    "section": "Preparation Work",
    "text": "Preparation Work\n Mark as complete\nThis assignment assumes that you have access to a computing cluster that allows installation of new software. We’ll be creating virtual environments using Conda/Mamba."
  },
  {
    "objectID": "monday/11-esm3.html#installation-steps",
    "href": "monday/11-esm3.html#installation-steps",
    "title": "11. ESM3 (Optional)",
    "section": "Installation Steps",
    "text": "Installation Steps\n Mark as complete\n\nCreate a conda environment:\n\nmamba create -n esm3 python=3.10\nmamba activate esm3\n\nInstall the ESM library:\n\npip install esm"
  },
  {
    "objectID": "monday/11-esm3.html#huggingface-authentication",
    "href": "monday/11-esm3.html#huggingface-authentication",
    "title": "11. ESM3 (Optional)",
    "section": "HuggingFace Authentication",
    "text": "HuggingFace Authentication\nESM3 weights are stored on HuggingFace Hub. You’ll need to authenticate:\n\nCreate a HuggingFace account at huggingface.co\nGenerate an API token with “Read” permission\nAuthenticate:\n\nfrom huggingface_hub import login\nlogin()  # Follow prompts to enter your token"
  },
  {
    "objectID": "monday/11-esm3.html#testing-the-installation",
    "href": "monday/11-esm3.html#testing-the-installation",
    "title": "11. ESM3 (Optional)",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test script test_esm3.py:\nfrom huggingface_hub import login\nfrom esm.models.esm3 import ESM3\nfrom esm.sdk.api import ESMProtein, GenerationConfig\n\n# Authenticate (first time only)\nlogin()\n\n# Load the model (this downloads weights on first run)\nmodel = ESM3.from_pretrained(\"esm3-small-2024-08\").to(\"cuda\")  # or \"cpu\"\n\n# Generate a protein sequence completion\nprompt = \"MKTVRQ_______________QLAEELSVSRQVIVQDIAYLRSLG\"\nprotein = ESMProtein(sequence=prompt)\n\n# Generate sequence\nprotein = model.generate(\n    protein,\n    GenerationConfig(track=\"sequence\", num_steps=8, temperature=0.7)\n)\n\nprint(\"Generated sequence:\")\nprint(protein.sequence)\n\n# Generate structure\nprotein = model.generate(\n    protein,\n    GenerationConfig(track=\"structure\", num_steps=8)\n)\n\n# Save structure\nprotein.to_pdb(\"./generated.pdb\")\nprint(\"Structure saved to generated.pdb\")\nRun the test:\npython test_esm3.py"
  },
  {
    "objectID": "monday/11-esm3.html#available-esm3-models",
    "href": "monday/11-esm3.html#available-esm3-models",
    "title": "11. ESM3 (Optional)",
    "section": "Available ESM3 Models",
    "text": "Available ESM3 Models\n\nesm3-small-2024-08 (1.4B parameters) - Fastest, runs locally\nesm3-medium-2024-08 (7B parameters) - Via Forge API\nesm3-large-2024-03 (98B parameters) - Via Forge API"
  },
  {
    "objectID": "monday/11-esm3.html#esm-c-for-embeddings",
    "href": "monday/11-esm3.html#esm-c-for-embeddings",
    "title": "11. ESM3 (Optional)",
    "section": "ESM C for Embeddings",
    "text": "ESM C for Embeddings\nIf you only need protein embeddings (not generation), use ESM C instead:\nfrom esm.models.esmc import ESMC\nfrom esm.sdk.api import ESMProtein, LogitsConfig\n\nprotein = ESMProtein(sequence=\"MKTVRQERLK\")\nclient = ESMC.from_pretrained(\"esmc_300m\").to(\"cuda\")\n\n# Get embeddings\nprotein_tensor = client.encode(protein)\nlogits_output = client.logits(\n    protein_tensor,\n    LogitsConfig(sequence=True, return_embeddings=True)\n)\n\nprint(logits_output.embeddings)"
  },
  {
    "objectID": "monday/11-esm3.html#hpc-job-script-example",
    "href": "monday/11-esm3.html#hpc-job-script-example",
    "title": "11. ESM3 (Optional)",
    "section": "HPC Job Script Example",
    "text": "HPC Job Script Example\nFor SLURM clusters:\n#!/bin/bash\n#SBATCH --job-name=esm3\n#SBATCH --gpus=1\n#SBATCH --time=02:00:00\n#SBATCH --mem=32G\n\nmodule load cuda/12.1\n\nmamba activate esm3\n\npython generate_protein.py"
  },
  {
    "objectID": "monday/11-esm3.html#use-cases",
    "href": "monday/11-esm3.html#use-cases",
    "title": "11. ESM3 (Optional)",
    "section": "Use Cases",
    "text": "Use Cases\n\nProtein generation: Create novel proteins with desired properties\nSequence completion: Fill in missing regions\nStructure prediction: Generate 3D structures from sequence\nFunction prediction: Predict protein function from sequence/structure\nProtein embeddings: Extract representations (use ESM C)"
  },
  {
    "objectID": "monday/11-esm3.html#troubleshooting",
    "href": "monday/11-esm3.html#troubleshooting",
    "title": "11. ESM3 (Optional)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nAuthentication errors: Ensure you’ve created a HuggingFace token and run login().\nDownload issues: Model weights are large. Ensure adequate storage and network.\nGPU memory: The small model (1.4B) requires significant GPU memory. Use CPU if needed.\n\nNavigation: ← BindCraft | Monday Overview | Next: RFdiffusion All Atom (Optional) →"
  },
  {
    "objectID": "monday/1-localcolabfold.html",
    "href": "monday/1-localcolabfold.html",
    "title": "1. LocalColabFold",
    "section": "",
    "text": "LocalColabFold (code) is a local installation of ColabFold, which provides an efficient implementation of AlphaFold2 protein structure prediction. ColabFold combines fast MSA generation from MMseqs2 with AlphaFold2’s structure prediction capabilities, making it significantly faster than the original AlphaFold2 implementation.\nLocalColabFold is useful for high-throughput protein structure prediction on your own HPC infrastructure without relying on Google Colab. It enables batch processing of structure predictions without time limits or GPU restrictions.\nThe goal of this module is to install LocalColabFold on your HPC cluster."
  },
  {
    "objectID": "monday/1-localcolabfold.html#preparation-work",
    "href": "monday/1-localcolabfold.html#preparation-work",
    "title": "1. LocalColabFold",
    "section": "Preparation Work",
    "text": "Preparation Work\n Mark as complete\nThis assignment assumes that you have access to a computing cluster that allows installation of new software.\nImportant: Make sure you have access to a GPU node for testing, as AlphaFold2 requires GPUs for efficient structure prediction."
  },
  {
    "objectID": "monday/1-localcolabfold.html#installation-steps",
    "href": "monday/1-localcolabfold.html#installation-steps",
    "title": "1. LocalColabFold",
    "section": "Installation Steps",
    "text": "Installation Steps\n Mark as complete\n\nDownload the installation script:\n\nwget https://raw.githubusercontent.com/YoshitakaMo/localcolabfold/main/install_colabbatch_linux.sh\n\nMake the script executable and run it:\n\nchmod +x install_colabbatch_linux.sh\n./install_colabbatch_linux.sh\nThis will create a directory called localcolabfold with a conda environment called colabfold_batch.\nThe script will automatically:\n\nInstall Miniconda (if not already present)\nCreate a conda environment with ColabFold and all dependencies\nDownload necessary model weights (~10-15 GB)\n\n\nAdd the environment to your PATH (add to your ~/.bashrc for permanent access):\n\nexport PATH=\"/path/to/your/localcolabfold/colabfold-conda/bin:$PATH\""
  },
  {
    "objectID": "monday/1-localcolabfold.html#testing-the-installation",
    "href": "monday/1-localcolabfold.html#testing-the-installation",
    "title": "1. LocalColabFold",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\n\nActivate the ColabFold environment:\n\nsource localcolabfold/colabfold-conda/bin/activate\n\nCreate a test FASTA file with a small protein sequence:\n\necho \"&gt;test_protein\nMKFLKFSLLTAVLLSVVFAFSSCGDDDDTYPYDVPDYAGTCGDDDDTYPYDVPDYA\" &gt; test.fasta\n\nRun ColabFold on the test sequence:\n\ncolabfold_batch test.fasta test_output/\nIf the command completes successfully and creates PDB files in test_output/, your installation was successful!"
  },
  {
    "objectID": "monday/1-localcolabfold.html#hpc-job-submission",
    "href": "monday/1-localcolabfold.html#hpc-job-submission",
    "title": "1. LocalColabFold",
    "section": "HPC Job Submission",
    "text": "HPC Job Submission\nFor HPC clusters using SLURM, create a job script like this:\n#!/bin/bash\n#SBATCH --job-name=colabfold\n#SBATCH --gpus=1\n#SBATCH --time=04:00:00\n#SBATCH --mem=32G\n\nsource /path/to/localcolabfold/colabfold-conda/bin/activate\n\ncolabfold_batch input.fasta output_dir/"
  },
  {
    "objectID": "monday/1-localcolabfold.html#troubleshooting",
    "href": "monday/1-localcolabfold.html#troubleshooting",
    "title": "1. LocalColabFold",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nDatabase Location: By default, ColabFold downloads databases to ~/.cache/colabfold/. On shared clusters, you may want to set COLABFOLD_DOWNLOAD_DIR to a shared location to avoid duplicate downloads:\nexport COLABFOLD_DOWNLOAD_DIR=/shared/path/colabfold_db\n\nNavigation: ← Monday Overview | Next: LigandMPNN →"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This bootcamp covers machine learning tools for protein structure prediction and design.\nMore information coming soon."
  },
  {
    "objectID": "tuesday/2.1-placeholder.html",
    "href": "tuesday/2.1-placeholder.html",
    "title": "2.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon."
  },
  {
    "objectID": "tuesday/2.1-placeholder.html#learning-objectives",
    "href": "tuesday/2.1-placeholder.html#learning-objectives",
    "title": "2.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon."
  },
  {
    "objectID": "tuesday/2.1-placeholder.html#section-1",
    "href": "tuesday/2.1-placeholder.html#section-1",
    "title": "2.1 Placeholder Module",
    "section": "Section 1",
    "text": "Section 1\n Mark Section 1 as complete\nPlaceholder content for Section 1."
  },
  {
    "objectID": "tuesday/2.1-placeholder.html#section-2",
    "href": "tuesday/2.1-placeholder.html#section-2",
    "title": "2.1 Placeholder Module",
    "section": "Section 2",
    "text": "Section 2\n Mark Section 2 as complete\nPlaceholder content for Section 2.\n\nNavigation: ← Tuesday Overview | Next Module →"
  },
  {
    "objectID": "thursday/4.1-placeholder.html",
    "href": "thursday/4.1-placeholder.html",
    "title": "4.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon."
  },
  {
    "objectID": "thursday/4.1-placeholder.html#learning-objectives",
    "href": "thursday/4.1-placeholder.html#learning-objectives",
    "title": "4.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon."
  },
  {
    "objectID": "thursday/4.1-placeholder.html#section-1",
    "href": "thursday/4.1-placeholder.html#section-1",
    "title": "4.1 Placeholder Module",
    "section": "Section 1",
    "text": "Section 1\n Mark Section 1 as complete\nPlaceholder content for Section 1."
  },
  {
    "objectID": "thursday/4.1-placeholder.html#section-2",
    "href": "thursday/4.1-placeholder.html#section-2",
    "title": "4.1 Placeholder Module",
    "section": "Section 2",
    "text": "Section 2\n Mark Section 2 as complete\nPlaceholder content for Section 2.\n\nNavigation: ← Thursday Overview | Next Module →"
  },
  {
    "objectID": "wednesday/3.1-placeholder.html",
    "href": "wednesday/3.1-placeholder.html",
    "title": "3.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon."
  },
  {
    "objectID": "wednesday/3.1-placeholder.html#learning-objectives",
    "href": "wednesday/3.1-placeholder.html#learning-objectives",
    "title": "3.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon."
  },
  {
    "objectID": "wednesday/3.1-placeholder.html#section-1",
    "href": "wednesday/3.1-placeholder.html#section-1",
    "title": "3.1 Placeholder Module",
    "section": "Section 1",
    "text": "Section 1\n Mark Section 1 as complete\nPlaceholder content for Section 1."
  },
  {
    "objectID": "wednesday/3.1-placeholder.html#section-2",
    "href": "wednesday/3.1-placeholder.html#section-2",
    "title": "3.1 Placeholder Module",
    "section": "Section 2",
    "text": "Section 2\n Mark Section 2 as complete\nPlaceholder content for Section 2.\n\nNavigation: ← Wednesday Overview | Next Module →"
  }
]