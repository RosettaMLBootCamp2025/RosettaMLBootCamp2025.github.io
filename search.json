[
  {
    "objectID": "thursday/4.1-placeholder.html",
    "href": "thursday/4.1-placeholder.html",
    "title": "4.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon.",
    "crumbs": [
      "Thursday",
      "4.1 Placeholder Module"
    ]
  },
  {
    "objectID": "thursday/4.1-placeholder.html#learning-objectives",
    "href": "thursday/4.1-placeholder.html#learning-objectives",
    "title": "4.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon.",
    "crumbs": [
      "Thursday",
      "4.1 Placeholder Module"
    ]
  },
  {
    "objectID": "thursday/4.1-placeholder.html#section-1",
    "href": "thursday/4.1-placeholder.html#section-1",
    "title": "4.1 Placeholder Module",
    "section": "Section 1",
    "text": "Section 1\n Mark Section 1 as complete\nPlaceholder content for Section 1.",
    "crumbs": [
      "Thursday",
      "4.1 Placeholder Module"
    ]
  },
  {
    "objectID": "thursday/4.1-placeholder.html#section-2",
    "href": "thursday/4.1-placeholder.html#section-2",
    "title": "4.1 Placeholder Module",
    "section": "Section 2",
    "text": "Section 2\n Mark Section 2 as complete\nPlaceholder content for Section 2.",
    "crumbs": [
      "Thursday",
      "4.1 Placeholder Module"
    ]
  },
  {
    "objectID": "capstone/index.html",
    "href": "capstone/index.html",
    "title": "Capstone",
    "section": "",
    "text": "Capstone project details coming soon.\nThe capstone project brings together everything you‚Äôve learned throughout the bootcamp.",
    "crumbs": [
      "Capstone"
    ]
  },
  {
    "objectID": "capstone/index.html#overview",
    "href": "capstone/index.html#overview",
    "title": "Capstone",
    "section": "",
    "text": "Capstone project details coming soon.\nThe capstone project brings together everything you‚Äôve learned throughout the bootcamp.",
    "crumbs": [
      "Capstone"
    ]
  },
  {
    "objectID": "capstone/index.html#project-description",
    "href": "capstone/index.html#project-description",
    "title": "Capstone",
    "section": "Project Description",
    "text": "Project Description\nComing soon.\n\n\n\n‚Üê Thursday\n\n\nBack to Home",
    "crumbs": [
      "Capstone"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html",
    "href": "monday/2-ligandmpnn.html",
    "title": "2. LigandMPNN",
    "section": "",
    "text": "LigandMPNN (paper, code) is a deep learning model for context-aware protein sequence design. It extends ProteinMPNN to handle small molecules, metal ions, and other non-protein components in protein design tasks.",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#why-use-ligandmpnn",
    "href": "monday/2-ligandmpnn.html#why-use-ligandmpnn",
    "title": "2. LigandMPNN",
    "section": "Why Use LigandMPNN?",
    "text": "Why Use LigandMPNN?\n\nLigand-aware design: Design sequences that account for bound cofactors, substrates, or drug molecules\nContext preservation: Maintain interactions with metals, DNA, RNA, or other molecules\nSide chain packing: Evaluate and optimize side chain conformations\nFlexible residue control: Fix, bias, or vary specific positions\n\nRelated Tools: Use with RFdiffusion2 for backbone design, or BindCraft for complete binder design pipelines.",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#resource-requirements",
    "href": "monday/2-ligandmpnn.html#resource-requirements",
    "title": "2. LigandMPNN",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n4 GB\n16 GB\nScales with protein size\n\n\nCPU RAM\n8 GB\n16 GB\nCPU-only is viable but slower\n\n\nDisk Space\n2 GB\n5 GB\nModel weights\n\n\nPython\n3.9+\n3.11\nRequired",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#preparation",
    "href": "monday/2-ligandmpnn.html#preparation",
    "title": "2. LigandMPNN",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nGit installed\n\nVerify your environment:\npython --version    # Should be 3.9+\nnvcc --version      # For GPU support (optional)",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#installation",
    "href": "monday/2-ligandmpnn.html#installation",
    "title": "2. LigandMPNN",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the LigandMPNN repository:\n\ngit clone https://github.com/dauparas/LigandMPNN.git\ncd LigandMPNN\n\nDownload the model parameters: Note: This step requires internet access. If your compute node doesn‚Äôt have internet, run this on a login node.\n\nbash get_model_params.sh \"./model_params\"\nExpected download: ~500 MB of model weights.\n\nCreate a new conda environment:\n\nmamba create -n ligandmpnn_env python=3.11\nmamba activate ligandmpnn_env\n\nInstall dependencies:\n\npip install -r requirements.txt\nThis installs PyTorch, NumPy, and ProDy for PDB file handling.",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#testing-the-installation",
    "href": "monday/2-ligandmpnn.html#testing-the-installation",
    "title": "2. LigandMPNN",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a test design on the provided example structure:\npython run.py \\\n    --seed 111 \\\n    --pdb_path \"./inputs/1BC8.pdb\" \\\n    --out_folder \"./outputs/test_output\"\nSuccess indicators:\n\nCommand completes without errors\nOutput folder contains:\n\nseqs/1BC8.fa - Designed sequences in FASTA format\nbackbones/1BC8.pdb - Input backbone (for reference)\npacked/1BC8_1.pdb - Structure with designed side chains\n\n\nExpected runtime: &lt;1 minute on GPU, ~5 minutes on CPU.",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#hpc-job-script",
    "href": "monday/2-ligandmpnn.html#hpc-job-script",
    "title": "2. LigandMPNN",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=ligandmpnn\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc  # Optional: Source shell profile if needed\nmamba activate ligandmpnn_env\n\ncd /path/to/LigandMPNN\n\npython run.py \\\n    --model_type \"ligand_mpnn\" \\\n    --seed 111 \\\n    --pdb_path \"./inputs/my_protein.pdb\" \\\n    --out_folder \"./outputs/my_design\" \\\n    --number_of_batches 10",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#usage-examples",
    "href": "monday/2-ligandmpnn.html#usage-examples",
    "title": "2. LigandMPNN",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic protein design (no ligand):\npython run.py \\\n    --pdb_path \"protein.pdb\" \\\n    --out_folder \"output/\"\nDesign with ligand context:\npython run.py \\\n    --model_type \"ligand_mpnn\" \\\n    --pdb_path \"protein_ligand.pdb\" \\\n    --out_folder \"output/\"\nFix specific residues (keep them unchanged):\npython run.py \\\n    --pdb_path \"protein.pdb\" \\\n    --fixed_residues \"A10 A20 A30\" \\\n    --out_folder \"output/\"\nDesign only specific positions:\npython run.py \\\n    --pdb_path \"protein.pdb\" \\\n    --redesigned_residues \"A50 A51 A52 A53\" \\\n    --out_folder \"output/\"\nBatch processing multiple structures:\n# Create a JSON file listing inputs\necho '{\"1\": \"input1.pdb\", \"2\": \"input2.pdb\"}' &gt; input_list.json\n\npython run.py \\\n    --pdb_path_multi \"input_list.json\" \\\n    --out_folder \"batch_output/\"\nWith temperature control (higher = more diverse):\npython run.py \\\n    --pdb_path \"protein.pdb\" \\\n    --temperature 0.2 \\\n    --out_folder \"output/\"",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#key-parameters",
    "href": "monday/2-ligandmpnn.html#key-parameters",
    "title": "2. LigandMPNN",
    "section": "Key Parameters",
    "text": "Key Parameters\n\n\n\n\n\n\n\n\nParameter\nDescription\nDefault\n\n\n\n\n--model_type\nModel variant: protein_mpnn, ligand_mpnn, soluble_mpnn, etc.\nprotein_mpnn\n\n\n--temperature\nSampling temperature (0.1-1.0). Lower = more conservative\n0.1\n\n\n--number_of_batches\nNumber of sequences to generate\n1\n\n\n--batch_size\nSequences per batch\n1\n\n\n--fixed_residues\nSpace-separated residues to keep unchanged\nNone\n\n\n--redesigned_residues\nOnly design these residues\nAll\n\n\n--bias_AA\nBias toward specific amino acids\nNone",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#model-types",
    "href": "monday/2-ligandmpnn.html#model-types",
    "title": "2. LigandMPNN",
    "section": "Model Types",
    "text": "Model Types\n\n\n\nModel\nUse Case\n\n\n\n\nprotein_mpnn\nStandard protein sequence design\n\n\nligand_mpnn\nDesign with small molecule context\n\n\nsoluble_mpnn\nBias toward soluble sequences\n\n\nglobal_label_membrane_mpnn\nMembrane protein design\n\n\nper_residue_label_membrane_mpnn\nFine-grained membrane design",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#understanding-the-output",
    "href": "monday/2-ligandmpnn.html#understanding-the-output",
    "title": "2. LigandMPNN",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\noutput/\n‚îú‚îÄ‚îÄ seqs/\n‚îÇ   ‚îî‚îÄ‚îÄ protein.fa          # Designed sequences\n‚îú‚îÄ‚îÄ backbones/\n‚îÇ   ‚îî‚îÄ‚îÄ protein.pdb         # Input structure\n‚îî‚îÄ‚îÄ packed/\n    ‚îú‚îÄ‚îÄ protein_1.pdb       # Design 1 with side chains\n    ‚îî‚îÄ‚îÄ protein_2.pdb       # Design 2 with side chains\nFASTA output format:\n&gt;protein, score=1.234, seq_recovery=0.456\nMVKLTAEGSE...\n\nscore: Negative log-likelihood (lower = better fit to backbone)\nseq_recovery: Fraction matching native sequence (if provided)",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#troubleshooting",
    "href": "monday/2-ligandmpnn.html#troubleshooting",
    "title": "2. LigandMPNN",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n‚ÄúRuntimeError: CUDA out of memory‚Äù:\n\nUse CPU instead: remove CUDA module and run without GPU\nReduce --batch_size\nLigandMPNN is efficient; usually not memory-limited\n\nPDB parsing errors:\n\nEnsure PDB has proper formatting\nRemove alternate conformations: keep only ‚ÄúA‚Äù conformers\nCheck that ligand has proper atom naming\n\nLigand not recognized:\n\nEnsure ligand is in the PDB file with HETATM records\nUse --ligand flag to specify ligand residue name\nCheck that ligand coordinates are reasonable\n\nLow sequence diversity:\n\nIncrease --temperature (e.g., 0.2 or 0.3)\nIncrease --number_of_batches\nUse different random seeds\n\nSide chain clashes in output:\n\nThis is expected - downstream relaxation is recommended\nUse PyRosetta or Rosetta FastRelax\nOr validate with your structure prediction tool of choice",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html",
    "href": "monday/10-bindcraft.html",
    "title": "10. BindCraft",
    "section": "",
    "text": "BindCraft (paper, code) is an end-to-end binder design pipeline that combines AlphaFold2 backpropagation, ProteinMPNN, and PyRosetta to design protein binders against target proteins.",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#why-use-bindcraft",
    "href": "monday/10-bindcraft.html#why-use-bindcraft",
    "title": "10. BindCraft",
    "section": "Why Use BindCraft?",
    "text": "Why Use BindCraft?\n\nComplete pipeline: Integrates structure prediction, sequence design, and scoring\nAutomated optimization: Multi-stage design with confidence-based filtering\nProduction ready: Validated binders in published work\nLearning resource: See how professional protein design pipelines work\n\nRelated Tools: For backbone design, see RFdiffusion2. For sequence design, see LigandMPNN. For structure prediction, see LocalColabFold.",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#resource-requirements",
    "href": "monday/10-bindcraft.html#resource-requirements",
    "title": "10. BindCraft",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n24 GB\n32+ GB\nLarge targets need more\n\n\nCPU RAM\n32 GB\n64 GB\nFor PyRosetta scoring\n\n\nDisk Space\n2 MB + 5.3 GB\n10 GB\nCode + AlphaFold2 weights\n\n\nTime\nHours\nDays\nPer design campaign\n\n\n\nImportant: BindCraft requires PyRosetta. Ensure you have a valid installation or license if required by your institution.",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#preparation",
    "href": "monday/10-bindcraft.html#preparation",
    "title": "10. BindCraft",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nPyRosetta installed (or accessible via license)\nCUDA-compatible GPU\n\nCheck your CUDA version:\nnvcc --version\n# Note the version number (e.g., 12.4)",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#installation",
    "href": "monday/10-bindcraft.html#installation",
    "title": "10. BindCraft",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the BindCraft repository:\n\ngit clone https://github.com/martinpacesa/BindCraft /path/to/bindcraft\ncd /path/to/bindcraft\n\nRun the installation script:\n\nbash install_bindcraft.sh --cuda '12.4' --pkg_manager 'conda'\nImportant options:\n\nReplace 12.4 with your actual CUDA version\nUse --pkg_manager 'mamba' for faster installation\nIf --cuda is left blank, auto-detection may fail\n\nExpected time: 20-40 minutes.\nThe script creates a conda environment called BindCraft with all dependencies.",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#testing-the-installation",
    "href": "monday/10-bindcraft.html#testing-the-installation",
    "title": "10. BindCraft",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\n\nActivate the environment:\n\nconda activate BindCraft\n\nRun a test design against the example target (PDL1):\n\ncd /path/to/bindcraft\npython -u ./bindcraft.py \\\n    --settings './settings_target/PDL1.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'\nSuccess indicators:\n\nStarts generating trajectories without errors\nLog shows design iterations progressing\nCreates output directory with design files\n\nNote: A complete run takes hours to days. For testing, stop after a few trajectories complete (Ctrl+C).",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#hpc-job-script",
    "href": "monday/10-bindcraft.html#hpc-job-script",
    "title": "10. BindCraft",
    "section": "HPC Job Script",
    "text": "HPC Job Script\nUsing the provided template:\nsbatch ./bindcraft.slurm \\\n    --settings './settings_target/PDL1.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'\nOr create your own:\n#!/bin/bash\n#SBATCH --job-name=bindcraft\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --time=48:00:00\n#SBATCH --output=%x_%j.out\n\n# source ~/.bashrc\nconda activate BindCraft\n\ncd /path/to/bindcraft\n\npython -u ./bindcraft.py \\\n    --settings './settings_target/my_target.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#setting-up-your-own-target",
    "href": "monday/10-bindcraft.html#setting-up-your-own-target",
    "title": "10. BindCraft",
    "section": "Setting Up Your Own Target",
    "text": "Setting Up Your Own Target\nStep 1: Prepare your target PDB\n\nPlace your target protein PDB in the BindCraft folder\nTrim unnecessary chains/residues to reduce memory and speed up design\n\nStep 2: Create target settings (settings_target/my_target.json):\n{\n    \"design_path\": \"./my_binder_designs\",\n    \"binder_name\": \"my_binder\",\n    \"starting_pdb\": \"./my_target.pdb\",\n    \"chains\": \"A\",\n    \"target_hotspot_residues\": \"A10-20\",\n    \"lengths\": \"50-100\",\n    \"number_of_final_designs\": 100\n}\nStep 3: Run the pipeline:\npython -u ./bindcraft.py \\\n    --settings './settings_target/my_target.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#key-settings-explained",
    "href": "monday/10-bindcraft.html#key-settings-explained",
    "title": "10. BindCraft",
    "section": "Key Settings Explained",
    "text": "Key Settings Explained\n\nTarget Settings (settings_target/*.json)\n\n\n\n\n\n\n\n\nSetting\nDescription\nExample\n\n\n\n\nstarting_pdb\nPath to target structure\n\"./my_target.pdb\"\n\n\nchains\nWhich chain(s) to target\n\"A\" or \"A,B\"\n\n\ntarget_hotspot_residues\nResidues to target\n\"A10-20\" or null (auto)\n\n\nlengths\nBinder length range\n\"50-100\"\n\n\nnumber_of_final_designs\nDesigns to generate\n100\n\n\n\n\n\nFilter Settings (settings_filters/*.json)\nControls which designs pass quality thresholds:\n\nConfidence scores (pLDDT, pTM, i_pTM)\nInterface quality (shape complementarity, energy)\nDefault filters are good starting points\n\n\n\nAdvanced Settings (settings_advanced/*.json)\n\nDesign algorithm (default: 4-stage)\nNumber of iterations per stage\nAlphaFold2 and ProteinMPNN parameters",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#understanding-the-pipeline",
    "href": "monday/10-bindcraft.html#understanding-the-pipeline",
    "title": "10. BindCraft",
    "section": "Understanding the Pipeline",
    "text": "Understanding the Pipeline\nBindCraft demonstrates a complete protein design workflow:\n1. DESIGN        ‚Üí AlphaFold2 backpropagation generates binder backbones\n       ‚Üì\n2. OPTIMIZE      ‚Üí ProteinMPNN designs sequences for backbones\n       ‚Üì\n3. VALIDATE      ‚Üí AlphaFold2 predicts designed complex structure\n       ‚Üì\n4. SCORE         ‚Üí PyRosetta evaluates interface quality\n       ‚Üì\n5. FILTER        ‚Üí Keep designs passing confidence thresholds",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#tips-for-success",
    "href": "monday/10-bindcraft.html#tips-for-success",
    "title": "10. BindCraft",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nTrim your target: Remove unnecessary chains/residues to reduce memory\nStart with defaults: Use default filter and advanced settings initially\nGenerate enough designs: Aim for 100+ final designs (top 5-20 for experiments)\nBe patient: Expect hundreds to thousands of trajectories for enough accepted binders\nMonitor acceptance rate: Low acceptance ‚Üí adjust design weights or filters\nCheck the wiki: BindCraft Wiki",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#understanding-the-output",
    "href": "monday/10-bindcraft.html#understanding-the-output",
    "title": "10. BindCraft",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\nmy_binder_designs/\n‚îú‚îÄ‚îÄ accepted/\n‚îÇ   ‚îú‚îÄ‚îÄ design_001.pdb        # Passing designs\n‚îÇ   ‚îú‚îÄ‚îÄ design_002.pdb\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ rejected/                  # Filtered out designs\n‚îú‚îÄ‚îÄ trajectories/              # All generated trajectories\n‚îú‚îÄ‚îÄ scores.csv                 # All metrics for each design\n‚îî‚îÄ‚îÄ summary.txt                # Run statistics",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#troubleshooting",
    "href": "monday/10-bindcraft.html#troubleshooting",
    "title": "10. BindCraft",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nGPU memory errors:\n\nReduce target PDB size (trim chains)\nRequest more GPU memory (32+ GB recommended)\nCheck with: nvidia-smi\n\nCUDA version mismatch:\n\nRe-run install with correct CUDA version\nCheck: nvcc --version\n\nLow acceptance rate (few designs pass filters):\n\nAdjust design weights in advanced settings\nRelax filter thresholds\nChange target hotspot selection\nIncrease target site area\n\nPyRosetta license errors:\n\nVerify PyRosetta license is valid\nCheck license file location\nContact PyRosetta for academic license\n\nSlow progress:\n\nThis is normal - protein design takes time\nMonitor trajectory count, not clock time\nLarge targets are slower",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html",
    "href": "monday/8-diffdock-pp.html",
    "title": "8. DiffDock-PP",
    "section": "",
    "text": "DiffDock-PP (paper, code) is a graph neural network trained for de-noising of rigid transformations (rotation and translation) to predict protein-protein docking orientations between two rigid protein subunits.",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#why-use-diffdock-pp",
    "href": "monday/8-diffdock-pp.html#why-use-diffdock-pp",
    "title": "8. DiffDock-PP",
    "section": "Why Use DiffDock-PP?",
    "text": "Why Use DiffDock-PP?\n\nFast protein-protein docking: Predicts binding orientations without expensive sampling\nValidation tool: Orthogonally validate structure predictions from other methods\nEnsemble predictions: Generate multiple docking poses for uncertainty estimation\nRigid-body docking: Efficient for cases where backbone flexibility is minimal\n\nRelated Tools: For protein-ligand docking, see PLACER. For flexible ligand binding with ensemble generation, see PLACER. For structure prediction of complexes, see Chai-1 or Boltz-2.",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#resource-requirements",
    "href": "monday/8-diffdock-pp.html#resource-requirements",
    "title": "8. DiffDock-PP",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n8 GB\n16 GB\nScales with protein size\n\n\nCPU RAM\n8 GB\n16 GB\nFor preprocessing\n\n\nDisk Space\n2 GB\n5 GB\nModel weights\n\n\nCUDA\n11.6\n11.6-11.7\nSpecific version required",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#preparation",
    "href": "monday/8-diffdock-pp.html#preparation",
    "title": "8. DiffDock-PP",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nCUDA 11.6 or 11.7 available\n\nCheck CUDA availability:\nmodule avail cuda\n# Look for cuda/11.6 or cuda/11.7",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#installation",
    "href": "monday/8-diffdock-pp.html#installation",
    "title": "8. DiffDock-PP",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the DiffDock-PP repository:\n\ngit clone https://github.com/ketatam/DiffDock-PP.git\ncd DiffDock-PP\n\nCreate a new environment:\n\nmamba create -n diffdock_pp python=3.9\nmamba activate diffdock_pp\n\nInstall PyTorch with CUDA 11.6:\n\nmamba install pytorch=1.13.0 pytorch-cuda=11.6 -c pytorch -c nvidia\nWhy CUDA 11.6? DiffDock-PP was developed and tested with this version. Using different versions may cause compatibility issues with PyG.\n\nInstall PyTorch Geometric (PyG) packages:\n\nmamba install pytorch-scatter pytorch-sparse pytorch-cluster pytorch-spline-conv pyg -c pyg\n\nInstall remaining dependencies:\n\nmamba install mkl=2024.0 \"numpy&lt;2.0\" dill tqdm pyyaml pandas biopandas scikit-learn biopython e3nn wandb tensorboard tensorboardX matplotlib\nWhy numpy&lt;2.0? NumPy 2.0 introduced breaking changes that affect many scientific packages. Keeping NumPy below 2.0 ensures compatibility.",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#testing-the-installation",
    "href": "monday/8-diffdock-pp.html#testing-the-installation",
    "title": "8. DiffDock-PP",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\n\nCreate required directories:\n\nmkdir storage\n\nRun the test script on the DB5 benchmark:\n\nbash src/db5_inference.sh\nSuccess indicators:\n\nCommand completes without errors\nOutput folder visualization/epoch-0/ is created\nDirectory contains PDB files of docked complexes (multiple .pdb files)\n\nExpected runtime: 5-15 minutes depending on GPU.\nVerify output:\nls visualization/epoch-0/*.pdb | wc -l\n# Should show multiple PDB files",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#hpc-job-script",
    "href": "monday/8-diffdock-pp.html#hpc-job-script",
    "title": "8. DiffDock-PP",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=diffdock_pp\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/11.6\n\n# source ~/.bashrc\nmamba activate diffdock_pp\n\ncd /path/to/DiffDock-PP\n\n# Create output directory\nmkdir -p storage\n\n# Run inference\nbash src/db5_inference.sh",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#usage-examples",
    "href": "monday/8-diffdock-pp.html#usage-examples",
    "title": "8. DiffDock-PP",
    "section": "Usage Examples",
    "text": "Usage Examples\nRun DB5 benchmark (default test):\nbash src/db5_inference.sh\nCustom docking (requires understanding the codebase):\nDiffDock-PP requires input data in a specific format. For custom proteins:\n\nPrepare receptor and ligand PDB files\nCreate data configuration files\nRun inference script\n\nSee the repository documentation for detailed input format requirements.",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#understanding-the-output",
    "href": "monday/8-diffdock-pp.html#understanding-the-output",
    "title": "8. DiffDock-PP",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput structure:\nvisualization/\n‚îî‚îÄ‚îÄ epoch-0/\n    ‚îú‚îÄ‚îÄ complex_1_pose_0.pdb    # Docking pose 1\n    ‚îú‚îÄ‚îÄ complex_1_pose_1.pdb    # Docking pose 2\n    ‚îú‚îÄ‚îÄ complex_1_pose_2.pdb    # Docking pose 3\n    ‚îî‚îÄ‚îÄ ...                      # More poses\nEach PDB file contains:\n\nBoth protein chains with predicted relative orientation\nMultiple poses represent different docking predictions\nCompare poses to assess uncertainty",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#use-cases",
    "href": "monday/8-diffdock-pp.html#use-cases",
    "title": "8. DiffDock-PP",
    "section": "Use Cases",
    "text": "Use Cases\n\nProtein-Protein Docking: Predict binding orientations between protein chains\nComplex Validation: Validate predicted protein-protein interfaces from other methods\nEnsemble Generation: Generate multiple docking poses to capture uncertainty\nBenchmarking: Compare against other docking methods",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#when-to-use-diffdock-pp-vs-other-tools",
    "href": "monday/8-diffdock-pp.html#when-to-use-diffdock-pp-vs-other-tools",
    "title": "8. DiffDock-PP",
    "section": "When to Use DiffDock-PP vs Other Tools",
    "text": "When to Use DiffDock-PP vs Other Tools\n\n\n\nTool\nBest For\n\n\n\n\nDiffDock-PP\nRigid protein-protein docking\n\n\nPLACER\nProtein-ligand docking with conformational sampling\n\n\nChai-1/Boltz-2\nAb initio complex structure prediction\n\n\nBindCraft\nDe novo binder design",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#troubleshooting",
    "href": "monday/8-diffdock-pp.html#troubleshooting",
    "title": "8. DiffDock-PP",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nPyG installation fails:\n\nEnsure CUDA toolkit is loaded before installing\nInstall PyTorch first, then PyG packages\nVerify versions match:\npython -c \"import torch; print(torch.__version__, torch.cuda.is_available())\"\n\nCUDA version mismatch:\nCheck your system‚Äôs CUDA version:\nnvcc --version\nThis should match the pytorch-cuda version (11.6). If not:\nmodule load cuda/11.6\n‚ÄúModuleNotFoundError‚Äù for PyG components:\n\nInstall all PyG packages together:\nmamba install pytorch-scatter pytorch-sparse pytorch-cluster pytorch-spline-conv pyg -c pyg\n\nNumPy errors:\n\nEnsure numpy&lt;2.0 is installed\nDon‚Äôt upgrade NumPy even if prompted\n\nGPU not detected:\n# Verify CUDA is available to PyTorch\npython -c \"import torch; print(torch.cuda.is_available())\"\n# Should print: True\nEmpty output directory:\n\nCheck for error messages in terminal output\nVerify input files exist and are formatted correctly\nEnsure storage/ directory was created",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html",
    "href": "monday/prework-1-env-git.html",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "",
    "text": "This assignment ensures you have properly set up your development environment and can use GitHub for version control. By completing this assignment, you will:\n\nInstall Anaconda (if not already installed)\nCreate a conda environment with all required packages for the bootcamp\nVerify your installation with a Python script\nCommit and push your verification results to GitHub",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#overview",
    "href": "monday/prework-1-env-git.html#overview",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "",
    "text": "This assignment ensures you have properly set up your development environment and can use GitHub for version control. By completing this assignment, you will:\n\nInstall Anaconda (if not already installed)\nCreate a conda environment with all required packages for the bootcamp\nVerify your installation with a Python script\nCommit and push your verification results to GitHub",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#about-autograding",
    "href": "monday/prework-1-env-git.html#about-autograding",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "About Autograding",
    "text": "About Autograding\nNote: This assignment has automated tests that run when you push your code. You‚Äôll see a score out of 100 points. These points are just for feedback - they help you know if you‚Äôve completed everything correctly. This is a bootcamp focused on learning, not grades!",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#prerequisites",
    "href": "monday/prework-1-env-git.html#prerequisites",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nWindows Users: Install WSL (Windows Subsystem for Linux)\nIMPORTANT for Windows Users: This bootcamp requires PyRosetta, which only runs on Unix-based systems (Linux/Mac). Windows users must use Windows Subsystem for Linux (WSL) to complete this assignment.\nInstall WSL:\n\nOpen PowerShell or Windows Command Prompt as Administrator (right-click and ‚ÄúRun as administrator‚Äù)\nInstall WSL with Ubuntu:\nwsl --install\nRestart your computer when prompted\nAfter restart, Ubuntu will automatically open and ask you to create a username and password. Remember these credentials!\nUpdate your WSL installation:\nsudo apt update && sudo apt upgrade -y\n\nUsing WSL: - Launch Ubuntu from your Start menu, or type wsl in PowerShell/Command Prompt - All commands in this assignment should be run in the WSL Ubuntu terminal - Your Windows files are accessible at /mnt/c/ (C: drive), /mnt/d/ (D: drive), etc. - We recommend working in your Linux home directory: cd ~\nFor more detailed WSL documentation, see: https://learn.microsoft.com/en-us/windows/wsl/install\n\n\nGitHub Account\nIf you don‚Äôt have a GitHub account yet, create one at: https://github.com/signup\n\n\nGit Installation\nMac Users: Install Xcode Command Line Tools, which includes git:\nxcode-select --install\nThis will open a dialog to install the command line developer tools. Once installed, you‚Äôll have git available.\nWindows Users (in WSL): Git should be pre-installed in WSL Ubuntu. If not, install it with:\nsudo apt install git\nLinux Users: Download and install git from: https://git-scm.com/downloads Or use your distribution‚Äôs package manager (e.g., sudo apt install git for Ubuntu/Debian)\nVerify git is installed by running:\ngit --version\n\n\nGitHub CLI (Recommended)\nWe recommend installing the GitHub CLI for easier GitHub integration: - Installation instructions: https://cli.github.com/ - After installing, authenticate with: gh auth login\n\n\nBasic Command Line Familiarity\nYou should be comfortable navigating directories and running commands in a terminal.\nHere is a popular GitHub and command line tutorial video: https://www.youtube.com/watch?v=HVsySz-h9r4",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#instructions",
    "href": "monday/prework-1-env-git.html#instructions",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "Instructions",
    "text": "Instructions\n\nStep 1: Install Anaconda (or Alternative)\nNote: If you already have Miniconda, Miniforge, Mambaforge, or Mamba installed, you can use those instead - they all work with this assignment!\nOption 1: Download Anaconda (Mac/Windows/Linux)\nFor Mac/Windows, download and install from: https://www.anaconda.com/download\nFor Windows users in WSL, use the Linux installer:\n# Download the Linux installer in WSL\nwget https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh\n\n# Run the installer\nbash Anaconda3-2024.10-1-Linux-x86_64.sh\n\n# Follow the prompts and accept the license\n# When asked to initialize conda, type 'yes'\n\n# Restart your terminal or run:\nsource ~/.bashrc\nOption 2: Install via Homebrew (Mac users only) If you have Homebrew installed:\nbrew install --cask anaconda\nThen add conda to your PATH (follow the instructions shown after installation).\nVerify installation by running:\nconda --version\n\n\nStep 2: Clone Your GitHub Classroom Repository\nWhen you accepted the GitHub Classroom assignment, a repository was created specifically for you. You need to clone this repository to your computer.\nFinding your repository URL: 1. After accepting the assignment, you should see a link to your repository 2. Go to your repository on GitHub 3. Click the green ‚ÄúCode‚Äù button 4. Copy the repository URL (HTTPS or SSH)\nClone the repository:\n# Replace YOUR_REPO_URL with your actual repository URL\ngit clone YOUR_REPO_URL\n\n# Navigate into the cloned directory\n# (The directory name will match your repository name)\ncd HW1-setup-verification-YOUR_USERNAME\nNote: Your repository URL will be unique to you and will look something like: - HTTPS: https://github.com/YOUR_ORGANIZATION/HW1-setup-verification-YOUR_USERNAME.git - SSH: git@github.com:YOUR_ORGANIZATION/HW1-setup-verification-YOUR_USERNAME.git\n\n\nStep 3: Create the Conda Environment\nNow that you‚Äôre in the repository directory (which contains this README and the environment.yml file), run:\nconda env create -f environment.yml\nThis will create an environment named bootcamp2025_HW1 with all required packages: - Python 3.11 - PyRosetta (for protein structure manipulation - channels configured in environment.yml) - NumPy (numerical computing) - Pandas (data analysis) - Matplotlib & Seaborn (visualization) - Jupyter (interactive notebooks) - SciPy (scientific computing) - scikit-learn (machine learning) - Biopython (bioinformatics tools)\nNote: The environment.yml file includes the RosettaCommons conda channel, so you don‚Äôt need to configure it manually.\n\n\nStep 4: Activate the Environment\nconda activate bootcamp2025_HW1\nYou should see (bootcamp2025_HW1) in your terminal prompt.\n\n\nStep 5: Run the Verification Script\npython verify_setup.py\nThis script will: - Check that all required packages are installed - Display version information for each package - Generate a verification_result.json file\nIf all checks pass, you should see:\nüéâ SUCCESS! All packages are installed correctly!\nIf some checks fail, the script will provide guidance on what went wrong.\n\n\nStep 6: Update the Verification File\nOpen verification_result.json in a text editor and replace \"REPLACE_WITH_YOUR_NAME\" with your actual name.\n\n\nStep 7: Commit and Push Your Verification\nNow that you have your verification file, commit and push it to your GitHub Classroom repository:\n# Add the verification file\ngit add verification_result.json\n\n# Create your commit\ngit commit -m \"Add environment verification\"\n\n# Push to GitHub\ngit push\n\n\nStep 8: Verify Your Submission\nGo to your GitHub repository (the one created when you accepted the assignment) and verify that: - The verification_result.json file is present - It contains your name (not ‚ÄúREPLACE_WITH_YOUR_NAME‚Äù) - The verification_passed field is true",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#submission",
    "href": "monday/prework-1-env-git.html#submission",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "Submission",
    "text": "Submission\nYour work is automatically submitted when you push to your GitHub Classroom repository. The instructor will be able to see your verification file and check that all requirements are met.",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#troubleshooting",
    "href": "monday/prework-1-env-git.html#troubleshooting",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nPyRosetta Installation Issues\nIf PyRosetta fails to install: 1. Make sure you‚Äôre using the environment.yml file which includes the RosettaCommons channel 2. Try creating the environment again with verbose output: conda env create -f environment.yml --verbose 3. If that fails, try installing PyRosetta separately: conda install -c https://conda.rosettacommons.org pyrosetta 4. Check that you‚Äôre using a compatible Python version (3.11 is specified in environment.yml)\n\n\nImport Errors\nIf packages are installed but imports fail: 1. Make sure you‚Äôve activated the environment: conda activate bootcamp2025_HW1 2. Try reinstalling the problematic package: conda install --force-reinstall &lt;package-name&gt;\n\n\nGit Issues\nIf you have trouble with git: - Make sure git is installed: git --version - Configure your git identity if needed: bash   git config --global user.name \"Your Name\"   git config --global user.email \"your.email@example.com\"\n\n\nAuthentication with GitHub\nIf you have trouble pushing to GitHub, you may need to set up authentication: - Use GitHub‚Äôs personal access token: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token - Or set up SSH keys: https://docs.github.com/en/authentication/connecting-to-github-with-ssh",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#questions-and-getting-help",
    "href": "monday/prework-1-env-git.html#questions-and-getting-help",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "Questions and Getting Help",
    "text": "Questions and Getting Help\nWe strongly encourage you to use our Slack workspace for questions and collaboration! One of the best ways to learn is by discussing problems with your peers and seeing how others approach challenges.\nJoin the Bootcamp Slack channel: The Slack workspace is currently reserved for in-person bootcamp participants. We apologize for the inconvenience.\nIn the Slack channel, you can: - Ask questions and get help from fellow students - Share tips and solutions you‚Äôve discovered - Learn from others‚Äô questions and answers - Collaborate on troubleshooting issues\nThe TAs and I will be active in the Slack channel to help guide discussions and answer questions. Don‚Äôt hesitate to jump in - chances are if you‚Äôre stuck on something, others are too!\nIf you have questions, please open an issue on the GitHub repository instead of emailing directly.",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/6-chai1.html",
    "href": "monday/6-chai1.html",
    "title": "6. Chai-1",
    "section": "",
    "text": "Chai-1 (paper, code) is a multi-modal foundation model for molecular structure prediction that achieves state-of-the-art performance across diverse benchmarks. Chai-1 enables unified prediction of proteins, small molecules, DNA, RNA, glycosylations, and more.",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#why-use-chai-1",
    "href": "monday/6-chai1.html#why-use-chai-1",
    "title": "6. Chai-1",
    "section": "Why Use Chai-1?",
    "text": "Why Use Chai-1?\n\nMulti-modal: Predict proteins, nucleic acids, small molecules, and modifications in one model\nState-of-the-art: Top performance on structure prediction benchmarks\nFlexible inputs: Handles complex multi-component assemblies\nExperimental restraints: Can incorporate known distance constraints\n\nRelated Tools: For protein-only predictions, see ESMFold (faster) or LocalColabFold (MSA-based). For binding affinity predictions, see Boltz-2.",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#resource-requirements",
    "href": "monday/6-chai1.html#resource-requirements",
    "title": "6. Chai-1",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n24 GB\n80 GB\nA100 80GB or H100 ideal\n\n\nCPU RAM\n32 GB\n64 GB\nFor preprocessing\n\n\nDisk Space\n10 GB\n20 GB\nModel weights\n\n\nPython\n3.10+\n3.11\nRequired\n\n\n\nGPU Compatibility: Requires bfloat16 support. Compatible GPUs include:\n\nA100, H100, L40S (recommended)\nA10, A30, RTX 4090 (works)\nOlder GPUs may not support bfloat16",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#preparation",
    "href": "monday/6-chai1.html#preparation",
    "title": "6. Chai-1",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nGPU with bfloat16 support\nPython 3.10+\n\nVerify bfloat16 support:\nimport torch\nprint(torch.cuda.is_bf16_supported())  # Should print True",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#installation",
    "href": "monday/6-chai1.html#installation",
    "title": "6. Chai-1",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nCreate a conda environment:\n\nmamba create -n chailab python=3.11\nmamba activate chailab\n\nInstall Chai-1:\n\npip install chai_lab==0.6.1\nExpected download: ~5-10 GB of model weights (downloaded on first run).\nAlternative: Latest development version:\npip install git+https://github.com/chaidiscovery/chai-lab.git",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#testing-the-installation",
    "href": "monday/6-chai1.html#testing-the-installation",
    "title": "6. Chai-1",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test FASTA file test.fasta:\n&gt;protein|name=example\nMKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\nRun prediction:\nchai-lab fold test.fasta output_folder/\nSuccess indicators:\n\nCommand completes without errors\noutput_folder/ contains:\n\npred.model_idx_0.cif - Predicted structure\nscores.model_idx_0.npz - Confidence scores\n\n\nExpected runtime: 2-5 minutes for first run (includes model download), ~30 seconds for subsequent runs.\nNote: By default, this generates 5 sample predictions using embeddings without MSAs.",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#hpc-job-script",
    "href": "monday/6-chai1.html#hpc-job-script",
    "title": "6. Chai-1",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=chai\n#SBATCH --partition=gpu\n#SBATCH --gpus=a100:1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate chailab\n\n# Set custom download directory (avoid filling home)\nexport CHAI_DOWNLOADS_DIR=/scratch/$USER/chai_models\n\n# Run prediction with MSAs\nchai-lab fold --use-msa-server --use-templates-server \\\n    my_complex.fasta \\\n    predictions/",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#usage-examples",
    "href": "monday/6-chai1.html#usage-examples",
    "title": "6. Chai-1",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic prediction (no MSAs, fast):\nchai-lab fold input.fasta output/\nWith MSAs (higher accuracy, uses ColabFold server):\nchai-lab fold --use-msa-server --use-templates-server input.fasta output/\nUsing internal MSA server (if your HPC has one):\nchai-lab fold --use-msa-server \\\n    --msa-server-url \"https://internal.colabserver.edu\" \\\n    input.fasta output/\nGenerate more samples:\nchai-lab fold --num-trunk-recycles 5 --num-diffn-timesteps 200 \\\n    input.fasta output/",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#input-format",
    "href": "monday/6-chai1.html#input-format",
    "title": "6. Chai-1",
    "section": "Input Format",
    "text": "Input Format\nChai-1 uses a modified FASTA format with entity type headers:\nProtein:\n&gt;protein|name=my_protein\nMKTVRQERLKSIVRILERSKEPVSG...\nLigand (SMILES):\n&gt;ligand|name=my_drug\nCC(C)CC1=CC=C(C=C1)C(C)C(=O)O\nDNA:\n&gt;dna|name=promoter\nATGCATGCATGCATGC\nRNA:\n&gt;rna|name=aptamer\nAUGCAUGCAUGCAUGC\nProtein complex (multiple chains):\n&gt;protein|name=chain_A\nMKTVRQERLK...\n&gt;protein|name=chain_B\nMVKLTAEGSE...",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#python-api",
    "href": "monday/6-chai1.html#python-api",
    "title": "6. Chai-1",
    "section": "Python API",
    "text": "Python API\nfrom chai_lab.chai1 import run_inference\n\nresults = run_inference(\n    fasta_file=\"input.fasta\",\n    output_dir=\"output/\",\n    num_trunk_recycles=3,\n    num_diffn_timesteps=200,\n    seed=42\n)\nSee examples/predict_structure.py in the repository for more details.",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#advanced-features",
    "href": "monday/6-chai1.html#advanced-features",
    "title": "6. Chai-1",
    "section": "Advanced Features",
    "text": "Advanced Features\nCustom Templates:\nchai-lab fold --custom-template template.cif input.fasta output/\nExperimental Restraints: Specify inter-chain contacts:\n# See: github.com/chaidiscovery/chai-lab/tree/main/examples/restraints\nCovalent Bonds: Specify covalent modifications:\n# See: github.com/chaidiscovery/chai-lab/tree/main/examples/covalent_bonds",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#understanding-the-output",
    "href": "monday/6-chai1.html#understanding-the-output",
    "title": "6. Chai-1",
    "section": "Understanding the Output",
    "text": "Understanding the Output\n\n\n\nFile\nDescription\n\n\n\n\npred.model_idx_N.cif\nPredicted structure (mmCIF format)\n\n\nscores.model_idx_N.npz\nConfidence scores\n\n\nmsa_*.a3m\nGenerated MSAs (if using MSA server)\n\n\n\nConfidence metrics (in scores file):\n\npLDDT: Per-residue confidence\npTM: Predicted TM-score\npAE: Predicted aligned error\ninterface scores: For multi-chain predictions",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#web-server",
    "href": "monday/6-chai1.html#web-server",
    "title": "6. Chai-1",
    "section": "Web Server",
    "text": "Web Server\nFor quick tests without installation: lab.chaidiscovery.com",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#troubleshooting",
    "href": "monday/6-chai1.html#troubleshooting",
    "title": "6. Chai-1",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n‚Äúbfloat16 not supported‚Äù:\n\nYour GPU doesn‚Äôt support bfloat16\nTry a newer GPU (A100, H100, RTX 4090)\nOlder GPUs (V100, etc.) may not work\n\nOut of memory:\n\nRequest GPU with more memory\nReduce --num-diffn-timesteps\nFor very large complexes, split into smaller units\n\nModel download location:\n# Set before running\nexport CHAI_DOWNLOADS_DIR=/scratch/$USER/chai_models\nMSA server rate limits:\n\nThe public ColabFold MMseqs2 server is a shared resource\nFor batch jobs, space out requests\nConsider setting up a local MSA server for high-throughput\n\nSlow first run:\n\nFirst run downloads ~5-10 GB of model weights\nSubsequent runs are much faster\nSet CHAI_DOWNLOADS_DIR to avoid re-downloading",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/11-esm3.html",
    "href": "monday/11-esm3.html",
    "title": "11. ESM3 (Optional)",
    "section": "",
    "text": "ESM3 (paper, code) is a frontier generative model for biology that jointly reasons across three fundamental biological properties of proteins: sequence, structure, and function. It represents a multimodal generative masked language model.\nNote: This tool is marked as OPTIONAL. Install if you‚Äôre interested in protein generation and multimodal design beyond structure prediction.",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#why-use-esm3",
    "href": "monday/11-esm3.html#why-use-esm3",
    "title": "11. ESM3 (Optional)",
    "section": "Why Use ESM3?",
    "text": "Why Use ESM3?\n\nMultimodal generation: Jointly reason about sequence, structure, and function\nProtein generation: Create novel proteins with desired properties\nSequence completion: Fill in masked or missing regions\nEmbeddings: Extract rich protein representations (ESM C)\n\nRelated Tools: For structure prediction only, see ESMFold. For sequence design given structure, see LigandMPNN.",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#resource-requirements",
    "href": "monday/11-esm3.html#resource-requirements",
    "title": "11. ESM3 (Optional)",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n24+ GB\nFor esm3-small (1.4B params)\n\n\nCPU RAM\n16 GB\n32 GB\nFor preprocessing\n\n\nDisk Space\n10 GB\n20 GB\nModel weights\n\n\nPython\n3.10+\n3.10\nRequired\n\n\n\nModel sizes:\n\nesm3-small-2024-08 (1.4B params): Runs locally\nesm3-medium-2024-08 (7B params): Via Forge API\nesm3-large-2024-03 (98B params): Via Forge API",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#preparation",
    "href": "monday/11-esm3.html#preparation",
    "title": "11. ESM3 (Optional)",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nHuggingFace account (for model access)",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#installation",
    "href": "monday/11-esm3.html#installation",
    "title": "11. ESM3 (Optional)",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nCreate a conda environment:\n\nmamba create -n esm3 python=3.10\nmamba activate esm3\n\nInstall the ESM library:\n\npip install esm",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#huggingface-authentication",
    "href": "monday/11-esm3.html#huggingface-authentication",
    "title": "11. ESM3 (Optional)",
    "section": "HuggingFace Authentication",
    "text": "HuggingFace Authentication\nESM3 weights are stored on HuggingFace Hub. You need to authenticate:\n\nCreate a HuggingFace account at huggingface.co\nGenerate an API token with ‚ÄúRead‚Äù permission at huggingface.co/settings/tokens\nAuthenticate in Python:\n\nfrom huggingface_hub import login\nlogin()  # Follow prompts to enter your token\nOr set environment variable:\nexport HF_TOKEN=\"your_token_here\"",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#testing-the-installation",
    "href": "monday/11-esm3.html#testing-the-installation",
    "title": "11. ESM3 (Optional)",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test script test_esm3.py:\nimport os\nfrom huggingface_hub import login\nfrom esm.models.esm3 import ESM3\nfrom esm.sdk.api import ESMProtein, GenerationConfig\n\n# Authenticate\n# Method 1: Environment variable (Recommended for HPC jobs)\nif \"HF_TOKEN\" in os.environ:\n    login(token=os.environ[\"HF_TOKEN\"])\n# Method 2: Interactive login (Run once on login node)\nelse:\n    login()\n\n# Load the model (downloads weights on first run)\nmodel = ESM3.from_pretrained(\"esm3-small-2024-08\").to(\"cuda\")  # or \"cpu\"\n\n# Generate a protein sequence completion\nprompt = \"MKTVRQ_______________QLAEELSVSRQVIVQDIAYLRSLG\"\nprotein = ESMProtein(sequence=prompt)\n\n# Generate sequence\nprotein = model.generate(\n    protein,\n    GenerationConfig(track=\"sequence\", num_steps=8, temperature=0.7)\n)\n\nprint(\"Generated sequence:\")\nprint(protein.sequence)\n\n# Generate structure\nprotein = model.generate(\n    protein,\n    GenerationConfig(track=\"structure\", num_steps=8)\n)\n\n# Save structure\nprotein.to_pdb(\"./generated.pdb\")\nprint(\"Structure saved to generated.pdb\")\nRun the test:\npython test_esm3.py\nSuccess indicators:\n\nModel loads without errors\nSequence completion fills in the masked region\nStructure is generated and saved as PDB\n\nExpected runtime: 2-5 minutes (first run downloads ~3GB weights).",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#hpc-job-script",
    "href": "monday/11-esm3.html#hpc-job-script",
    "title": "11. ESM3 (Optional)",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=esm3\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate esm3\n\n# Set HuggingFace token\nexport HF_TOKEN=\"your_token_here\"\n\npython generate_protein.py",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#usage-examples",
    "href": "monday/11-esm3.html#usage-examples",
    "title": "11. ESM3 (Optional)",
    "section": "Usage Examples",
    "text": "Usage Examples\nSequence generation (fill masked regions):\nfrom esm.models.esm3 import ESM3\nfrom esm.sdk.api import ESMProtein, GenerationConfig\n\nmodel = ESM3.from_pretrained(\"esm3-small-2024-08\").to(\"cuda\")\n\n# Use underscores for masked positions\nprotein = ESMProtein(sequence=\"MKTVRQ_______________QLAEELSVSRQVIVQDIAYLRSLG\")\n\n# Generate\nprotein = model.generate(\n    protein,\n    GenerationConfig(track=\"sequence\", num_steps=8, temperature=0.7)\n)\nprint(protein.sequence)\nStructure prediction:\nprotein = ESMProtein(sequence=\"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLG\")\n\nprotein = model.generate(\n    protein,\n    GenerationConfig(track=\"structure\", num_steps=8)\n)\n\nprotein.to_pdb(\"predicted.pdb\")\nUsing ESM C for embeddings only (faster, smaller):\nfrom esm.models.esmc import ESMC\nfrom esm.sdk.api import ESMProtein, LogitsConfig\n\nprotein = ESMProtein(sequence=\"MKTVRQERLK\")\nclient = ESMC.from_pretrained(\"esmc_300m\").to(\"cuda\")\n\n# Get embeddings\nprotein_tensor = client.encode(protein)\nlogits_output = client.logits(\n    protein_tensor,\n    LogitsConfig(sequence=True, return_embeddings=True)\n)\n\nprint(f\"Embedding shape: {logits_output.embeddings.shape}\")",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#available-models",
    "href": "monday/11-esm3.html#available-models",
    "title": "11. ESM3 (Optional)",
    "section": "Available Models",
    "text": "Available Models\n\n\n\nModel\nParameters\nAvailability\n\n\n\n\nesm3-small-2024-08\n1.4B\nLocal (free)\n\n\nesmc_300m\n300M\nLocal (fast embeddings)\n\n\nesmc_600m\n600M\nLocal\n\n\nesm3-medium-2024-08\n7B\nForge API\n\n\nesm3-large-2024-03\n98B\nForge API",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#generation-tracks",
    "href": "monday/11-esm3.html#generation-tracks",
    "title": "11. ESM3 (Optional)",
    "section": "Generation Tracks",
    "text": "Generation Tracks\nESM3 can generate different ‚Äútracks‚Äù:\n\n\n\nTrack\nDescription\n\n\n\n\nsequence\nGenerate amino acid sequence\n\n\nstructure\nGenerate 3D coordinates\n\n\nfunction\nGenerate functional annotations",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#key-parameters",
    "href": "monday/11-esm3.html#key-parameters",
    "title": "11. ESM3 (Optional)",
    "section": "Key Parameters",
    "text": "Key Parameters\n\n\n\nParameter\nDescription\n\n\n\n\ntrack\nWhat to generate: sequence, structure, function\n\n\nnum_steps\nNumber of generation steps (more = better quality)\n\n\ntemperature\nSampling diversity (higher = more diverse)",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#use-cases",
    "href": "monday/11-esm3.html#use-cases",
    "title": "11. ESM3 (Optional)",
    "section": "Use Cases",
    "text": "Use Cases\n\nProtein generation: Create novel proteins\nSequence completion: Fill in missing regions\nStructure prediction: Generate 3D structures\nFunction prediction: Predict functional properties\nEmbeddings: Extract protein representations for ML",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#troubleshooting",
    "href": "monday/11-esm3.html#troubleshooting",
    "title": "11. ESM3 (Optional)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nHuggingFace authentication errors:\n\nVerify token has ‚ÄúRead‚Äù permission\nRun login() in Python and follow prompts\nOr set HF_TOKEN environment variable\n\nModel download issues:\n\nCheck network connectivity\nWeights are large (~3GB for small model)\nSet HF_HOME to location with space:\nexport HF_HOME=/scratch/$USER/huggingface\n\nGPU memory issues:\n\nUse CPU if GPU is insufficient: .to(\"cpu\")\nReduce batch size if processing multiple proteins\nesmc_300m is smaller and faster for embeddings\n\nSlow generation:\n\nGPU strongly recommended\nReduce num_steps for faster (lower quality) results\nUse ESM C for embeddings (no structure generation)",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html",
    "href": "monday/12-rfdiffusion-aa.html",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "",
    "text": "RFdiffusion All Atom (code) is the predecessor to RFdiffusion2, enabling all-atom protein design with small molecule binding. It can design protein binders to ligands with all-atom precision.\nNote: This tool is marked as OPTIONAL because RFdiffusion2 is the newer, more capable version. Install this only if you need the earlier methodology or specific features.",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#why-use-rfdiffusion-all-atom",
    "href": "monday/12-rfdiffusion-aa.html#why-use-rfdiffusion-all-atom",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Why Use RFdiffusion All Atom?",
    "text": "Why Use RFdiffusion All Atom?\n\nSmall molecule binding: Design proteins that bind specific ligands\nMotif incorporation: Include functional motifs in designs\nHistorical reference: Understand the evolution of diffusion-based design\nSpecific workflows: Some published protocols may reference this version\n\nRelated Tools: For the newer version, see RFdiffusion2. For sequence design, see LigandMPNN.",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#resource-requirements",
    "href": "monday/12-rfdiffusion-aa.html#resource-requirements",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n8 GB\n16 GB\nScales with design size\n\n\nCPU RAM\n8 GB\n16 GB\nContainer-based\n\n\nDisk Space\n5 GB\n10 GB\nContainer + weights\n\n\nContainer\nApptainer/Singularity\nRequired\nNot Docker",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#preparation",
    "href": "monday/12-rfdiffusion-aa.html#preparation",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nApptainer/Singularity available\nGPU access recommended\n\nImportant: Like RFdiffusion2, this uses Apptainer/Singularity containers. Most academic HPCs do NOT support Docker.\nVerify container runtime:\nmodule load apptainer    # or: module load singularity\napptainer --version",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#installation",
    "href": "monday/12-rfdiffusion-aa.html#installation",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the repository:\n\ngit clone https://github.com/baker-laboratory/rf_diffusion_all_atom.git\ncd rf_diffusion_all_atom\n\nDownload the Singularity container:\n\nwget http://files.ipd.uw.edu/pub/RF-All-Atom/containers/rf_se3_diffusion.sif\nExpected download: ~2-3 GB.\n\nDownload the model weights:\n\nwget http://files.ipd.uw.edu/pub/RF-All-Atom/weights/RFDiffusionAA_paper_weights.pt\nExpected download: ~500 MB.\n\nInitialize git submodules:\n\ngit submodule init\ngit submodule update",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#testing-the-installation",
    "href": "monday/12-rfdiffusion-aa.html#testing-the-installation",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a ligand binder design example:\napptainer run --nv rf_se3_diffusion.sif -u run_inference.py \\\n    inference.deterministic=True \\\n    diffuser.T=100 \\\n    inference.output_prefix=output/ligand_test/sample \\\n    inference.input_pdb=input/7v11.pdb \\\n    contigmap.contigs=\"['150-150']\" \\\n    inference.ligand=OQO \\\n    inference.num_designs=1 \\\n    inference.design_startnum=0\nNote: Omit --nv flag if running without GPU.\nSuccess indicators:\n\nCommand completes without errors\nOutput files created:\n\noutput/ligand_test/sample_0.pdb - The designed structure\noutput/ligand_test/sample_0_Xt-1_traj.pdb - Denoising trajectory\noutput/ligand_test/sample_0_X0-1_traj.pdb - Predicted ground truth at each step\n\n\nExpected runtime: 5-10 minutes on GPU, 30+ minutes on CPU.",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#hpc-job-script",
    "href": "monday/12-rfdiffusion-aa.html#hpc-job-script",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=rfdaa\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load apptainer\nmodule load cuda/12.1\n\ncd /path/to/rf_diffusion_all_atom\n\napptainer run --nv rf_se3_diffusion.sif -u run_inference.py \\\n    inference.deterministic=True \\\n    diffuser.T=100 \\\n    inference.output_prefix=output/my_design/sample \\\n    inference.input_pdb=input/my_protein.pdb \\\n    contigmap.contigs=\"['150-150']\" \\\n    inference.ligand=HEM \\\n    inference.num_designs=10",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#usage-examples",
    "href": "monday/12-rfdiffusion-aa.html#usage-examples",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic ligand binder design:\napptainer run --nv rf_se3_diffusion.sif -u run_inference.py \\\n    inference.input_pdb=input/complex.pdb \\\n    inference.output_prefix=output/design \\\n    inference.ligand=LIG \\\n    contigmap.contigs=\"['100-100']\" \\\n    inference.num_designs=10\nMultiple designs with different lengths:\napptainer run --nv rf_se3_diffusion.sif -u run_inference.py \\\n    inference.input_pdb=input/complex.pdb \\\n    inference.output_prefix=output/design \\\n    inference.ligand=LIG \\\n    contigmap.contigs=\"['80-120']\" \\\n    inference.num_designs=20",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#key-parameters",
    "href": "monday/12-rfdiffusion-aa.html#key-parameters",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Key Parameters",
    "text": "Key Parameters\n\n\n\nParameter\nDescription\n\n\n\n\ninference.input_pdb\nInput PDB with ligand\n\n\ninference.output_prefix\nOutput path prefix\n\n\ninference.ligand\nLigand residue name\n\n\ncontigmap.contigs\nProtein length range (e.g., ['100-100'])\n\n\ninference.num_designs\nNumber of designs\n\n\ndiffuser.T\nDiffusion timesteps (100 typical)\n\n\ninference.deterministic\nReproducible results",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#docker-to-apptainer-translation",
    "href": "monday/12-rfdiffusion-aa.html#docker-to-apptainer-translation",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Docker to Apptainer Translation",
    "text": "Docker to Apptainer Translation\nThe official docs may show Docker commands. Translate as follows:\n\n\n\n\n\n\n\nDocker\nApptainer\n\n\n\n\ndocker run --gpus all\napptainer run --nv\n\n\ndocker run -v $(pwd):/workspace\napptainer run --bind $(pwd):/workspace\n\n\n-it (interactive)\nUse apptainer shell --nv",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#understanding-the-output",
    "href": "monday/12-rfdiffusion-aa.html#understanding-the-output",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Understanding the Output",
    "text": "Understanding the Output\n\n\n\n\n\n\n\nOutput File\nDescription\n\n\n\n\nsample_N.pdb\nFinal designed structure\n\n\nsample_N_Xt-1_traj.pdb\nDenoising trajectory (animation of design)\n\n\nsample_N_X0-1_traj.pdb\nModel predictions at each step\n\n\n\nThe trajectory files can be loaded into PyMOL to visualize the diffusion process.",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#comparison-with-rfdiffusion2",
    "href": "monday/12-rfdiffusion-aa.html#comparison-with-rfdiffusion2",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Comparison with RFdiffusion2",
    "text": "Comparison with RFdiffusion2\n\n\n\nFeature\nRFdiffusion AA\nRFdiffusion2\n\n\n\n\nAtomic precision\nYes\nYes (improved)\n\n\nLigand binding\nYes\nYes\n\n\nActive site scaffolding\nLimited\nAdvanced\n\n\nModel architecture\nEarlier\nUpdated\n\n\nRecommended\nLegacy workflows\nNew projects\n\n\n\nRecommendation: Use RFdiffusion2 for new projects unless you have specific reasons to use this version.",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#troubleshooting",
    "href": "monday/12-rfdiffusion-aa.html#troubleshooting",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nContainer not found:\n\nEnsure .sif file is in current directory\nOr provide full path to container\n\nGPU errors:\n\nEnsure --nv flag is included for GPU\nLoad CUDA module: module load cuda/12.1\nVerify GPU availability: nvidia-smi\n\nPermission denied on container:\nchmod +x rf_se3_diffusion.sif\nInput PDB errors:\n\nVerify ligand is present in input PDB\nCheck ligand residue name matches inference.ligand\nEnsure PDB has proper formatting\n\nSubmodule errors:\ngit submodule init\ngit submodule update",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/index.html",
    "href": "monday/index.html",
    "title": "Monday: Tool Installation",
    "section": "",
    "text": "Monday is dedicated to installing the essential ML-based protein design and structure prediction tools on your HPC cluster. Each module guides you through installing a specific tool, from structure predictors like LocalColabFold and ESMFold to design tools like LigandMPNN and BindCraft.\nGoal: By the end of Monday, you should have all major tools installed and tested on your HPC cluster.",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#overview",
    "href": "monday/index.html#overview",
    "title": "Monday: Tool Installation",
    "section": "",
    "text": "Monday is dedicated to installing the essential ML-based protein design and structure prediction tools on your HPC cluster. Each module guides you through installing a specific tool, from structure predictors like LocalColabFold and ESMFold to design tools like LigandMPNN and BindCraft.\nGoal: By the end of Monday, you should have all major tools installed and tested on your HPC cluster.",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#pre-work",
    "href": "monday/index.html#pre-work",
    "title": "Monday: Tool Installation",
    "section": "Pre-work",
    "text": "Pre-work\nBefore starting the main modules, complete these pre-work assignments to ensure your local environment is ready.\n\n\n\n#\nModule\nDescription\nStatus\n\n\n\n\nP1\nEnvironment & GitHub\nSetup conda, git, and GitHub\nRequired\n\n\nP2\nPyMOL & VS Code\nInstall visualization and coding tools\nRequired\n\n\nP3\nPython Refresher\nRefresh Python skills for bioinformatics\nRecommended",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#getting-started",
    "href": "monday/index.html#getting-started",
    "title": "Monday: Tool Installation",
    "section": "Getting Started",
    "text": "Getting Started\nBefore installing individual tools, complete the HPC Setup module to ensure your environment is properly configured:\n\n\n\n#\nModule\nDescription\nStatus\n\n\n\n\n0\nCommon HPC Setup\nCUDA, Conda, containers, and environment setup\nStart Here",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#tool-installation-modules",
    "href": "monday/index.html#tool-installation-modules",
    "title": "Monday: Tool Installation",
    "section": "Tool Installation Modules",
    "text": "Tool Installation Modules\n\n\n\n#\nTool\nDescription\nStatus\n\n\n\n\n1\nLocalColabFold\nFast AlphaFold2 structure prediction\nRequired\n\n\n2\nLigandMPNN\nContext-aware protein sequence design\nRequired\n\n\n3\nRFdiffusion2\nAtom-level active site scaffolding\nRequired\n\n\n4\nESMFold\nSingle-sequence structure prediction\nRequired\n\n\n5\nOpenFold\nOpen-source AlphaFold2 reproduction\nOptional\n\n\n6\nChai-1\nMulti-modal biomolecular structure prediction\nRequired\n\n\n7\nBoltz-2\nStructure + binding affinity prediction\nRequired\n\n\n8\nDiffDock-PP\nProtein-protein docking\nRequired\n\n\n9\nPLACER\nProtein-ligand conformational ensemble prediction\nRequired\n\n\n10\nBindCraft\nEnd-to-end binder design pipeline\nRequired\n\n\n11\nESM3\nMultimodal protein generation\nOptional\n\n\n12\nRFdiffusion All Atom\nAll-atom protein design (predecessor to RFd2)\nOptional",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#tool-categories",
    "href": "monday/index.html#tool-categories",
    "title": "Monday: Tool Installation",
    "section": "Tool Categories",
    "text": "Tool Categories\n\nStructure Prediction\n\n\n\n\n\n\n\n\n\n\nTool\nInput\nOutput\nSpeed\nBest For\n\n\n\n\nLocalColabFold\nSequence + MSA\nStructure\nMedium\nHigh-accuracy single proteins\n\n\nESMFold\nSequence only\nStructure\nFast\nQuick predictions, no MSA needed\n\n\nOpenFold\nSequence + MSA\nStructure\nMedium\nResearch, custom training\n\n\nChai-1\nMulti-modal\nComplex structures\nMedium\nProteins + ligands + nucleic acids\n\n\nBoltz-2\nMulti-modal\nStructure + affinity\nMedium\nDrug discovery\n\n\n\n\n\nProtein Design\n\n\n\n\n\n\n\n\n\nTool\nInput\nOutput\nBest For\n\n\n\n\nLigandMPNN\nBackbone\nSequence\nSequence design with ligand context\n\n\nRFdiffusion2\nConstraints\nBackbone\nActive site scaffolding\n\n\nBindCraft\nTarget structure\nBinder designs\nEnd-to-end binder design\n\n\n\n\n\nDocking\n\n\n\n\n\n\n\n\n\nTool\nInput\nOutput\nBest For\n\n\n\n\nDiffDock-PP\nTwo proteins\nDocked complex\nProtein-protein docking\n\n\nPLACER\nProtein + ligand\nEnsemble poses\nProtein-ligand docking",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#tips-for-success",
    "href": "monday/index.html#tips-for-success",
    "title": "Monday: Tool Installation",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nStart with HPC Setup - Complete Module 0 before installing any tools\nUse separate conda environments - Each tool should have its own environment to avoid dependency conflicts\nCheck GPU availability - Most tools require GPU access; make sure you can request GPU nodes on your cluster\nNote your paths - Keep track of where you install each tool; you‚Äôll need these paths later\nTest each installation - Don‚Äôt move on until you‚Äôve verified each tool works\nCheck shared resources - Your HPC may already have databases (AlphaFold, ColabFold) installed",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#resource-overview",
    "href": "monday/index.html#resource-overview",
    "title": "Monday: Tool Installation",
    "section": "Resource Overview",
    "text": "Resource Overview\nApproximate requirements across all tools:\n\n\n\nResource\nTotal Needed\n\n\n\n\nDisk Space\n50-100 GB (tools only), 2+ TB (with databases)\n\n\nGPU RAM\n16-80 GB depending on task\n\n\nCPU RAM\n32-64 GB",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#getting-help",
    "href": "monday/index.html#getting-help",
    "title": "Monday: Tool Installation",
    "section": "Getting Help",
    "text": "Getting Help\nIf you encounter issues:\n\nCheck the tool‚Äôs official documentation (linked in each module)\nSearch for existing GitHub issues on the tool‚Äôs repository\nReport an issue on the bootcamp site\n\n\n\n\n‚Üê Back to Home\n\n\nTuesday ‚Üí",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html",
    "href": "monday/prework-2-pymol-vscode.html",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "",
    "text": "This assignment ensures you have PyMOL and VS Code installed and configured for the bootcamp. You will:\n\nInstall PyMOL (a molecular visualization tool)\nInstall VS Code (a powerful code editor/IDE)\nVerify your installations by completing hands-on tasks\nCommit your work to GitHub",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#overview",
    "href": "monday/prework-2-pymol-vscode.html#overview",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "",
    "text": "This assignment ensures you have PyMOL and VS Code installed and configured for the bootcamp. You will:\n\nInstall PyMOL (a molecular visualization tool)\nInstall VS Code (a powerful code editor/IDE)\nVerify your installations by completing hands-on tasks\nCommit your work to GitHub",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#about-autograding",
    "href": "monday/prework-2-pymol-vscode.html#about-autograding",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "About Autograding",
    "text": "About Autograding\nNote: This assignment has automated tests that run when you push your code. You‚Äôll see a score out of 100 points. These points are just for feedback - they help you know if you‚Äôve completed everything correctly. This is a bootcamp focused on learning, not grades!",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#prerequisites",
    "href": "monday/prework-2-pymol-vscode.html#prerequisites",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nHW1 completed",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#part-1-pymol-installation",
    "href": "monday/prework-2-pymol-vscode.html#part-1-pymol-installation",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Part 1: PyMOL Installation",
    "text": "Part 1: PyMOL Installation\nPyMOL is a molecular visualization tool that is super useful for visualizing, manipulating, and analyzing molecular structures, especially proteins. We will use PyMOL to explore protein structures during bootcamp. We recognize that it‚Äôs likely you are already quite familiar with PyMOL but if you have been using an alternative visualization software, this is a great opportunity to explore PyMOL. Please be ready to use this in the Bootcamp.\n\nInstallation Options\nOption 1: Official PyMOL (Requires Educational License) 1. Visit the PyMOL website: https://pymol.org/ 2. Obtain a free educational license: https://pymol.org/edu/ 3. Download and install PyMOL\nOption 2: Open-Source PyMOL via Homebrew (Mac users) If you have Homebrew installed:\nbrew install pymol\nVerify installation by launching PyMOL (GUI) from your applications or by running:\npymol\n\n\nOptional PyMOL Learning Resources\nThough a comprehensive understanding of the various features in PyMOL is not within the scope of this course, it may be a useful tool for your future work and presentations.\n\nPyMOL Tutorial Video - Complete this tutorial using a protein of interest\nPyMOL Guide - Comprehensive written guide\nPyMOL Wiki - Explore more PyMOL features and tools\n\n\n\nCustomizing PyMOL (Optional)\nIf you would like to alter your visual defaults, you may do so by editing your .pymolrc, which is a script that gets read every time PyMOL opens. To do so, go to File &gt; Edit pymolrc. Here are the defaults that Rosetta member Joey Lubin uses, which can be pasted into the window that opens.",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#part-2-vs-code-installation",
    "href": "monday/prework-2-pymol-vscode.html#part-2-vs-code-installation",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Part 2: VS Code Installation",
    "text": "Part 2: VS Code Installation\nVS Code is a powerful IDE that is largely customizable and can incorporate many different extensions and features. If you do not have VS Code or an alternative IDE, please install it before the bootcamp.\n\nInstallation Options\nOption 1: Download from Website Visit the VS Code website: https://code.visualstudio.com/\nOption 2: Install via Homebrew (Mac users) If you have Homebrew installed:\nbrew install --cask visual-studio-code\nVerify installation by launching VS Code or running:\ncode --version\n\n\nRecommended VS Code Extensions\nOnce VS Code is installed, we recommend installing the following extensions:\n\nPython (Microsoft) - Python language support\nPylance (Microsoft) - Fast Python language server\nJupyter (Microsoft) - Jupyter notebook support\n\nTo install extensions: 1. Open VS Code 2. Click the Extensions icon in the sidebar (or press Cmd+Shift+X / Ctrl+Shift+X) 3. Search for each extension and click ‚ÄúInstall‚Äù",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#part-3-verification-tasks",
    "href": "monday/prework-2-pymol-vscode.html#part-3-verification-tasks",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Part 3: Verification Tasks",
    "text": "Part 3: Verification Tasks\nNow let‚Äôs verify that everything is working correctly!\n\nTask 1: PyMOL Verification\n\nFetch a protein structure:\n\nOpen PyMOL\nIn the PyMOL command line, type: fetch 1ubq\nThis fetches ubiquitin (PDB ID: 1UBQ), a small regulatory protein\n\nStyle your visualization:\n\nTry different representations (cartoon, sticks, surface, etc.)\nColor the structure in a way you find visually appealing\nFeel free to experiment!\n\nSave your work:\n\nSave the PyMOL session: File &gt; Save Session As...\nName it 1ubq_session.pse and save it in this assignment directory\nExport an image: File &gt; Export Image As &gt; PNG...\nName it 1ubq_visualization.png and save it in this assignment directory\n\n\n\n\nTask 2: VS Code Verification\n\nOpen VS Code\nCreate a Python script:\n\nOpen this assignment folder in VS Code: File &gt; Open Folder...\nCreate a new file called verify_setup.py\nCopy and paste the verification script (provided in this repository)\nRun the script to verify your setup\n\nRun the verification script:\n\nOpen the integrated terminal in VS Code: Terminal &gt; New Terminal\nActivate your conda environment from HW1: conda activate bootcamp2025_HW1\nRun: python verify_setup.py\nThe script will check your installations and create a verification_result.txt file",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#part-4-submission",
    "href": "monday/prework-2-pymol-vscode.html#part-4-submission",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Part 4: Submission",
    "text": "Part 4: Submission\nOnce you‚Äôve completed both tasks, commit your work to your GitHub Classroom repository:\n# Add all the files you created\ngit add 1ubq_session.pse 1ubq_visualization.png verification_result.txt\n\n# Commit your work\ngit commit -m \"Complete PyMOL and VS Code setup verification\"\n\n# Push to GitHub\ngit push\n\nRequired Files for Submission\nMake sure your repository contains: - [ ] 1ubq_session.pse - Your PyMOL session file - [ ] 1ubq_visualization.png - Your exported PyMOL visualization - [ ] verification_result.txt - Output from the VS Code verification script",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#troubleshooting",
    "href": "monday/prework-2-pymol-vscode.html#troubleshooting",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nPyMOL Issues\nPyMOL won‚Äôt launch: - Make sure you‚Äôve completed the installation fully - Mac users: Try the Homebrew installation method - Make sure you‚Äôre launching the GUI application, not just the Python package\nCan‚Äôt fetch PDB structures: - Check your internet connection - Try: fetch 1ubq, type=pdb1 if the default doesn‚Äôt work\n\n\nVS Code Issues\ncode command not found: - On Mac: Open VS Code, press Cmd+Shift+P, type ‚Äúshell command‚Äù, and select ‚ÄúInstall ‚Äòcode‚Äô command in PATH‚Äù - On Windows: VS Code should be in your PATH automatically after installation\nPython extension not working: - Make sure Python is installed (you should have this from HW1) - Restart VS Code after installing extensions",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#questions-and-getting-help",
    "href": "monday/prework-2-pymol-vscode.html#questions-and-getting-help",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Questions and Getting Help",
    "text": "Questions and Getting Help\nWe strongly encourage you to use our Slack workspace for questions and collaboration! One of the best ways to learn is by discussing problems with your peers and seeing how others approach challenges.\nJoin the Bootcamp Slack channel: The Slack workspace is currently reserved for in-person bootcamp participants. We apologize for the inconvenience.\nIn the Slack channel, you can: - Ask questions and get help from fellow students - Share tips and solutions you‚Äôve discovered - Learn from others‚Äô questions and answers - Collaborate on troubleshooting issues\nThe TAs and I will be active in the Slack channel to help guide discussions and answer questions. Don‚Äôt hesitate to jump in - chances are if you‚Äôre stuck on something, others are too!\nIf you have questions, please open an issue on the GitHub repository instead of emailing directly.",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "tuesday/index.html",
    "href": "tuesday/index.html",
    "title": "Tuesday",
    "section": "",
    "text": "Tuesday content coming soon.",
    "crumbs": [
      "Tuesday"
    ]
  },
  {
    "objectID": "tuesday/index.html#overview",
    "href": "tuesday/index.html#overview",
    "title": "Tuesday",
    "section": "",
    "text": "Tuesday content coming soon.",
    "crumbs": [
      "Tuesday"
    ]
  },
  {
    "objectID": "tuesday/index.html#modules",
    "href": "tuesday/index.html#modules",
    "title": "Tuesday",
    "section": "Modules",
    "text": "Modules\n\n\n\nModule\nTopic\nStatus\n\n\n\n\n2.1\nPlaceholder Module\nComing soon\n\n\n\nMore modules will be added here.\n\n\n\n‚Üê Monday\n\n\nBack to Home\n\n\nWednesday ‚Üí",
    "crumbs": [
      "Tuesday"
    ]
  },
  {
    "objectID": "wednesday/3.1-placeholder.html",
    "href": "wednesday/3.1-placeholder.html",
    "title": "3.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon.",
    "crumbs": [
      "Wednesday",
      "3.1 Placeholder Module"
    ]
  },
  {
    "objectID": "wednesday/3.1-placeholder.html#learning-objectives",
    "href": "wednesday/3.1-placeholder.html#learning-objectives",
    "title": "3.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon.",
    "crumbs": [
      "Wednesday",
      "3.1 Placeholder Module"
    ]
  },
  {
    "objectID": "wednesday/3.1-placeholder.html#section-1",
    "href": "wednesday/3.1-placeholder.html#section-1",
    "title": "3.1 Placeholder Module",
    "section": "Section 1",
    "text": "Section 1\n Mark Section 1 as complete\nPlaceholder content for Section 1.",
    "crumbs": [
      "Wednesday",
      "3.1 Placeholder Module"
    ]
  },
  {
    "objectID": "wednesday/3.1-placeholder.html#section-2",
    "href": "wednesday/3.1-placeholder.html#section-2",
    "title": "3.1 Placeholder Module",
    "section": "Section 2",
    "text": "Section 2\n Mark Section 2 as complete\nPlaceholder content for Section 2.",
    "crumbs": [
      "Wednesday",
      "3.1 Placeholder Module"
    ]
  },
  {
    "objectID": "wednesday/index.html",
    "href": "wednesday/index.html",
    "title": "Wednesday",
    "section": "",
    "text": "Wednesday content coming soon.",
    "crumbs": [
      "Wednesday"
    ]
  },
  {
    "objectID": "wednesday/index.html#overview",
    "href": "wednesday/index.html#overview",
    "title": "Wednesday",
    "section": "",
    "text": "Wednesday content coming soon.",
    "crumbs": [
      "Wednesday"
    ]
  },
  {
    "objectID": "wednesday/index.html#modules",
    "href": "wednesday/index.html#modules",
    "title": "Wednesday",
    "section": "Modules",
    "text": "Modules\n\n\n\nModule\nTopic\nStatus\n\n\n\n\n3.1\nPlaceholder Module\nComing soon\n\n\n\nMore modules will be added here.\n\n\n\n‚Üê Tuesday\n\n\nBack to Home\n\n\nThursday ‚Üí",
    "crumbs": [
      "Wednesday"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "",
    "text": "Welcome back! You were last on:   Resume where you left off\nWelcome to the ML Protein Design Bootcamp 2025! This self-paced course covers machine learning tools for protein structure prediction and design."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "Course Overview",
    "text": "Course Overview\nThis bootcamp is organized into 4 days of content plus a capstone project. Each day contains sequential modules that build on each other.\n\n\nMonday\nComing soon\nIntroduction and setup modules.\n\n\nTuesday\nComing soon\nModules for Tuesday.\n\n\nWednesday\nComing soon\nModules for Wednesday.\n\n\nThursday\nComing soon\nModules for Thursday.\n\n\nCapstone\nComing soon\nCapstone project bringing together everything you‚Äôve learned."
  },
  {
    "objectID": "index.html#your-progress",
    "href": "index.html#your-progress",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "Your Progress",
    "text": "Your Progress\n\n\nCompleted modules: 0\n\n\n&lt;div id=\"progress-bar\" class=\"progress-bar\" role=\"progressbar\" style=\"width: 0%;\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"&gt;0%&lt;/div&gt;\n\n\n\nReset Progress"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "Getting Started",
    "text": "Getting Started\n\nChoose a day from the navigation above or the cards below\nWork through modules in order - each builds on the previous\nCheck off sections as you complete them - your progress is saved automatically\nReturn anytime - use the ‚ÄúResume‚Äù button to pick up where you left off\n\n\nReport an Issue / Ask a Question"
  },
  {
    "objectID": "tuesday/2.1-placeholder.html",
    "href": "tuesday/2.1-placeholder.html",
    "title": "2.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon.",
    "crumbs": [
      "Tuesday",
      "2.1 Placeholder Module"
    ]
  },
  {
    "objectID": "tuesday/2.1-placeholder.html#learning-objectives",
    "href": "tuesday/2.1-placeholder.html#learning-objectives",
    "title": "2.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon.",
    "crumbs": [
      "Tuesday",
      "2.1 Placeholder Module"
    ]
  },
  {
    "objectID": "tuesday/2.1-placeholder.html#section-1",
    "href": "tuesday/2.1-placeholder.html#section-1",
    "title": "2.1 Placeholder Module",
    "section": "Section 1",
    "text": "Section 1\n Mark Section 1 as complete\nPlaceholder content for Section 1.",
    "crumbs": [
      "Tuesday",
      "2.1 Placeholder Module"
    ]
  },
  {
    "objectID": "tuesday/2.1-placeholder.html#section-2",
    "href": "tuesday/2.1-placeholder.html#section-2",
    "title": "2.1 Placeholder Module",
    "section": "Section 2",
    "text": "Section 2\n Mark Section 2 as complete\nPlaceholder content for Section 2.",
    "crumbs": [
      "Tuesday",
      "2.1 Placeholder Module"
    ]
  },
  {
    "objectID": "monday/9-placer.html",
    "href": "monday/9-placer.html",
    "title": "9. PLACER",
    "section": "",
    "text": "PLACER (paper, code) stands for Protein-Ligand Atomistic Conformational Ensemble Resolver. It‚Äôs a graph neural network that operates entirely at the atomic level to generate conformational ensembles of protein-ligand complexes.",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#why-use-placer",
    "href": "monday/9-placer.html#why-use-placer",
    "title": "9. PLACER",
    "section": "Why Use PLACER?",
    "text": "Why Use PLACER?\n\nEnsemble predictions: Generates multiple conformations to capture binding uncertainty\nAll-atom accuracy: Operates at atomic level for precise interactions\nSide chain flexibility: Predicts side chain conformations alongside ligand poses\nConfidence scores: Multiple metrics for ranking and validating predictions\n\nRelated Tools: For protein-protein docking, see DiffDock-PP. For structure prediction of complexes, see Chai-1 or Boltz-2.",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#resource-requirements",
    "href": "monday/9-placer.html#resource-requirements",
    "title": "9. PLACER",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n8 GB\n16 GB\n1-3 sec/model on GPU\n\n\nCPU RAM\n16 GB\n32 GB\n~1 min/model on 8 cores\n\n\nDisk Space\n2 GB\n5 GB\nModel weights included\n\n\nPython\n3.9+\n3.10\nRequired\n\n\n\nPerformance:\n\nGPU: 1-3 seconds per model\nCPU (8 cores): ~1 minute per model\nLigands with many symmetric groups take longer",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#preparation",
    "href": "monday/9-placer.html#preparation",
    "title": "9. PLACER",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nGPU recommended for reasonable throughput",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#installation",
    "href": "monday/9-placer.html#installation",
    "title": "9. PLACER",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the repository:\n\ngit clone https://github.com/baker-laboratory/PLACER.git\ncd PLACER\nThe repository includes model weights - no separate download needed.\n\nCreate the conda environment from the provided file:\n\nmamba env create -f envs/placer_env.yml\n\nActivate the environment:\n\nmamba activate placer_env",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#testing-the-installation",
    "href": "monday/9-placer.html#testing-the-installation",
    "title": "9. PLACER",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a simple heme docking prediction:\npython run_PLACER.py \\\n    --ifile examples/inputs/dnHEM1.pdb \\\n    --odir test_output \\\n    --rerank prmsd \\\n    -n 10 \\\n    --ligand_file HEM:examples/ligands/HEM.mol2\nSuccess indicators:\n\nCommand completes without errors\ntest_output/ directory is created\nContains ranked PDB files of docked complexes\n\nExpected runtime: 30-60 seconds on GPU, 10-15 minutes on CPU.",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#hpc-job-script",
    "href": "monday/9-placer.html#hpc-job-script",
    "title": "9. PLACER",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=placer\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate placer_env\n\ncd /path/to/PLACER\n\n# Predict ligand binding with 100 samples\npython run_PLACER.py \\\n    --ifile my_complex.pdb \\\n    --odir results/ \\\n    --predict_ligand LIG-501 \\\n    --rerank prmsd \\\n    -n 100 \\\n    --ligand_file LIG:ligand.sdf",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#usage-examples",
    "href": "monday/9-placer.html#usage-examples",
    "title": "9. PLACER",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic ligand docking:\npython run_PLACER.py \\\n    -f INPUT.pdb \\\n    -o OUTPUT_DIR \\\n    -n 50\nLigand docking with cofactor fixed:\npython run_PLACER.py \\\n    --ifile 4dtz.cif \\\n    --odir output/ \\\n    --predict_ligand D-LDP-501 \\\n    --fixed_ligand C-HEM-500 \\\n    -n 100 \\\n    --rerank prmsd\nSide chain prediction (apo mode, no ligand):\npython run_PLACER.py \\\n    --ifile protein.pdb \\\n    --odir output/ \\\n    --target_res A-149 \\\n    -n 50 \\\n    --no-use_sm\nMultiple ligands simultaneously:\npython run_PLACER.py \\\n    --ifile complex.pdb \\\n    --odir output/ \\\n    --predict_multi \\\n    --predict_ligand LIG1 LIG2 \\\n    -n 100",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#key-parameters",
    "href": "monday/9-placer.html#key-parameters",
    "title": "9. PLACER",
    "section": "Key Parameters",
    "text": "Key Parameters\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\n-f or --ifile\nInput PDB/mmCIF file\n\n\n-o or --odir\nOutput directory\n\n\n-n or --nsamples\nNumber of ensemble samples (50-100 recommended)\n\n\n--rerank\nRank by confidence: prmsd, plddt, or plddt_pde\n\n\n--predict_ligand\nSpecify which ligand(s) to predict\n\n\n--fixed_ligand\nKeep certain ligands fixed in place\n\n\n--ligand_file\nProvide SDF/MOL2 for correct atom typing\n\n\n--target_res\nSpecific residue(s) for side chain prediction\n\n\n--no-use_sm\nApo mode - predict without small molecules",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#understanding-confidence-scores",
    "href": "monday/9-placer.html#understanding-confidence-scores",
    "title": "9. PLACER",
    "section": "Understanding Confidence Scores",
    "text": "Understanding Confidence Scores\nPLACER provides multiple confidence metrics:\n\n\n\n\n\n\n\n\nMetric\nDescription\nGood Values\n\n\n\n\nprmsd\nPredicted RMSD to true pose\n&lt;2.0 √Ö (excellent), &lt;4.0 √Ö (acceptable)\n\n\nplddt\nPer-residue confidence (1D track)\n&gt;0.8\n\n\nplddt_pde\nPer-residue confidence (2D track)\n&gt;0.8\n\n\nfape\nAll-atom FAPE loss\nLower is better\n\n\nrmsd\nActual RMSD to reference (if available)\n&lt;2.0 √Ö\n\n\nkabsch\nSuperimposed RMSD\nMeasures conformation accuracy\n\n\n\nRecommendation: Use --rerank prmsd for docking tasks.",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#python-api",
    "href": "monday/9-placer.html#python-api",
    "title": "9. PLACER",
    "section": "Python API",
    "text": "Python API\nPLACER can be imported as a Python module:\nimport sys\nsys.path.append(\"/path/to/PLACER\")\nimport PLACER\n\n# Load model\nplacer = PLACER.PLACER()\n\n# Set up input\npl_input = PLACER.PLACERinput()\npl_input.pdb(\"complex.pdb\")\npl_input.name(\"my_prediction\")\npl_input.ligand_reference({\"HEM\": \"heme.mol2\"})\n\n# Run 50 predictions\noutputs = placer.run(pl_input, 50)\n\n# Access results\nfor out in outputs:\n    print(f\"pRMSD: {out.prmsd:.2f}, pLDDT: {out.plddt:.2f}\")",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#understanding-the-output",
    "href": "monday/9-placer.html#understanding-the-output",
    "title": "9. PLACER",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\noutput/\n‚îú‚îÄ‚îÄ ranked_0.pdb          # Best pose by confidence\n‚îú‚îÄ‚îÄ ranked_1.pdb          # Second best\n‚îú‚îÄ‚îÄ ranked_2.pdb          # ...\n‚îú‚îÄ‚îÄ scores.csv            # All confidence metrics\n‚îî‚îÄ‚îÄ ensemble/             # All generated samples\nInterpreting ensemble results:\n\nMultiple similar poses = high confidence\nDiverse poses = uncertain binding mode\nCompare top-ranked poses to assess convergence",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#input-format-requirements",
    "href": "monday/9-placer.html#input-format-requirements",
    "title": "9. PLACER",
    "section": "Input Format Requirements",
    "text": "Input Format Requirements\nLigand must be in input structure:\n\nPLACER requires the ligand to be present in the PDB\nSMILES-only input is not supported\nUse --ligand_file to provide correct bonding information\n\nLigand file formats:\n\nSDF files: Best for drug-like molecules\nMOL2 files: Good for cofactors\nHelps with: aromatic rings, stereochemistry, bond orders",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#troubleshooting",
    "href": "monday/9-placer.html#troubleshooting",
    "title": "9. PLACER",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nNon-planar aromatic rings:\n\nProvide SDF/MOL2 file with --ligand_file\nThis ensures correct bonding information\n\nMissing ligands in PDB:\n\nLigands must be in the input structure\nSMILES-only input is not currently supported\n\nCustom/non-canonical residues:\npython run_PLACER.py \\\n    --ifile input.pdb \\\n    --residue_json custom_residues.json \\\n    --odir output/\nSlow predictions:\n\nSymmetric ligands (many equivalent atoms) are slower\nUse GPU for production runs\nReduce -n for initial testing\n\nOut of memory:\n\nPLACER is generally memory-efficient\nIf issues persist, try reducing ensemble size\nOr use CPU with multiple cores",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html",
    "href": "monday/4-esmfold.html",
    "title": "4. ESMFold",
    "section": "",
    "text": "ESMFold (paper, code) is an end-to-end single-sequence structure predictor that uses the ESM-2 language model to generate accurate 3D protein structures directly from sequence, without requiring multiple sequence alignments (MSAs).",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#why-use-esmfold",
    "href": "monday/4-esmfold.html#why-use-esmfold",
    "title": "4. ESMFold",
    "section": "Why Use ESMFold?",
    "text": "Why Use ESMFold?\n\nSpeed: Significantly faster than AlphaFold2 (seconds vs minutes)\nNo MSA required: Works directly from sequence alone\nCompetitive accuracy: Often comparable to AlphaFold2 for well-folded domains\nLower resource usage: Can run on smaller GPUs\n\nRelated Tools: For MSA-based prediction with potentially higher accuracy, see LocalColabFold or OpenFold. For protein language model embeddings only, see ESM3.",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#resource-requirements",
    "href": "monday/4-esmfold.html#resource-requirements",
    "title": "4. ESMFold",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n40+ GB\nLarger proteins need more memory\n\n\nCPU RAM\n16 GB\n32 GB\nCPU-only is possible but slow\n\n\nDisk Space\n5 GB\n10 GB\nModel weights\n\n\nPython\n‚â§3.9\n3.9\nImportant: Python 3.10+ may have issues\n\n\n\nWhy Python ‚â§3.9? ESMFold depends on OpenFold, which has compatibility issues with newer Python versions.",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#preparation",
    "href": "monday/4-esmfold.html#preparation",
    "title": "4. ESMFold",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nnvcc available (for compiling OpenFold dependencies)\n\nVerify your environment:\nnvcc --version      # Required for OpenFold compilation\nmodule load cuda    # If nvcc not found",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#installation",
    "href": "monday/4-esmfold.html#installation",
    "title": "4. ESMFold",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nCreate a conda environment with Python 3.9:\n\nmamba create -n esmfold python=3.9\nmamba activate esmfold\n\nInstall PyTorch (adjust CUDA version to match your cluster):\n\nmamba install pytorch pytorch-cuda=12.1 -c pytorch -c nvidia\n\nInstall ESM with ESMFold dependencies:\n\npip install \"fair-esm[esmfold]\"\n\nInstall OpenFold dependencies:\n\npip install 'dllogger @ git+https://github.com/NVIDIA/dllogger.git'\npip install 'openfold @ git+https://github.com/aqlaboratory/openfold.git@4b41059694619831a7db195b7e0988fc4ff3a307'\nNote: OpenFold compilation requires nvcc. If it fails, verify CUDA toolkit is loaded.\nAlternative method (using environment file):\nwget https://raw.githubusercontent.com/facebookresearch/esm/main/environment.yml\nmamba env create -f environment.yml\nmamba activate esmfold",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#testing-the-installation",
    "href": "monday/4-esmfold.html#testing-the-installation",
    "title": "4. ESMFold",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test script test_esmfold.py:\nimport torch\nimport esm\n\n# Load ESMFold model\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()  # Remove .cuda() if using CPU\n\n# Test sequence (65 residues)\nsequence = \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"\n\n# Run prediction\nwith torch.no_grad():\n    output = model.infer_pdb(sequence)\n\n# Save output\nwith open(\"test_result.pdb\", \"w\") as f:\n    f.write(output)\n\nprint(\"Structure prediction successful!\")\nprint(f\"Output saved to test_result.pdb\")\nprint(f\"Sequence length: {len(sequence)} residues\")\nRun the test:\npython test_esmfold.py\nSuccess indicators:\n\nCommand completes without errors\ntest_result.pdb file is created\nFile contains valid PDB coordinates\n\nExpected runtime: ~10-30 seconds on GPU for this small protein.",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#hpc-job-script",
    "href": "monday/4-esmfold.html#hpc-job-script",
    "title": "4. ESMFold",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=esmfold\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate esmfold\n\n# Predict structures for all sequences in FASTA file\nesm-fold -i my_proteins.fasta -o predictions/ \\\n    --num-recycles 4 \\\n    --max-tokens-per-batch 1024",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#usage-examples",
    "href": "monday/4-esmfold.html#usage-examples",
    "title": "4. ESMFold",
    "section": "Usage Examples",
    "text": "Usage Examples\nCommand line interface:\nesm-fold -i sequences.fasta -o output_pdbs/\nKey CLI options:\n\n\n\nOption\nDescription\n\n\n\n\n-i\nInput FASTA file\n\n\n-o\nOutput directory for PDB files\n\n\n--num-recycles\nNumber of recycles (default: 4)\n\n\n--max-tokens-per-batch\nBatch shorter sequences together\n\n\n--chunk-size\nReduce memory (values: 128, 64, 32)\n\n\n--cpu-only\nRun on CPU only\n\n\n--cpu-offload\nOffload to CPU RAM for long sequences\n\n\n\nReduce memory for large proteins:\nesm-fold -i large_proteins.fasta -o output/ --chunk-size 64\nProcess very long sequences:\nesm-fold -i long_sequences.fasta -o output/ --cpu-offload\nPython API:\nimport torch\nimport esm\n\n# Load model\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\n# Predict structure\nsequence = \"MVKLTAEGSEVSRQVIVQDIAYLRSLG\"\nwith torch.no_grad():\n    pdb_string = model.infer_pdb(sequence)\n\n# Save\nwith open(\"prediction.pdb\", \"w\") as f:\n    f.write(pdb_string)\nGet confidence scores:\nimport torch\nimport esm\n\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\nsequence = \"MVKLTAEGSEVSRQVIVQDIAYLRSLG\"\nwith torch.no_grad():\n    output = model.infer(sequence)\n\n# Per-residue confidence (pLDDT)\nplddt = output[\"plddt\"]  # Shape: (1, L)\nprint(f\"Mean pLDDT: {plddt.mean().item():.2f}\")\n\n# Predicted TM-score\nptm = output[\"ptm\"]\nprint(f\"pTM: {ptm.item():.3f}\")",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#understanding-the-output",
    "href": "monday/4-esmfold.html#understanding-the-output",
    "title": "4. ESMFold",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nPDB output:\n\nStandard PDB format with predicted coordinates\nB-factor column contains pLDDT confidence scores (0-100)\nHigher pLDDT = higher confidence\n\nConfidence score interpretation:\n\n\n\npLDDT Range\nInterpretation\n\n\n\n\n90-100\nVery high confidence\n\n\n70-90\nConfident\n\n\n50-70\nLow confidence (may be disordered)\n\n\n&lt;50\nVery low confidence (likely disordered)",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#memory-usage-guide",
    "href": "monday/4-esmfold.html#memory-usage-guide",
    "title": "4. ESMFold",
    "section": "Memory Usage Guide",
    "text": "Memory Usage Guide\nApproximate GPU memory by sequence length:\n\n\n\nSequence Length\nGPU Memory Needed\n\n\n\n\n&lt;200 aa\n8-16 GB\n\n\n200-400 aa\n16-24 GB\n\n\n400-600 aa\n24-40 GB\n\n\n600-1000 aa\n40-80 GB\n\n\n&gt;1000 aa\nUse --cpu-offload",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#troubleshooting",
    "href": "monday/4-esmfold.html#troubleshooting",
    "title": "4. ESMFold",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nOpenFold installation fails:\n\nVerify nvcc is available:\nnvcc --version\n# If not found:\nmodule load cuda\nEnsure PyTorch CUDA version matches system CUDA\n\n‚ÄúCUDA out of memory‚Äù:\n# Use chunking to reduce memory\nesm-fold -i input.fasta -o output/ --chunk-size 64\n\n# Or use CPU offloading for very long sequences\nesm-fold -i input.fasta -o output/ --cpu-offload\nSlow on GPU (should be fast):\n# Verify CUDA is detected\npython -c \"import torch; print(torch.cuda.is_available())\"\n# Should print: True\nPython version errors:\n\nESMFold requires Python ‚â§3.9 due to OpenFold dependencies\nCreate a new environment with Python 3.9 if needed\n\nModel download hangs:\n\nFirst run downloads ~2GB of weights\nSet custom cache location:\nexport TORCH_HOME=/scratch/$USER/torch_cache",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html",
    "href": "monday/1-localcolabfold.html",
    "title": "1. LocalColabFold",
    "section": "",
    "text": "LocalColabFold (code) is a local installation of ColabFold, which provides an efficient implementation of AlphaFold2 protein structure prediction. ColabFold combines fast MSA generation from MMseqs2 with AlphaFold2‚Äôs structure prediction capabilities, making it significantly faster than the original AlphaFold2 implementation.",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#why-use-localcolabfold",
    "href": "monday/1-localcolabfold.html#why-use-localcolabfold",
    "title": "1. LocalColabFold",
    "section": "Why Use LocalColabFold?",
    "text": "Why Use LocalColabFold?\n\nHigh-throughput predictions: Run batch jobs without Colab time limits\nNo internet dependency: All computations run locally after setup\nHPC integration: Leverage your cluster‚Äôs GPUs for faster predictions\nMSA flexibility: Use pre-computed MSAs or generate them on-the-fly\n\nRelated Tools: For structure prediction without MSAs, see ESMFold. For multi-modal complexes, see Chai-1 or Boltz-2.",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#resource-requirements",
    "href": "monday/1-localcolabfold.html#resource-requirements",
    "title": "1. LocalColabFold",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n40+ GB\nA100 recommended for proteins &gt;500 aa\n\n\nCPU RAM\n32 GB\n64 GB\nMSA generation is memory-intensive\n\n\nDisk Space\n15 GB\n100+ GB\nModel weights + optional databases\n\n\nCUDA\n11.1+\n12.1+\nCheck compatibility",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#preparation",
    "href": "monday/1-localcolabfold.html#preparation",
    "title": "1. LocalColabFold",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\n\n\n\n\n\n\nImportantPrerequisites\n\n\n\n\nCompleted HPC Setup guide\nAccess to a GPU node for testing\n~15 GB disk space for installation\n\n\n\n\n\n\n\n\n\nNoteVerify your environment\n\n\n\nnvidia-smi          # Check GPU is available\nnvcc --version      # Check CUDA version",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#installation",
    "href": "monday/1-localcolabfold.html#installation",
    "title": "1. LocalColabFold",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nDownload the installation script: Navigate to the directory where you want to install LocalColabFold (e.g., your scratch directory or apps folder).\n\nwget https://raw.githubusercontent.com/YoshitakaMo/localcolabfold/main/install_colabbatch_linux.sh\n\nMake the script executable and run it:\n\nchmod +x install_colabbatch_linux.sh\n./install_colabbatch_linux.sh\nThis creates a localcolabfold directory containing: - A conda environment (colabfold_batch) - ColabFold and all dependencies - Model weights (~10-15 GB, downloaded automatically)\nExpected install time: 15-30 minutes depending on network speed.\n\nAdd the environment to your PATH (add to ~/.bashrc for permanent access):\n\nexport PATH=\"/path/to/your/localcolabfold/colabfold-conda/bin:$PATH\"",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#testing-the-installation",
    "href": "monday/1-localcolabfold.html#testing-the-installation",
    "title": "1. LocalColabFold",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\n\nActivate the ColabFold environment:\n\nsource localcolabfold/colabfold-conda/bin/activate\n\nCreate a directory for testing and a test FASTA file:\n\nmkdir -p tests\necho \"&gt;test_protein\nMKFLKFSLLTAVLLSVVFAFSSCGDDDDTYPYDVPDYAGTCGDDDDTYPYDVPDYA\" &gt; tests/test.fasta\n\nRun prediction:\n\ncolabfold_batch tests/test.fasta tests/test_output/\nSuccess indicators:\n\nCommand completes without errors\ntests/test_output/ directory contains:\n\ntest_protein_relaxed_rank_001_*.pdb (predicted structure)\ntest_protein_scores_rank_001_*.json (confidence scores)\ntest_protein_coverage.png (MSA coverage plot)\n\n\nExpected runtime: 2-5 minutes for this small test protein.\nVerify GPU is being used:\n# In another terminal while prediction runs:\nnvidia-smi\n# Look for python process using GPU memory",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#hpc-job-script",
    "href": "monday/1-localcolabfold.html#hpc-job-script",
    "title": "1. LocalColabFold",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=colabfold\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\n# Activate environment\nsource /path/to/localcolabfold/colabfold-conda/bin/activate\n\n# Optional: Use shared database location\nexport COLABFOLD_DOWNLOAD_DIR=/shared/colabfold_db\n\n# Run prediction\ncolabfold_batch input.fasta output_dir/",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#usage-examples",
    "href": "monday/1-localcolabfold.html#usage-examples",
    "title": "1. LocalColabFold",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic prediction:\ncolabfold_batch sequences.fasta predictions/\nWith custom MSA server (if your HPC has one):\ncolabfold_batch --msa-server \"https://internal.server.edu\" sequences.fasta predictions/\nMultimer prediction (protein complexes):\n# Separate chains with : in the FASTA file\n# &gt;complex\n# SEQUENCEA:SEQUENCEB\ncolabfold_batch complex.fasta complex_output/\nBatch with templates:\ncolabfold_batch --templates sequences.fasta predictions/\nReduce memory usage (for large proteins):\ncolabfold_batch --amber --num-recycle 3 large_protein.fasta output/",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#understanding-the-output",
    "href": "monday/1-localcolabfold.html#understanding-the-output",
    "title": "1. LocalColabFold",
    "section": "Understanding the Output",
    "text": "Understanding the Output\n\n\n\n\n\n\n\nFile\nDescription\n\n\n\n\n*_relaxed_rank_001_*.pdb\nBest predicted structure (Amber-relaxed)\n\n\n*_unrelaxed_rank_001_*.pdb\nBest prediction before relaxation\n\n\n*_scores_rank_001_*.json\npLDDT and pTM scores\n\n\n*_coverage.png\nMSA coverage visualization\n\n\n*_pae.png\nPredicted Aligned Error heatmap\n\n\n\nConfidence scores:\n\npLDDT (per-residue): &gt;90 high confidence, 70-90 confident, 50-70 low, &lt;50 very low\npTM (overall): &gt;0.8 high confidence for whole structure\nPAE (pairwise): Lower is better, indicates domain organization confidence",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#troubleshooting",
    "href": "monday/1-localcolabfold.html#troubleshooting",
    "title": "1. LocalColabFold",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n\n\n\n\n\nWarningCommon Issues\n\n\n\n‚ÄúCUDA out of memory‚Äù:\n\nRequest GPU with more memory (#SBATCH --gpus=a100:1)\nUse --amber flag to reduce peak memory\nFor very large proteins (&gt;1000 aa), use Chai-1 or Boltz-2 instead\n\nMSA generation is slow:\n\nUse the MMseqs2 server option for faster MSA generation\nPre-compute MSAs for frequently used sequences\n\nDatabase location filling home directory:\n# Set in ~/.bashrc before running\nexport COLABFOLD_DOWNLOAD_DIR=/scratch/$USER/colabfold_db\nModel weights download fails:\n\nCheck network connectivity\nManually download from: https://storage.googleapis.com/alphafold/\nPlace in ~/.cache/colabfold/params/\n\nGPU not being used (slow prediction):\n# Verify CUDA is detected\npython -c \"import torch; print(torch.cuda.is_available())\"\n# Should print: True",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html",
    "href": "monday/prework-3-python.html",
    "title": "Pre-work 3: Python Refresher",
    "section": "",
    "text": "This assignment is designed as a Python refresher for biologists and bioinformaticians who may be a bit rusty. You‚Äôll complete a series of small exercises that build up to creating a simple sequence analysis tool. This assignment should take approximately 30-45 minutes to complete.\nIMPORTANT: Each Python file contains function templates with TODO comments. Your job is to fill in the missing code where you see # TODO: comments. The scripts won‚Äôt work until you complete the TODO sections!",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#overview",
    "href": "monday/prework-3-python.html#overview",
    "title": "Pre-work 3: Python Refresher",
    "section": "",
    "text": "This assignment is designed as a Python refresher for biologists and bioinformaticians who may be a bit rusty. You‚Äôll complete a series of small exercises that build up to creating a simple sequence analysis tool. This assignment should take approximately 30-45 minutes to complete.\nIMPORTANT: Each Python file contains function templates with TODO comments. Your job is to fill in the missing code where you see # TODO: comments. The scripts won‚Äôt work until you complete the TODO sections!",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#prerequisites",
    "href": "monday/prework-3-python.html#prerequisites",
    "title": "Pre-work 3: Python Refresher",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nHW1 completed (Python environment set up)",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#download-assignment-files",
    "href": "monday/prework-3-python.html#download-assignment-files",
    "title": "Pre-work 3: Python Refresher",
    "section": "Download Assignment Files",
    "text": "Download Assignment Files\n  Download Assignment Files (ZIP) \n\nDownload the ZIP file above.\nUnzip the folder on your computer.\nOpen the folder in VS Code.",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#how-to-complete-this-assignment",
    "href": "monday/prework-3-python.html#how-to-complete-this-assignment",
    "title": "Pre-work 3: Python Refresher",
    "section": "How to Complete This Assignment",
    "text": "How to Complete This Assignment\nEach Python file is a template with incomplete functions. Look for # TODO: comments - these mark where you need to write code. Read the docstrings and hints carefully, then implement the missing functionality.\nThe scripts will not work until you complete the TODO sections!\n\nPart 1: Python Basics Warmup (basics.py)\nOpen basics.py and fill in the TODO sections for these exercises: - reverse_string() - Write a function to reverse a string - count_characters() - Use a for loop to count characters in a string - amino_acid_composition() - Create a dictionary to calculate amino acid percentages - filter_sequences_by_length() - Use a list comprehension to filter sequences\nTo test your work:\n# Navigate to the unzipped directory first!\ncd path/to/python-refresher\n\nconda activate bootcamp2025_HW1\npython basics.py\nIf you see None or errors, you haven‚Äôt completed all the TODOs yet!\n\n\nPart 2: Sequence Utilities (sequence_utils.py)\nOpen sequence_utils.py and implement the TODO sections for these bioinformatics functions: - molecular_weight() - Calculate the molecular weight of a protein - count_hydrophobic() - Count hydrophobic amino acids - find_motif() - Find all positions of a motif in a sequence - count_charged_residues() - Count positive and negative charges\nTo test your work:\npython sequence_utils.py\nThe test cases at the bottom will only work once you‚Äôve filled in all the TODOs!\n\n\nPart 3: FASTA File Parsing (read_fasta.py)\nOpen read_fasta.py and complete the TODO sections to: - read_fasta() - Read and parse a FASTA file, storing sequences in a dictionary (header ‚Üí sequence) - print_fasta_stats() - Print basic statistics about the sequences\nTo test your work:\npython read_fasta.py sample.fasta\nYou should see information about each protein sequence. If not, check your TODOs!\n\n\nPart 4: Sequence Analysis Tool (analyze_sequence.py)\nOpen analyze_sequence.py and fill in the TODO sections to create a complete analysis script: - analyze_sequences() - Use your sequence_utils functions to analyze each protein - write_results() - Write the analysis results to a file\nThis part brings together everything from Parts 1-3!\nTo run your completed analysis:\npython analyze_sequence.py sample.fasta\nThis should create an analysis_results.txt file with your results. If it doesn‚Äôt work, you‚Äôre missing some TODOs!",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#tips",
    "href": "monday/prework-3-python.html#tips",
    "title": "Pre-work 3: Python Refresher",
    "section": "Tips",
    "text": "Tips\n\nTest frequently: Run your code after each function to catch errors early\nUse print statements: Debug by printing intermediate values\nRead the TODO comments carefully: They contain hints and requirements\nDon‚Äôt worry about edge cases: Focus on getting the basic functionality working\nAsk for help: If you‚Äôre stuck, reach out!",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#expected-output",
    "href": "monday/prework-3-python.html#expected-output",
    "title": "Pre-work 3: Python Refresher",
    "section": "Expected Output",
    "text": "Expected Output\nWhen you run analyze_sequence.py, you should see output similar to:\nAnalyzing sequences from sample.fasta...\n\nSequence: gene_example_1\n  Length: 120 bp\n  GC Content: 45.83%\n  Reverse Complement: CGATCG...\n\nResults written to analysis_results.txt",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#questions-and-getting-help",
    "href": "monday/prework-3-python.html#questions-and-getting-help",
    "title": "Pre-work 3: Python Refresher",
    "section": "Questions and Getting Help",
    "text": "Questions and Getting Help\nWe strongly encourage you to use our Slack workspace for questions and collaboration! One of the best ways to learn is by discussing problems with your peers and seeing how others approach challenges.\nJoin the Bootcamp Slack channel: The Slack workspace is currently reserved for in-person bootcamp participants. We apologize for the inconvenience.\nIn the Slack channel, you can: - Ask questions and get help from fellow students - Share tips and solutions you‚Äôve discovered - Learn from others‚Äô questions and answers - Collaborate on troubleshooting issues\nThe TAs and I will be active in the Slack channel to help guide discussions and answer questions. Don‚Äôt hesitate to jump in - chances are if you‚Äôre stuck on something, others are too!\nIf you have questions, please open an issue on the GitHub repository instead of emailing directly.",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html",
    "href": "monday/7-boltz2.html",
    "title": "7. Boltz-2",
    "section": "",
    "text": "Boltz-2 (paper, code) is a biomolecular foundation model that jointly models complex structures and binding affinities. It‚Äôs the first deep learning model to approach the accuracy of physics-based free-energy perturbation (FEP) methods while running 1000x faster.",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#why-use-boltz-2",
    "href": "monday/7-boltz2.html#why-use-boltz-2",
    "title": "7. Boltz-2",
    "section": "Why Use Boltz-2?",
    "text": "Why Use Boltz-2?\n\nStructure + Affinity: Predict both binding pose and binding strength\nDrug discovery ready: Affinity predictions useful for hit-to-lead optimization\nMulti-modal: Handles proteins, nucleic acids, small molecules, covalent modifications\nSpeed: 1000x faster than FEP methods for affinity prediction\n\nRelated Tools: For structure prediction only, see Chai-1. For protein-ligand docking, see PLACER or DiffDock-PP.",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#resource-requirements",
    "href": "monday/7-boltz2.html#resource-requirements",
    "title": "7. Boltz-2",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n32+ GB\nScales with complex size\n\n\nCPU RAM\n16 GB\n32 GB\nFor preprocessing\n\n\nDisk Space\n5 GB\n10 GB\nModel weights\n\n\nPython\n3.9+\n3.11\nRequired",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#preparation",
    "href": "monday/7-boltz2.html#preparation",
    "title": "7. Boltz-2",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nCUDA-capable GPU (recommended) or CPU\n\nImportant: Install Boltz in a fresh Python environment to avoid dependency conflicts.",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#installation",
    "href": "monday/7-boltz2.html#installation",
    "title": "7. Boltz-2",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nCreate a fresh environment:\n\nmamba create -n boltz python=3.11\nmamba activate boltz\n\nInstall Boltz with CUDA support:\n\npip install boltz[cuda] -U\nFor CPU-only or non-CUDA GPUs:\npip install boltz -U\nAlternative: Install from GitHub (for latest updates):\ngit clone https://github.com/jwohlwend/boltz.git\ncd boltz\npip install -e .[cuda]",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#testing-the-installation",
    "href": "monday/7-boltz2.html#testing-the-installation",
    "title": "7. Boltz-2",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test YAML file test_input.yaml:\nversion: 1\nsequences:\n  - protein:\n      id: [A, B]\n      sequence: MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\nRun prediction:\nboltz predict test_input.yaml --use_msa_server\nSuccess indicators:\n\nCommand completes without errors\nOutput directory contains:\n\nPredicted structure files (CIF format)\nConfidence scores\n\n\nExpected runtime: 1-3 minutes for this small test.",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#hpc-job-script",
    "href": "monday/7-boltz2.html#hpc-job-script",
    "title": "7. Boltz-2",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=boltz\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate boltz\n\n# Run prediction\nboltz predict my_complex.yaml --use_msa_server --out_dir results/",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#usage-examples",
    "href": "monday/7-boltz2.html#usage-examples",
    "title": "7. Boltz-2",
    "section": "Usage Examples",
    "text": "Usage Examples\nStructure prediction only:\nboltz predict structure.yaml\nWith MSA server (higher accuracy):\nboltz predict input.yaml --use_msa_server\nWith affinity prediction:\n# input.yaml\nversion: 1\nsequences:\n  - protein:\n      id: A\n      sequence: MKTVRQERLK...\n  - ligand:\n      id: L\n      smiles: \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\"\nproperties:\n  - affinity\nboltz predict input.yaml",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#input-format-yaml",
    "href": "monday/7-boltz2.html#input-format-yaml",
    "title": "7. Boltz-2",
    "section": "Input Format (YAML)",
    "text": "Input Format (YAML)\nBoltz uses YAML files to describe biomolecules:\nSimple protein:\nversion: 1\nsequences:\n  - protein:\n      id: A\n      sequence: MKTVRQERLKSIVRILERSKEPVSG...\nProtein-ligand complex:\nversion: 1\nsequences:\n  - protein:\n      id: A\n      sequence: MKTVRQERLK...\n  - ligand:\n      id: L\n      smiles: \"CCO\"\nProtein complex (homodimer):\nversion: 1\nsequences:\n  - protein:\n      id: [A, B]  # Same sequence for both chains\n      sequence: MKTVRQERLK...\nWith affinity prediction:\nversion: 1\nsequences:\n  - protein:\n      id: A\n      sequence: MKTVRQERLK...\n  - ligand:\n      id: L\n      smiles: \"CC(=O)NC1=CC=C(O)C=C1\"\nproperties:\n  - affinity\nSee prediction documentation for full format details.",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#binding-affinity-predictions",
    "href": "monday/7-boltz2.html#binding-affinity-predictions",
    "title": "7. Boltz-2",
    "section": "Binding Affinity Predictions",
    "text": "Binding Affinity Predictions\nBoltz-2 provides two affinity metrics:\n\n\n\n\n\n\n\n\nMetric\nRange\nUse Case\n\n\n\n\naffinity_probability_binary\n0-1\nHit discovery - probability that ligand is a binder\n\n\naffinity_pred_value\nlog10(IC50) in ŒºM\nLead optimization - compare binding strengths\n\n\n\nInterpretation:\n\naffinity_probability_binary: Higher = more likely to bind\naffinity_pred_value: Lower = stronger binding (lower IC50)",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#msa-server-authentication",
    "href": "monday/7-boltz2.html#msa-server-authentication",
    "title": "7. Boltz-2",
    "section": "MSA Server Authentication",
    "text": "MSA Server Authentication\nFor servers requiring authentication:\nexport BOLTZ_MSA_TOKEN=\"your_token_here\"\nboltz predict input.yaml --use_msa_server",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#understanding-the-output",
    "href": "monday/7-boltz2.html#understanding-the-output",
    "title": "7. Boltz-2",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\nboltz_results_&lt;input&gt;/\n‚îú‚îÄ‚îÄ predictions/\n‚îÇ   ‚îú‚îÄ‚îÄ model_0.cif      # Predicted structure\n‚îÇ   ‚îî‚îÄ‚îÄ confidence.json  # Confidence scores\n‚îú‚îÄ‚îÄ msa/                 # Generated MSAs (if using server)\n‚îî‚îÄ‚îÄ affinity/            # Affinity predictions (if requested)\nConfidence metrics:\n\npLDDT: Per-residue confidence\npTM: Predicted TM-score\ninterface pTM: For complexes",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#performance-comparison",
    "href": "monday/7-boltz2.html#performance-comparison",
    "title": "7. Boltz-2",
    "section": "Performance Comparison",
    "text": "Performance Comparison\n\n\n\nMethod\nSpeed\nAffinity Accuracy\n\n\n\n\nFEP (physics-based)\nHours-days\nGold standard\n\n\nBoltz-2\nSeconds-minutes\nComparable to FEP\n\n\nTraditional docking\nSeconds\nLower accuracy",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#troubleshooting",
    "href": "monday/7-boltz2.html#troubleshooting",
    "title": "7. Boltz-2",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nInstallation issues:\n\nUse a fresh environment\nTry removing [cuda] if CUDA issues arise\nVerify CUDA version compatibility\n\n‚ÄúMSA server error‚Äù:\n\nCheck network connectivity\nVerify authentication token if required\nTry without --use_msa_server for testing\n\nOut of memory:\n\nRequest more GPU memory\nReduce complex size\nTry CPU-only mode for testing\n\nSlow without GPU:\n\nCPU mode is functional but significantly slower\nAlways use GPU for production runs\n\nYAML parsing errors:\n\nCheck YAML syntax (indentation matters)\nEnsure SMILES strings are quoted\nVerify sequence format",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/5-openfold.html",
    "href": "monday/5-openfold.html",
    "title": "5. OpenFold (Optional)",
    "section": "",
    "text": "OpenFold (paper, code) is a faithful, trainable PyTorch reproduction of DeepMind‚Äôs AlphaFold2. It achieves performance comparable to AlphaFold2 and provides a fully open-source implementation for protein structure prediction.",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#why-use-openfold",
    "href": "monday/5-openfold.html#why-use-openfold",
    "title": "5. OpenFold (Optional)",
    "section": "Why Use OpenFold?",
    "text": "Why Use OpenFold?\n\nFull transparency: Open-source model architecture and training code\nTrainable: Can be fine-tuned or retrained on custom data\nResearch-friendly: Ideal for understanding how structure prediction works\nMSA-based accuracy: Uses evolutionary information for high-accuracy predictions\n\nRelated Tools: For faster predictions without MSAs, see ESMFold. For a more user-friendly MSA-based option, see LocalColabFold.",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#resource-requirements",
    "href": "monday/5-openfold.html#resource-requirements",
    "title": "5. OpenFold (Optional)",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n40+ GB\nA100 for large proteins\n\n\nCPU RAM\n32 GB\n64+ GB\nMSA generation is memory-intensive\n\n\nDisk Space\n500 GB\n2+ TB\nSequence databases are large\n\n\nCUDA\n11.3+\n12.1+\nRequired for compilation\n\n\n\nNote: OpenFold requires significant disk space for sequence databases if generating MSAs locally. Check if your HPC already has AlphaFold/OpenFold databases installed.",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#preparation",
    "href": "monday/5-openfold.html#preparation",
    "title": "5. OpenFold (Optional)",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nnvcc available for CUDA compilation\nSignificant disk space (or access to shared databases)\n\nCheck for existing databases:\n# Ask your HPC admins or check common locations\nls /shared/databases/alphafold/\nls /shared/databases/openfold/\nMany HPCs have pre-installed AlphaFold databases that OpenFold can use.",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#installation",
    "href": "monday/5-openfold.html#installation",
    "title": "5. OpenFold (Optional)",
    "section": "Installation",
    "text": "Installation\n Mark as complete\nImportant: OpenFold installation can be complex. The official documentation at openfold.readthedocs.io has the most current instructions.\n\nClone the repository:\n\ngit clone https://github.com/aqlaboratory/openfold.git\ncd openfold\n\nCreate the conda environment:\n\nmamba env create -f environment.yml\nmamba activate openfold_venv\nExpected time: 10-20 minutes for environment creation.\n\nInstall OpenFold:\n\npip install -e .\n\nDownload model weights:\n\nbash scripts/download_openfold_params.sh openfold/resources\nExpected download: ~1-2 GB of model weights.\n\n(Optional) Download sequence databases for MSA generation:\n\n# This downloads ~2TB of data - skip if using HPC shared databases\nbash scripts/download_alphafold_dbs.sh /path/to/database/directory",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#testing-the-installation",
    "href": "monday/5-openfold.html#testing-the-installation",
    "title": "5. OpenFold (Optional)",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test FASTA file test.fasta:\n&gt;test_protein\nMKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\nRun a prediction (using pre-computed MSAs or without MSAs for testing):\npython run_pretrained_openfold.py \\\n    test.fasta \\\n    /path/to/database/directory \\\n    --output_dir predictions/ \\\n    --config_preset model_1_ptm \\\n    --model_device cuda:0\nNote: For testing without databases, you can use --use_precomputed_alignments with a directory containing pre-computed MSA files.\nSuccess indicators:\n\nCommand completes without errors\npredictions/ directory contains PDB files\nOutput includes confidence metrics (pLDDT, pTM)\n\nExpected runtime: 5-30 minutes depending on MSA availability and protein size.",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#hpc-job-script",
    "href": "monday/5-openfold.html#hpc-job-script",
    "title": "5. OpenFold (Optional)",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=openfold\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --time=08:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc # Source shell profile if needed\nmamba activate openfold_venv\n\ncd /path/to/openfold\n\n# Using HPC shared databases\nDATABASE_DIR=/shared/databases/alphafold\n\npython run_pretrained_openfold.py \\\n    my_protein.fasta \\\n    $DATABASE_DIR \\\n    --output_dir predictions/ \\\n    --config_preset model_1_ptm \\\n    --model_device cuda:0",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#usage-examples",
    "href": "monday/5-openfold.html#usage-examples",
    "title": "5. OpenFold (Optional)",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic prediction with local databases:\npython run_pretrained_openfold.py \\\n    input.fasta \\\n    /path/to/databases \\\n    --output_dir output/ \\\n    --config_preset model_1_ptm\nUsing pre-computed MSAs:\npython run_pretrained_openfold.py \\\n    input.fasta \\\n    /path/to/databases \\\n    --use_precomputed_alignments /path/to/msas/ \\\n    --output_dir output/\nMultiple model presets (ensemble):\nfor preset in model_1_ptm model_2_ptm model_3_ptm; do\n    python run_pretrained_openfold.py \\\n        input.fasta \\\n        /path/to/databases \\\n        --config_preset $preset \\\n        --output_dir output_${preset}/\ndone",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#model-presets",
    "href": "monday/5-openfold.html#model-presets",
    "title": "5. OpenFold (Optional)",
    "section": "Model Presets",
    "text": "Model Presets\n\n\n\nPreset\nDescription\n\n\n\n\nmodel_1_ptm\nStandard model with pTM head\n\n\nmodel_2_ptm\nAlternative model with pTM\n\n\nmodel_3_ptm\nThird model variant\n\n\nmodel_1_multimer_v3\nFor protein complexes",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#understanding-the-output",
    "href": "monday/5-openfold.html#understanding-the-output",
    "title": "5. OpenFold (Optional)",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\npredictions/\n‚îú‚îÄ‚îÄ test_protein_model_1_ptm_unrelaxed.pdb    # Predicted structure\n‚îú‚îÄ‚îÄ test_protein_model_1_ptm_confidences.json # Confidence scores\n‚îî‚îÄ‚îÄ test_protein_model_1_ptm_timings.json     # Runtime statistics\nConfidence metrics:\n\npLDDT: Per-residue confidence (0-100, higher is better)\npTM: Predicted TM-score (0-1, &gt;0.8 is confident)\nPAE: Predicted Aligned Error matrix",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#database-requirements",
    "href": "monday/5-openfold.html#database-requirements",
    "title": "5. OpenFold (Optional)",
    "section": "Database Requirements",
    "text": "Database Requirements\nIf generating MSAs locally, OpenFold needs these databases:\n\n\n\nDatabase\nSize\nPurpose\n\n\n\n\nBFD\n~1.7 TB\nSequence alignments\n\n\nMGnify\n~120 GB\nMetagenomic sequences\n\n\nUniRef90\n~100 GB\nSequence clustering\n\n\nUniRef30\n~200 GB\nHHblits templates\n\n\nPDB70\n~60 GB\nStructure templates\n\n\n\nTotal: ~2+ TB\nCheck HPC shared databases first - most research HPCs have these pre-installed.",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#troubleshooting",
    "href": "monday/5-openfold.html#troubleshooting",
    "title": "5. OpenFold (Optional)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nCompilation errors during install:\n# Ensure CUDA toolkit is loaded\nmodule load cuda/12.1\nnvcc --version\n\n# Clean and retry\npip uninstall openfold\npip install -e .\n‚ÄúDatabase not found‚Äù errors:\n\nVerify database paths exist\nCheck HPC documentation for shared database locations\nContact HPC admins about AlphaFold database availability\n\nOut of memory:\n\nRequest more GPU memory\nReduce --max_recycling_iters\nUse gradient checkpointing for training\n\nSlow MSA generation:\n\nMSA generation is CPU-bound and can take hours\nUse pre-computed MSAs when possible\nConsider using ColabFold‚Äôs MMseqs2 server instead\n\nModel weights not found:\n# Re-download weights\nbash scripts/download_openfold_params.sh openfold/resources\n\n# Verify files exist\nls openfold/resources/*.pt",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#for-researchers-training",
    "href": "monday/5-openfold.html#for-researchers-training",
    "title": "5. OpenFold (Optional)",
    "section": "For Researchers: Training",
    "text": "For Researchers: Training\nOpenFold can be retrained or fine-tuned:\npython train_openfold.py \\\n    /path/to/training/data \\\n    /path/to/template_mmcif \\\n    /path/to/output \\\n    --config_preset initial_training\nSee the training documentation for details.",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html",
    "href": "monday/0-hpc-setup.html",
    "title": "0. Common HPC Setup",
    "section": "",
    "text": "Before installing individual ML tools, ensure your HPC environment is properly configured. This guide covers the foundational setup that all subsequent modules depend on.",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#resource-requirements-overview",
    "href": "monday/0-hpc-setup.html#resource-requirements-overview",
    "title": "0. Common HPC Setup",
    "section": "Resource Requirements Overview",
    "text": "Resource Requirements Overview\nMost ML protein tools share similar computational requirements. Here‚Äôs a general guide:\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n40+ GB\nA100 80GB ideal for large proteins\n\n\nCPU RAM\n32 GB\n64 GB\nMore for MSA generation\n\n\nDisk Space\n50 GB\n200+ GB\nModel weights + databases\n\n\nCUDA\n11.6+\n12.1+\nCheck tool-specific requirements",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#checking-your-hpc-environment",
    "href": "monday/0-hpc-setup.html#checking-your-hpc-environment",
    "title": "0. Common HPC Setup",
    "section": "Checking Your HPC Environment",
    "text": "Checking Your HPC Environment\n Mark as complete\n\n\n\n\n\n\nTipInternet Access on HPC\n\n\n\nMany HPC clusters do not have internet access on compute nodes (the nodes where your heavy jobs run). They often only have internet on ‚Äúlogin‚Äù or ‚Äúhead‚Äù nodes.\n\nDownloads: Always run installation and download commands on a login node.\nExecution: When running jobs, ensure your tools don‚Äôt try to download models on the fly. Pre-download all weights and databases.\n\n\n\n\n1. Check Available CUDA Modules\nmodule avail cuda\nThis shows all CUDA versions installed on your cluster. Note the versions - you‚Äôll need to match them to tool requirements.\n\n\n2. Check GPU Availability\nRequest an interactive GPU session:\n# SLURM example\nsrun --gpus=1 --pty bash\nThen check GPU status:\nnvidia-smi\nThis shows: - GPU model (A100, V100, RTX 4090, etc.) - GPU memory (important for large models) - Current CUDA driver version\n\n\n3. Check CUDA Toolkit Version\nnvcc --version\nIf this fails, load a CUDA module first:\nmodule load cuda/12.1\nnvcc --version",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#condamamba-setup",
    "href": "monday/0-hpc-setup.html#condamamba-setup",
    "title": "0. Common HPC Setup",
    "section": "Conda/Mamba Setup",
    "text": "Conda/Mamba Setup\n Mark as complete\nMost tools use Conda environments. Mamba is recommended as it‚Äôs significantly faster than Conda for dependency resolution.\n\nInstalling Mamba (if not available)\nIf your HPC doesn‚Äôt have Mamba, install Miniforge:\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\nbash Miniforge3-Linux-x86_64.sh\nFollow prompts, then restart your shell or run:\nsource ~/.bashrc\n\n\nBest Practices for HPC Conda Usage\n\nUse dedicated environment directories: Set environment location to avoid filling home directory quota:\n\n# Add to ~/.condarc\nenvs_dirs:\n  - /scratch/$USER/conda_envs\npkgs_dirs:\n  - /scratch/$USER/conda_pkgs\n\nOne environment per tool: Don‚Äôt try to install all tools in one environment - dependency conflicts are common.\nExport environments for reproducibility:\n\nmamba env export &gt; environment.yml",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#docker-vs-singularityapptainer",
    "href": "monday/0-hpc-setup.html#docker-vs-singularityapptainer",
    "title": "0. Common HPC Setup",
    "section": "Docker vs Singularity/Apptainer",
    "text": "Docker vs Singularity/Apptainer\n Mark as complete\nIMPORTANT: Most academic HPCs do NOT support Docker for security reasons. Use Singularity or Apptainer instead.\n\nLoading Container Runtime\nmodule load apptainer\n# or on older systems:\nmodule load singularity\n\n\nConverting Docker Commands to Apptainer\nMany tool READMEs show Docker commands. Here‚Äôs how to translate them:\n\n\n\nDocker Command\nApptainer Equivalent\n\n\n\n\ndocker run\napptainer run\n\n\ndocker run --gpus all\napptainer run --nv\n\n\ndocker run -v /path:/path\napptainer run --bind /path:/path\n\n\ndocker pull image:tag\napptainer pull docker://image:tag\n\n\n\nExample conversion:\n# Docker (won't work on HPC):\ndocker run --gpus all -v $(pwd):/workspace myimage:latest python script.py\n\n# Apptainer (works on HPC):\napptainer run --nv --bind $(pwd):/workspace myimage.sif python script.py\n\n\nPulling Docker Images as Singularity Files\napptainer pull docker://nvcr.io/nvidia/pytorch:23.10-py3\n# Creates: pytorch_23.10-py3.sif",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#slurm-job-submission-basics",
    "href": "monday/0-hpc-setup.html#slurm-job-submission-basics",
    "title": "0. Common HPC Setup",
    "section": "SLURM Job Submission Basics",
    "text": "SLURM Job Submission Basics\n Mark as complete\nMost HPCs use SLURM for job scheduling. Here‚Äôs a template for ML jobs:\n#!/bin/bash\n#SBATCH --job-name=my_ml_job\n#SBATCH --partition=gpu          # GPU partition name (varies by cluster)\n#SBATCH --gpus=1                  # Number of GPUs\n#SBATCH --cpus-per-task=8         # CPUs for data loading\n#SBATCH --mem=64G                 # RAM\n#SBATCH --time=04:00:00           # Wall time (HH:MM:SS)\n#SBATCH --output=%x_%j.out        # Output file (%x=job name, %j=job ID)\n#SBATCH --error=%x_%j.err         # Error file\n\n# Load required modules\nmodule load cuda/12.1\nmodule load apptainer\n\n# Activate conda environment\n# source ~/.bashrc  # Source your shell profile if needed\nsource /path/to/your/miniconda3/etc/profile.d/conda.sh # Better: source conda.sh directly\nmamba activate my_env\n\n# Run your command\npython my_script.py\n\nCommon SLURM Commands\n\n\n\nCommand\nDescription\n\n\n\n\nsbatch script.sh\nSubmit job\n\n\nsqueue -u $USER\nCheck your jobs\n\n\nscancel JOB_ID\nCancel a job\n\n\nsinfo\nShow partition info\n\n\nsacct -j JOB_ID\nJob accounting info\n\n\n\n\n\nGPU Partition Names\nGPU partition names vary by cluster. Common names:\n\ngpu, gpus, gpu-shared\na100, v100, rtx\ngpu-debug (for testing)\n\nCheck your cluster‚Äôs documentation or run sinfo to see available partitions.",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#environment-variables",
    "href": "monday/0-hpc-setup.html#environment-variables",
    "title": "0. Common HPC Setup",
    "section": "Environment Variables",
    "text": "Environment Variables\n Mark as complete\nSeveral tools use environment variables. Add these to your ~/.bashrc:\n# Model weight storage (prevents filling home directory)\nexport TORCH_HOME=/scratch/$USER/torch_cache\nexport HF_HOME=/scratch/$USER/huggingface_cache\nexport TRANSFORMERS_CACHE=/scratch/$USER/transformers_cache\n\n# ColabFold databases\nexport COLABFOLD_DOWNLOAD_DIR=/scratch/$USER/colabfold_db\n\n# Chai-1 models\nexport CHAI_DOWNLOADS_DIR=/scratch/$USER/chai_models\n\n# General cache\nexport XDG_CACHE_HOME=/scratch/$USER/.cache\nReplace /scratch/$USER with your cluster‚Äôs scratch or work directory path.",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#verifying-gpu-works-with-pytorch",
    "href": "monday/0-hpc-setup.html#verifying-gpu-works-with-pytorch",
    "title": "0. Common HPC Setup",
    "section": "Verifying GPU Works with PyTorch",
    "text": "Verifying GPU Works with PyTorch\n Mark as complete\nAfter setting up an environment with PyTorch, verify GPU access:\nimport torch\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"GPU count: {torch.cuda.device_count()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n    # Quick computation test\n    x = torch.randn(1000, 1000, device='cuda')\n    y = torch.matmul(x, x)\n    print(\"GPU computation test: PASSED\")\nSave as test_gpu.py and run:\npython test_gpu.py\nExpected output (example):\nPyTorch version: 2.1.0\nCUDA available: True\nCUDA version: 12.1\nGPU count: 1\nGPU name: NVIDIA A100-SXM4-80GB\nGPU memory: 84.9 GB\nGPU computation test: PASSED",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#understanding-gpu-memory-requirements",
    "href": "monday/0-hpc-setup.html#understanding-gpu-memory-requirements",
    "title": "0. Common HPC Setup",
    "section": "Understanding GPU Memory Requirements",
    "text": "Understanding GPU Memory Requirements\nDifferent tasks require different GPU memory:\n\n\n\nTask\nTypical GPU Memory\n\n\n\n\nStructure prediction (small protein &lt;200 aa)\n8-16 GB\n\n\nStructure prediction (large protein &gt;500 aa)\n32-80 GB\n\n\nProtein design (RFdiffusion2)\n16-32 GB\n\n\nDocking (DiffDock-PP, PLACER)\n8-16 GB\n\n\nLanguage model inference (ESM3)\n16-40 GB\n\n\nBinder design (BindCraft)\n32-80 GB\n\n\n\nIf you get out-of-memory errors: 1. Request a GPU with more memory 2. Reduce batch size or sequence length 3. Use CPU offloading if available 4. Process sequences in chunks",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#troubleshooting-common-issues",
    "href": "monday/0-hpc-setup.html#troubleshooting-common-issues",
    "title": "0. Common HPC Setup",
    "section": "Troubleshooting Common Issues",
    "text": "Troubleshooting Common Issues\n\n‚ÄúCUDA out of memory‚Äù\n\nRequest more GPU memory\nReduce batch size\nUse gradient checkpointing if training\n\n\n\n‚ÄúNo CUDA runtime found‚Äù\nmodule load cuda/12.1  # Load CUDA module\nnvcc --version         # Verify it loaded\n\n\n‚ÄúSingularity: command not found‚Äù\nmodule load apptainer  # or: module load singularity\n\n\nConda environment activation fails in SLURM\nAdd to your job script:\n# Source conda.sh directly (adjust path to your installation)\nsource /path/to/miniforge3/etc/profile.d/conda.sh\nmamba activate my_env\n\n\nPermission denied on container\nchmod +x container.sif",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html",
    "href": "monday/3-rfdiffusion2.html",
    "title": "3. RFdiffusion2",
    "section": "",
    "text": "RFdiffusion2 (paper, code) is a protein design model capable of atom-level active site scaffolding. It extends the original RFdiffusion to enable precise control over protein-ligand interactions at the atomic level.",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#why-use-rfdiffusion2",
    "href": "monday/3-rfdiffusion2.html#why-use-rfdiffusion2",
    "title": "3. RFdiffusion2",
    "section": "Why Use RFdiffusion2?",
    "text": "Why Use RFdiffusion2?\n\nAtomic-level control: Design proteins with precise active site geometries\nLigand scaffolding: Build proteins around small molecules with atomic accuracy\nMotif grafting: Incorporate functional motifs into new scaffolds\nFlexible backbone design: Generate novel folds with specific functional constraints\n\nRelated Tools: Use with LigandMPNN for sequence design after backbone generation. For the earlier version without atomic control, see RFdiffusion All Atom (Optional).",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#resource-requirements",
    "href": "monday/3-rfdiffusion2.html#resource-requirements",
    "title": "3. RFdiffusion2",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\n\n\n\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n32+ GB\nA100 for larger designs\n\n\nCPU RAM\n16 GB\n32 GB\nContainer-based execution\n\n\nDisk Space\n10 GB\n20 GB\nContainer + weights\n\n\nContainer\nApptainer/Singularity\nRequired\nNo native Docker on HPC",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#preparation",
    "href": "monday/3-rfdiffusion2.html#preparation",
    "title": "3. RFdiffusion2",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nApptainer/Singularity available on your cluster\nCUDA-capable GPU\n\nVerify your environment:\nmodule load apptainer    # or: module load singularity\napptainer --version\nnvidia-smi\nImportant: RFdiffusion2 uses containers. Most academic HPCs do NOT support Docker for security reasons - use Apptainer/Singularity instead.",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#installation",
    "href": "monday/3-rfdiffusion2.html#installation",
    "title": "3. RFdiffusion2",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the repository:\n\ngit clone https://github.com/RosettaCommons/RFdiffusion2.git\ncd RFdiffusion2\n\nAdd the repo to your PYTHONPATH (add to ~/.bashrc):\n\nexport PYTHONPATH=\"/path/to/your/RFdiffusion2:$PYTHONPATH\"\n\nDownload the model weights and container:\n\npython setup.py\nExpected download: ~5-10 GB (container + weights). This can take 30+ minutes.\nIf download is interrupted:\npython setup.py overwrite\n\nVerify Apptainer/Singularity is available:\n\nmodule load apptainer\n# or: module load singularity\nThe downloaded .sif file in rf_diffusion/exec/ is the Singularity container.",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#testing-the-installation",
    "href": "monday/3-rfdiffusion2.html#testing-the-installation",
    "title": "3. RFdiffusion2",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a demo case:\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=open_source_demo \\\n    sweep.benchmarks=active_site_unindexed_atomic_partial_ligand\nNote: Omit --nv flag if running without GPU (will be very slow).\nSuccess indicators:\n\nCommand completes without errors\nOutput directory created at pipeline_outputs/&lt;timestamp&gt;_open_source_demo/\nContains PDB files with designed structures\n\nExpected runtime: 5-15 minutes on GPU, 30+ minutes on CPU.",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#hpc-job-script",
    "href": "monday/3-rfdiffusion2.html#hpc-job-script",
    "title": "3. RFdiffusion2",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=rfdiff2\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load apptainer\nmodule load cuda/12.1\n\ncd /path/to/RFdiffusion2\n\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=open_source_demo",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#usage-examples",
    "href": "monday/3-rfdiffusion2.html#usage-examples",
    "title": "3. RFdiffusion2",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic backbone design:\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=my_config\nWith custom output directory:\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=open_source_demo \\\n    sweep.output_dir=/path/to/output\nMultiple design benchmarks:\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=open_source_demo \\\n    sweep.benchmarks=\"[benchmark1,benchmark2]\"",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#docker-to-apptainer-translation",
    "href": "monday/3-rfdiffusion2.html#docker-to-apptainer-translation",
    "title": "3. RFdiffusion2",
    "section": "Docker to Apptainer Translation",
    "text": "Docker to Apptainer Translation\nThe official documentation may show Docker commands. Here‚Äôs how to translate:\n\n\n\nDocker Command\nApptainer Equivalent\n\n\n\n\ndocker run --gpus all image\napptainer exec --nv image.sif\n\n\ndocker run -v /path:/path\napptainer exec --bind /path:/path\n\n\n-it (interactive)\napptainer shell --nv\n\n\n\nExample conversion:\n# Docker (won't work on HPC):\ndocker run --gpus all -v $(pwd):/workspace rfdiffusion:latest python script.py\n\n# Apptainer (works on HPC):\napptainer exec --nv --bind $(pwd):/workspace rfdiffusion.sif python script.py",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#understanding-the-output",
    "href": "monday/3-rfdiffusion2.html#understanding-the-output",
    "title": "3. RFdiffusion2",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput structure:\npipeline_outputs/\n‚îî‚îÄ‚îÄ &lt;timestamp&gt;_&lt;config_name&gt;/\n    ‚îú‚îÄ‚îÄ designs/\n    ‚îÇ   ‚îú‚îÄ‚îÄ design_0.pdb    # Designed backbone\n    ‚îÇ   ‚îú‚îÄ‚îÄ design_1.pdb\n    ‚îÇ   ‚îî‚îÄ‚îÄ ...\n    ‚îú‚îÄ‚îÄ logs/\n    ‚îÇ   ‚îî‚îÄ‚îÄ run.log         # Execution log\n    ‚îî‚îÄ‚îÄ config.yaml         # Configuration used",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#configuration-system",
    "href": "monday/3-rfdiffusion2.html#configuration-system",
    "title": "3. RFdiffusion2",
    "section": "Configuration System",
    "text": "Configuration System\nRFdiffusion2 uses Hydra for configuration. Key config options:\n\n\n\nParameter\nDescription\n\n\n\n\nsweep.benchmarks\nWhich design task(s) to run\n\n\nsweep.output_dir\nOutput directory\n\n\ndiffuser.T\nNumber of diffusion timesteps\n\n\ninference.num_designs\nNumber of designs to generate",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#troubleshooting",
    "href": "monday/3-rfdiffusion2.html#troubleshooting",
    "title": "3. RFdiffusion2",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n‚ÄúNo GPU available‚Äù / extremely slow:\n\nEnsure --nv flag is included\nVerify GPU allocation: nvidia-smi\nLoad CUDA module: module load cuda/12.1\n\nContainer permission errors:\nchmod +x rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif\n‚ÄúFileNotFoundError‚Äù for weights:\n\nRe-run python setup.py to ensure all files downloaded\nCheck rf_diffusion/weights/ directory exists\n\nContainer not found:\n\nProvide full path to .sif file\nOr run from the RFdiffusion2 directory\n\nSetup script hangs during download:\n\nLarge files may take 30+ minutes\nCheck network connectivity\nIf interrupted, run python setup.py overwrite\n\nModule not found errors inside container:\n\nEnsure PYTHONPATH is set correctly\nContainer may need --bind for additional paths",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This bootcamp covers machine learning tools for protein structure prediction and design.\nMore information coming soon."
  },
  {
    "objectID": "thursday/index.html",
    "href": "thursday/index.html",
    "title": "Thursday",
    "section": "",
    "text": "Thursday content coming soon.",
    "crumbs": [
      "Thursday"
    ]
  },
  {
    "objectID": "thursday/index.html#overview",
    "href": "thursday/index.html#overview",
    "title": "Thursday",
    "section": "",
    "text": "Thursday content coming soon.",
    "crumbs": [
      "Thursday"
    ]
  },
  {
    "objectID": "thursday/index.html#modules",
    "href": "thursday/index.html#modules",
    "title": "Thursday",
    "section": "Modules",
    "text": "Modules\n\n\n\nModule\nTopic\nStatus\n\n\n\n\n4.1\nPlaceholder Module\nComing soon\n\n\n\nMore modules will be added here.\n\n\n\n‚Üê Wednesday\n\n\nBack to Home\n\n\nCapstone ‚Üí",
    "crumbs": [
      "Thursday"
    ]
  }
]