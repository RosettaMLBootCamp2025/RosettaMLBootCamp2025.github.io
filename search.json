[
  {
    "objectID": "thursday/4.1-placeholder.html",
    "href": "thursday/4.1-placeholder.html",
    "title": "4.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon.",
    "crumbs": [
      "Thursday",
      "4.1 Placeholder Module"
    ]
  },
  {
    "objectID": "thursday/4.1-placeholder.html#learning-objectives",
    "href": "thursday/4.1-placeholder.html#learning-objectives",
    "title": "4.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon.",
    "crumbs": [
      "Thursday",
      "4.1 Placeholder Module"
    ]
  },
  {
    "objectID": "thursday/4.1-placeholder.html#section-1",
    "href": "thursday/4.1-placeholder.html#section-1",
    "title": "4.1 Placeholder Module",
    "section": "Section 1",
    "text": "Section 1\n Mark Section 1 as complete\nPlaceholder content for Section 1.",
    "crumbs": [
      "Thursday",
      "4.1 Placeholder Module"
    ]
  },
  {
    "objectID": "thursday/4.1-placeholder.html#section-2",
    "href": "thursday/4.1-placeholder.html#section-2",
    "title": "4.1 Placeholder Module",
    "section": "Section 2",
    "text": "Section 2\n Mark Section 2 as complete\nPlaceholder content for Section 2.",
    "crumbs": [
      "Thursday",
      "4.1 Placeholder Module"
    ]
  },
  {
    "objectID": "capstone/targets/trka.html",
    "href": "capstone/targets/trka.html",
    "title": "TrkA Receptor Deep Dive",
    "section": "",
    "text": "Tropomyosin receptor kinase A (TrkA) is the high-affinity catalytic receptor for Nerve Growth Factor (NGF).\nWhy it matters: The TrkA pathway is essential for the development and survival of neurons. However, it is also implicated in chronic pain and some cancers.\nThe Goal: Design a binder that interacts with the extracellular domain of TrkA. This could be an antagonist to treat pain (blocking NGF) or a tool to study receptor activation.",
    "crumbs": [
      "Capstone",
      "TrkA Receptor Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/trka.html#biological-context",
    "href": "capstone/targets/trka.html#biological-context",
    "title": "TrkA Receptor Deep Dive",
    "section": "",
    "text": "Tropomyosin receptor kinase A (TrkA) is the high-affinity catalytic receptor for Nerve Growth Factor (NGF).\nWhy it matters: The TrkA pathway is essential for the development and survival of neurons. However, it is also implicated in chronic pain and some cancers.\nThe Goal: Design a binder that interacts with the extracellular domain of TrkA. This could be an antagonist to treat pain (blocking NGF) or a tool to study receptor activation.",
    "crumbs": [
      "Capstone",
      "TrkA Receptor Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/trka.html#interactive-structure",
    "href": "capstone/targets/trka.html#interactive-structure",
    "title": "TrkA Receptor Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nThe viewer below shows the extracellular domain of TrkA (Chain A) in complex with NGF (Chain B).\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "TrkA Receptor Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/trka.html#design-mission",
    "href": "capstone/targets/trka.html#design-mission",
    "title": "TrkA Receptor Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nDesign a protein that binds to the NGF-binding face of TrkA.\n\nTarget Specifications\n\n\n\n\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nTrkA Receptor\n\n\nPDB ID\n1WWC\n\n\nTarget Chain\nChain A (TrkA)\n\n\nBinder to Mimic\nChain B (NGF)\n\n\nInterface / Hotspot\nThe residues on Chain A that contact Chain B.\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 1WWC.\nClean the structure: Isolate Chain A.\nInterface Definition: Use PyMOL to find residues on Chain A within 5Å of Chain B.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "TrkA Receptor Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/gm3.html",
    "href": "capstone/targets/gm3.html",
    "title": "GM2 Activator Protein Deep Dive",
    "section": "",
    "text": "GM2 Activator Protein (GM2AP) is a small glycoprotein required for the degradation of gangliosides (types of lipids) in the lysosome. You may also see it referenced in the context of GM3 pathway interactions.\nWhy it matters: Defects in this protein cause GM2 gangliosidosis (AB variant), a severe neurodegenerative storage disease.\nThe Goal: Design a binder that can stabilize this protein or mimic its interaction partners.",
    "crumbs": [
      "Capstone",
      "GM2 Activator Protein Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/gm3.html#biological-context",
    "href": "capstone/targets/gm3.html#biological-context",
    "title": "GM2 Activator Protein Deep Dive",
    "section": "",
    "text": "GM2 Activator Protein (GM2AP) is a small glycoprotein required for the degradation of gangliosides (types of lipids) in the lysosome. You may also see it referenced in the context of GM3 pathway interactions.\nWhy it matters: Defects in this protein cause GM2 gangliosidosis (AB variant), a severe neurodegenerative storage disease.\nThe Goal: Design a binder that can stabilize this protein or mimic its interaction partners.",
    "crumbs": [
      "Capstone",
      "GM2 Activator Protein Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/gm3.html#interactive-structure",
    "href": "capstone/targets/gm3.html#interactive-structure",
    "title": "GM2 Activator Protein Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nThe viewer below shows the GM2 Activator Protein.\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "GM2 Activator Protein Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/gm3.html#design-mission",
    "href": "capstone/targets/gm3.html#design-mission",
    "title": "GM2 Activator Protein Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nCreate a binder that interacts with the lipid-binding cavity or surface loops of GM2AP.\n\nTarget Specifications\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nGM2 Activator Protein\n\n\nPDB ID\n1G13\n\n\nTarget Chain\nChain A\n\n\nInterface / Hotspot\nThe hydrophobic cavity or surface loops.\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 1G13.\nSurface Analysis: Identify the opening of the lipid-binding cavity. This is a challenging but interesting target for “pocket” binding.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "GM2 Activator Protein Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/tem-1.html",
    "href": "capstone/targets/tem-1.html",
    "title": "TEM-1 Beta-Lactamase Deep Dive",
    "section": "",
    "text": "TEM-1 Beta-Lactamase is an enzyme produced by bacteria that confers resistance to penicillin and other beta-lactam antibiotics. It works by breaking the antibiotic molecule apart.\nWhy it matters: This is a classic target for “Binder Design” as a means of inhibition. If you can design a protein that binds tightly to the active site or an allosteric site, you might stop the enzyme from working, making the bacteria susceptible to antibiotics again.\nThe Goal: Design a binder that inhibits TEM-1 function. This is often done by targeting the active site groove.",
    "crumbs": [
      "Capstone",
      "TEM-1 Beta-Lactamase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/tem-1.html#biological-context",
    "href": "capstone/targets/tem-1.html#biological-context",
    "title": "TEM-1 Beta-Lactamase Deep Dive",
    "section": "",
    "text": "TEM-1 Beta-Lactamase is an enzyme produced by bacteria that confers resistance to penicillin and other beta-lactam antibiotics. It works by breaking the antibiotic molecule apart.\nWhy it matters: This is a classic target for “Binder Design” as a means of inhibition. If you can design a protein that binds tightly to the active site or an allosteric site, you might stop the enzyme from working, making the bacteria susceptible to antibiotics again.\nThe Goal: Design a binder that inhibits TEM-1 function. This is often done by targeting the active site groove.",
    "crumbs": [
      "Capstone",
      "TEM-1 Beta-Lactamase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/tem-1.html#interactive-structure",
    "href": "capstone/targets/tem-1.html#interactive-structure",
    "title": "TEM-1 Beta-Lactamase Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nThe viewer below shows TEM-1 Beta-Lactamase.\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "TEM-1 Beta-Lactamase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/tem-1.html#design-mission",
    "href": "capstone/targets/tem-1.html#design-mission",
    "title": "TEM-1 Beta-Lactamase Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nCreate a binder that targets the active site cleft of TEM-1.\n\nTarget Specifications\n\n\n\n\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nTEM-1 Beta-Lactamase\n\n\nPDB ID\n1FQG\n\n\nTarget Chain\nChain A\n\n\nInterface / Hotspot\nThe active site pocket (look for where inhibitors typically bind).\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 1FQG.\nIdentify the Pocket: Use tools like PyMOL to locate the concave active site.\nTargeting: When using RFdiffusion, you may want to specify “binder”contigs that fill this specific pocket.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "TEM-1 Beta-Lactamase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/pd-l1.html",
    "href": "capstone/targets/pd-l1.html",
    "title": "PD-L1 Target Deep Dive",
    "section": "",
    "text": "Programmed Death-Ligand 1 (PD-L1) is a critical protein expressed on the surface of many cells, including cancer cells. Under normal conditions, it acts as a “brake” on the immune system. When PD-L1 binds to the PD-1 receptor on T-cells, it signals the T-cell to become inactive.\nWhy it matters: Cancer cells often hijack this mechanism by over-expressing PD-L1. This allows them to “trick” the immune system into ignoring the tumor.\nThe Goal: By designing a protein binder that sticks tightly to PD-L1, we can block it from interacting with PD-1. This releases the “brake,” allowing T-cells to recognize and attack the cancer.",
    "crumbs": [
      "Capstone",
      "PD-L1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/pd-l1.html#biological-context",
    "href": "capstone/targets/pd-l1.html#biological-context",
    "title": "PD-L1 Target Deep Dive",
    "section": "",
    "text": "Programmed Death-Ligand 1 (PD-L1) is a critical protein expressed on the surface of many cells, including cancer cells. Under normal conditions, it acts as a “brake” on the immune system. When PD-L1 binds to the PD-1 receptor on T-cells, it signals the T-cell to become inactive.\nWhy it matters: Cancer cells often hijack this mechanism by over-expressing PD-L1. This allows them to “trick” the immune system into ignoring the tumor.\nThe Goal: By designing a protein binder that sticks tightly to PD-L1, we can block it from interacting with PD-1. This releases the “brake,” allowing T-cells to recognize and attack the cancer.",
    "crumbs": [
      "Capstone",
      "PD-L1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/pd-l1.html#interactive-structure",
    "href": "capstone/targets/pd-l1.html#interactive-structure",
    "title": "PD-L1 Target Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nExplore the native interaction between PD-L1 (the target) and PD-1 (the natural binder) below. * Target (PD-L1): The surface we want to bind to. * Binder (PD-1): The natural partner we want to compete with.\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "PD-L1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/pd-l1.html#design-mission",
    "href": "capstone/targets/pd-l1.html#design-mission",
    "title": "PD-L1 Target Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nYour objective is to create a de novo protein binder that binds to the same interface on PD-L1 that PD-1 currently occupies.\n\nTarget Specifications\n\n\n\n\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nPD-L1 (Programmed Death-Ligand 1)\n\n\nPDB ID\n4ZQK\n\n\nTarget Chain\nChain B\n\n\nInterface / Hotspot\nResidues 54-66 and 113-123 (approximate binding loop area)\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 4ZQK.\nClean the structure: Keep Chain B (PD-L1) as the target. Remove Chain A (PD-1) and any water molecules.\nDefine the Interface: When running tools like RFdiffusion or BindCraft, specify the hotspot residues listed above to guide the diffusion process to the correct face of the protein.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "PD-L1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/index.html",
    "href": "capstone/index.html",
    "title": "Capstone Project",
    "section": "",
    "text": "Binder design is often formulated as “given a target protein (and optionally a specific epitope on that protein), design a (often smaller) protein capable of binding the target”. It is one of the most ubiquitous protein design problems and has many potential applications, largely functioning by blocking or competing with natural interactions involving the target protein.\nIn this bootcamp, you’ll have the opportunity to explore the computational binder design problem across three different categories of targets. Students will choose from a curated list of targets that have been validated with BindCraft and other computational binder design tools.",
    "crumbs": [
      "Capstone",
      "Capstone Project"
    ]
  },
  {
    "objectID": "capstone/index.html#overview",
    "href": "capstone/index.html#overview",
    "title": "Capstone Project",
    "section": "",
    "text": "Binder design is often formulated as “given a target protein (and optionally a specific epitope on that protein), design a (often smaller) protein capable of binding the target”. It is one of the most ubiquitous protein design problems and has many potential applications, largely functioning by blocking or competing with natural interactions involving the target protein.\nIn this bootcamp, you’ll have the opportunity to explore the computational binder design problem across three different categories of targets. Students will choose from a curated list of targets that have been validated with BindCraft and other computational binder design tools.",
    "crumbs": [
      "Capstone",
      "Capstone Project"
    ]
  },
  {
    "objectID": "capstone/index.html#target-categories-and-options",
    "href": "capstone/index.html#target-categories-and-options",
    "title": "Capstone Project",
    "section": "Target Categories and Options",
    "text": "Target Categories and Options\n\nImmune Checkpoint / Receptor Targets\n\n\n\nTarget\nUniProt\nDescription\n\n\n\n\nPD-L1\nQ9NZQ7\nImmune Checkpoint\n\n\nIFNAR2\nP48551\nInterferon receptor\n\n\nIL-7Rα\nP16871\nInterleukin receptor\n\n\n\n\n\nAntibody-like Targets\n\n\n\nTarget\nUniProt\nDescription\n\n\n\n\nBet v 1\nP15494\nBirch pollen allergen\n\n\n\n\n\nEnzyme / Small Molecule Binders\n\n\n\nTarget\nPDB\nDescription\n\n\n\n\nTrkA receptor\n1WWC\nNerve Growth Factor Receptor\n\n\nTEM-1 Beta-Lactamase\n1FQG\nAntibiotic Resistance Enzyme\n\n\nGM2 Activator Protein\n1G13\nLipid Transport Protein\n\n\nBeta-Glucosidase\n2JIE\nEnzymatic Catalyst",
    "crumbs": [
      "Capstone",
      "Capstone Project"
    ]
  },
  {
    "objectID": "capstone/index.html#project-structure",
    "href": "capstone/index.html#project-structure",
    "title": "Capstone Project",
    "section": "Project Structure",
    "text": "Project Structure\nSelect a target from the list above and work through the binder design process. As you learn about each tool in the preceding lessons, explore how that tool works for your particular protein. Be sure to make note of any quirks or nuances related to your protein, e.g. any particular settings used or any issues encountered.\n\n\n\n\n\n\nTipDocumentation Tip\n\n\n\nKeep a lab notebook (digital or physical) to document your predictions, results, settings tried, and commands used. This will be invaluable when synthesizing your findings at the end of the project.",
    "crumbs": [
      "Capstone",
      "Capstone Project"
    ]
  },
  {
    "objectID": "capstone/index.html#goals",
    "href": "capstone/index.html#goals",
    "title": "Capstone Project",
    "section": "Goals",
    "text": "Goals\n\nGain hands-on experience running DL-based tools on specific proteins\nTroubleshoot and address any issues related to your target protein\nLearn about tips and tricks used to get more favorable outputs\nGain experience communicating about computational tools and settings\nGain experience interpreting results and exploring various settings",
    "crumbs": [
      "Capstone",
      "Capstone Project"
    ]
  },
  {
    "objectID": "capstone/index.html#synthesizing-your-work",
    "href": "capstone/index.html#synthesizing-your-work",
    "title": "Capstone Project",
    "section": "Synthesizing Your Work",
    "text": "Synthesizing Your Work\nA key part of learning is synthesizing and communicating what you’ve done. In the live workshop, students gave 20-minute group presentations. For self-paced learners, consider one of these formats to consolidate your learning:\n\nWritten report summarizing your process and findings\nVideo walkthrough explaining your approach and results\nJupyter notebook with narrative text alongside code and figures\nGitHub repository with a comprehensive README documenting your project\n\n\n\n\n\n\n\nTip\n\n\n\nEven if you’re not presenting to anyone, creating slides is a great way to learn! The process of organizing your work into a clear narrative helps solidify your understanding.\n\n\nHowever you choose to document your work, aim to cover the following:\n\nBackground and introduction to target protein\nStructure prediction results\nBackbone generation results\nSequence design results\nDesign rationale - explain the specific choices, settings, and approaches used and why\n\n\n\n\n\n\n\nNote\n\n\n\nI find the design rationale section to be the most helpful to my learning and understanding.\n\n\n\nResults from miscellaneous tools\nSummary and takeaways\n\n\n\n← Thursday\nBack to Home",
    "crumbs": [
      "Capstone",
      "Capstone Project"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html",
    "href": "monday/2-ligandmpnn.html",
    "title": "2. LigandMPNN",
    "section": "",
    "text": "LigandMPNN (paper, code) is a deep learning model for context-aware protein sequence design. It extends ProteinMPNN to handle small molecules, metal ions, and other non-protein components in protein design tasks.",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#why-use-ligandmpnn",
    "href": "monday/2-ligandmpnn.html#why-use-ligandmpnn",
    "title": "2. LigandMPNN",
    "section": "Why Use LigandMPNN?",
    "text": "Why Use LigandMPNN?\n\nLigand-aware design: Design sequences that account for bound cofactors, substrates, or drug molecules\nContext preservation: Maintain interactions with metals, DNA, RNA, or other molecules\nSide chain packing: Evaluate and optimize side chain conformations\nFlexible residue control: Fix, bias, or vary specific positions\n\nRelated Tools: Use with RFdiffusion2 for backbone design, or BindCraft for complete binder design pipelines.",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#resource-requirements",
    "href": "monday/2-ligandmpnn.html#resource-requirements",
    "title": "2. LigandMPNN",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n4 GB\n16 GB\nScales with protein size\n\n\nCPU RAM\n8 GB\n16 GB\nCPU-only is viable but slower\n\n\nDisk Space\n2 GB\n5 GB\nModel weights\n\n\nPython\n3.9+\n3.11\nRequired",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#preparation",
    "href": "monday/2-ligandmpnn.html#preparation",
    "title": "2. LigandMPNN",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nGit installed\n\nVerify your environment:\npython --version    # Should be 3.9+\nnvcc --version      # For GPU support (optional)",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#installation",
    "href": "monday/2-ligandmpnn.html#installation",
    "title": "2. LigandMPNN",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the LigandMPNN repository:\n\ngit clone https://github.com/dauparas/LigandMPNN.git\ncd LigandMPNN\n\nDownload the model parameters: Note: This step requires internet access. If your compute node doesn’t have internet, run this on a login node.\n\nbash get_model_params.sh \"./model_params\"\nExpected download: ~500 MB of model weights.\n\nCreate a new conda environment:\n\nmamba create -n ligandmpnn_env python=3.11\nmamba activate ligandmpnn_env\n\nInstall dependencies:\n\npip install -r requirements.txt\nThis installs PyTorch, NumPy, and ProDy for PDB file handling.",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#testing-the-installation",
    "href": "monday/2-ligandmpnn.html#testing-the-installation",
    "title": "2. LigandMPNN",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a test design on the provided example structure:\npython run.py \\\n    --seed 111 \\\n    --pdb_path \"./inputs/1BC8.pdb\" \\\n    --out_folder \"./outputs/test_output\"\nSuccess indicators:\n\nCommand completes without errors\nOutput folder contains:\n\nseqs/1BC8.fa - Designed sequences in FASTA format\nbackbones/1BC8.pdb - Input backbone (for reference)\npacked/1BC8_1.pdb - Structure with designed side chains\n\n\nExpected runtime: &lt;1 minute on GPU, ~5 minutes on CPU.",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#hpc-job-script",
    "href": "monday/2-ligandmpnn.html#hpc-job-script",
    "title": "2. LigandMPNN",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=ligandmpnn\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc  # Optional: Source shell profile if needed\nmamba activate ligandmpnn_env\n\ncd /path/to/LigandMPNN\n\npython run.py \\\n    --model_type \"ligand_mpnn\" \\\n    --seed 111 \\\n    --pdb_path \"./inputs/my_protein.pdb\" \\\n    --out_folder \"./outputs/my_design\" \\\n    --number_of_batches 10",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#usage-examples",
    "href": "monday/2-ligandmpnn.html#usage-examples",
    "title": "2. LigandMPNN",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic protein design (no ligand):\npython run.py \\\n    --pdb_path \"protein.pdb\" \\\n    --out_folder \"output/\"\nDesign with ligand context:\npython run.py \\\n    --model_type \"ligand_mpnn\" \\\n    --pdb_path \"protein_ligand.pdb\" \\\n    --out_folder \"output/\"\nFix specific residues (keep them unchanged):\npython run.py \\\n    --pdb_path \"protein.pdb\" \\\n    --fixed_residues \"A10 A20 A30\" \\\n    --out_folder \"output/\"\nDesign only specific positions:\npython run.py \\\n    --pdb_path \"protein.pdb\" \\\n    --redesigned_residues \"A50 A51 A52 A53\" \\\n    --out_folder \"output/\"\nBatch processing multiple structures:\n# Create a JSON file listing inputs\necho '{\"1\": \"input1.pdb\", \"2\": \"input2.pdb\"}' &gt; input_list.json\n\npython run.py \\\n    --pdb_path_multi \"input_list.json\" \\\n    --out_folder \"batch_output/\"\nWith temperature control (higher = more diverse):\npython run.py \\\n    --pdb_path \"protein.pdb\" \\\n    --temperature 0.2 \\\n    --out_folder \"output/\"",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#key-parameters",
    "href": "monday/2-ligandmpnn.html#key-parameters",
    "title": "2. LigandMPNN",
    "section": "Key Parameters",
    "text": "Key Parameters\n\n\n\n\n\n\n\n\nParameter\nDescription\nDefault\n\n\n\n\n--model_type\nModel variant: protein_mpnn, ligand_mpnn, soluble_mpnn, etc.\nprotein_mpnn\n\n\n--temperature\nSampling temperature (0.1-1.0). Lower = more conservative\n0.1\n\n\n--number_of_batches\nNumber of sequences to generate\n1\n\n\n--batch_size\nSequences per batch\n1\n\n\n--fixed_residues\nSpace-separated residues to keep unchanged\nNone\n\n\n--redesigned_residues\nOnly design these residues\nAll\n\n\n--bias_AA\nBias toward specific amino acids\nNone",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#model-types",
    "href": "monday/2-ligandmpnn.html#model-types",
    "title": "2. LigandMPNN",
    "section": "Model Types",
    "text": "Model Types\n\n\n\nModel\nUse Case\n\n\n\n\nprotein_mpnn\nStandard protein sequence design\n\n\nligand_mpnn\nDesign with small molecule context\n\n\nsoluble_mpnn\nBias toward soluble sequences\n\n\nglobal_label_membrane_mpnn\nMembrane protein design\n\n\nper_residue_label_membrane_mpnn\nFine-grained membrane design",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#understanding-the-output",
    "href": "monday/2-ligandmpnn.html#understanding-the-output",
    "title": "2. LigandMPNN",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\noutput/\n├── seqs/\n│   └── protein.fa          # Designed sequences\n├── backbones/\n│   └── protein.pdb         # Input structure\n└── packed/\n    ├── protein_1.pdb       # Design 1 with side chains\n    └── protein_2.pdb       # Design 2 with side chains\nFASTA output format:\n&gt;protein, score=1.234, seq_recovery=0.456\nMVKLTAEGSE...\n\nscore: Negative log-likelihood (lower = better fit to backbone)\nseq_recovery: Fraction matching native sequence (if provided)",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-ligandmpnn.html#troubleshooting",
    "href": "monday/2-ligandmpnn.html#troubleshooting",
    "title": "2. LigandMPNN",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n“RuntimeError: CUDA out of memory”:\n\nUse CPU instead: remove CUDA module and run without GPU\nReduce --batch_size\nLigandMPNN is efficient; usually not memory-limited\n\nPDB parsing errors:\n\nEnsure PDB has proper formatting\nRemove alternate conformations: keep only “A” conformers\nCheck that ligand has proper atom naming\n\nLigand not recognized:\n\nEnsure ligand is in the PDB file with HETATM records\nUse --ligand flag to specify ligand residue name\nCheck that ligand coordinates are reasonable\n\nLow sequence diversity:\n\nIncrease --temperature (e.g., 0.2 or 0.3)\nIncrease --number_of_batches\nUse different random seeds\n\nSide chain clashes in output:\n\nThis is expected - downstream relaxation is recommended\nUse PyRosetta or Rosetta FastRelax\nOr validate with your structure prediction tool of choice",
    "crumbs": [
      "Monday",
      "2. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html",
    "href": "monday/10-bindcraft.html",
    "title": "10. BindCraft",
    "section": "",
    "text": "BindCraft (paper, code) is an end-to-end binder design pipeline that combines AlphaFold2 backpropagation, ProteinMPNN, and PyRosetta to design protein binders against target proteins.",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#why-use-bindcraft",
    "href": "monday/10-bindcraft.html#why-use-bindcraft",
    "title": "10. BindCraft",
    "section": "Why Use BindCraft?",
    "text": "Why Use BindCraft?\n\nComplete pipeline: Integrates structure prediction, sequence design, and scoring\nAutomated optimization: Multi-stage design with confidence-based filtering\nProduction ready: Validated binders in published work\nLearning resource: See how professional protein design pipelines work\n\nRelated Tools: For backbone design, see RFdiffusion2. For sequence design, see LigandMPNN. For structure prediction, see LocalColabFold.",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#resource-requirements",
    "href": "monday/10-bindcraft.html#resource-requirements",
    "title": "10. BindCraft",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n24 GB\n32+ GB\nLarge targets need more\n\n\nCPU RAM\n32 GB\n64 GB\nFor PyRosetta scoring\n\n\nDisk Space\n2 MB + 5.3 GB\n10 GB\nCode + AlphaFold2 weights\n\n\nTime\nHours\nDays\nPer design campaign\n\n\n\nImportant: BindCraft requires PyRosetta. Ensure you have a valid installation or license if required by your institution.",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#preparation",
    "href": "monday/10-bindcraft.html#preparation",
    "title": "10. BindCraft",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nPyRosetta installed (or accessible via license)\nCUDA-compatible GPU\n\nCheck your CUDA version:\nnvcc --version\n# Note the version number (e.g., 12.4)",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#installation",
    "href": "monday/10-bindcraft.html#installation",
    "title": "10. BindCraft",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the BindCraft repository:\n\ngit clone https://github.com/martinpacesa/BindCraft /path/to/bindcraft\ncd /path/to/bindcraft\n\nRun the installation script:\n\nbash install_bindcraft.sh --cuda '12.4' --pkg_manager 'conda'\nImportant options:\n\nReplace 12.4 with your actual CUDA version\nUse --pkg_manager 'mamba' for faster installation\nIf --cuda is left blank, auto-detection may fail\n\nExpected time: 20-40 minutes.\nThe script creates a conda environment called BindCraft with all dependencies.",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#testing-the-installation",
    "href": "monday/10-bindcraft.html#testing-the-installation",
    "title": "10. BindCraft",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\n\nActivate the environment:\n\nconda activate BindCraft\n\nRun a test design against the example target (PDL1):\n\ncd /path/to/bindcraft\npython -u ./bindcraft.py \\\n    --settings './settings_target/PDL1.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'\nSuccess indicators:\n\nStarts generating trajectories without errors\nLog shows design iterations progressing\nCreates output directory with design files\n\nNote: A complete run takes hours to days. For testing, stop after a few trajectories complete (Ctrl+C).",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#hpc-job-script",
    "href": "monday/10-bindcraft.html#hpc-job-script",
    "title": "10. BindCraft",
    "section": "HPC Job Script",
    "text": "HPC Job Script\nUsing the provided template:\nsbatch ./bindcraft.slurm \\\n    --settings './settings_target/PDL1.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'\nOr create your own:\n#!/bin/bash\n#SBATCH --job-name=bindcraft\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --time=48:00:00\n#SBATCH --output=%x_%j.out\n\n# source ~/.bashrc\nconda activate BindCraft\n\ncd /path/to/bindcraft\n\npython -u ./bindcraft.py \\\n    --settings './settings_target/my_target.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#setting-up-your-own-target",
    "href": "monday/10-bindcraft.html#setting-up-your-own-target",
    "title": "10. BindCraft",
    "section": "Setting Up Your Own Target",
    "text": "Setting Up Your Own Target\nStep 1: Prepare your target PDB\n\nPlace your target protein PDB in the BindCraft folder\nTrim unnecessary chains/residues to reduce memory and speed up design\n\nStep 2: Create target settings (settings_target/my_target.json):\n{\n    \"design_path\": \"./my_binder_designs\",\n    \"binder_name\": \"my_binder\",\n    \"starting_pdb\": \"./my_target.pdb\",\n    \"chains\": \"A\",\n    \"target_hotspot_residues\": \"A10-20\",\n    \"lengths\": \"50-100\",\n    \"number_of_final_designs\": 100\n}\nStep 3: Run the pipeline:\npython -u ./bindcraft.py \\\n    --settings './settings_target/my_target.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#key-settings-explained",
    "href": "monday/10-bindcraft.html#key-settings-explained",
    "title": "10. BindCraft",
    "section": "Key Settings Explained",
    "text": "Key Settings Explained\n\nTarget Settings (settings_target/*.json)\n\n\n\n\n\n\n\n\nSetting\nDescription\nExample\n\n\n\n\nstarting_pdb\nPath to target structure\n\"./my_target.pdb\"\n\n\nchains\nWhich chain(s) to target\n\"A\" or \"A,B\"\n\n\ntarget_hotspot_residues\nResidues to target\n\"A10-20\" or null (auto)\n\n\nlengths\nBinder length range\n\"50-100\"\n\n\nnumber_of_final_designs\nDesigns to generate\n100\n\n\n\n\n\nFilter Settings (settings_filters/*.json)\nControls which designs pass quality thresholds:\n\nConfidence scores (pLDDT, pTM, i_pTM)\nInterface quality (shape complementarity, energy)\nDefault filters are good starting points\n\n\n\nAdvanced Settings (settings_advanced/*.json)\n\nDesign algorithm (default: 4-stage)\nNumber of iterations per stage\nAlphaFold2 and ProteinMPNN parameters",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#understanding-the-pipeline",
    "href": "monday/10-bindcraft.html#understanding-the-pipeline",
    "title": "10. BindCraft",
    "section": "Understanding the Pipeline",
    "text": "Understanding the Pipeline\nBindCraft demonstrates a complete protein design workflow:\n1. DESIGN        → AlphaFold2 backpropagation generates binder backbones\n       ↓\n2. OPTIMIZE      → ProteinMPNN designs sequences for backbones\n       ↓\n3. VALIDATE      → AlphaFold2 predicts designed complex structure\n       ↓\n4. SCORE         → PyRosetta evaluates interface quality\n       ↓\n5. FILTER        → Keep designs passing confidence thresholds",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#tips-for-success",
    "href": "monday/10-bindcraft.html#tips-for-success",
    "title": "10. BindCraft",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nTrim your target: Remove unnecessary chains/residues to reduce memory\nStart with defaults: Use default filter and advanced settings initially\nGenerate enough designs: Aim for 100+ final designs (top 5-20 for experiments)\nBe patient: Expect hundreds to thousands of trajectories for enough accepted binders\nMonitor acceptance rate: Low acceptance → adjust design weights or filters\nCheck the wiki: BindCraft Wiki",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#understanding-the-output",
    "href": "monday/10-bindcraft.html#understanding-the-output",
    "title": "10. BindCraft",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\nmy_binder_designs/\n├── accepted/\n│   ├── design_001.pdb        # Passing designs\n│   ├── design_002.pdb\n│   └── ...\n├── rejected/                  # Filtered out designs\n├── trajectories/              # All generated trajectories\n├── scores.csv                 # All metrics for each design\n└── summary.txt                # Run statistics",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/10-bindcraft.html#troubleshooting",
    "href": "monday/10-bindcraft.html#troubleshooting",
    "title": "10. BindCraft",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nGPU memory errors:\n\nReduce target PDB size (trim chains)\nRequest more GPU memory (32+ GB recommended)\nCheck with: nvidia-smi\n\nCUDA version mismatch:\n\nRe-run install with correct CUDA version\nCheck: nvcc --version\n\nLow acceptance rate (few designs pass filters):\n\nAdjust design weights in advanced settings\nRelax filter thresholds\nChange target hotspot selection\nIncrease target site area\n\nPyRosetta license errors:\n\nVerify PyRosetta license is valid\nCheck license file location\nContact PyRosetta for academic license\n\nSlow progress:\n\nThis is normal - protein design takes time\nMonitor trajectory count, not clock time\nLarge targets are slower",
    "crumbs": [
      "Monday",
      "10. BindCraft"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html",
    "href": "monday/8-diffdock-pp.html",
    "title": "8. DiffDock-PP",
    "section": "",
    "text": "DiffDock-PP (paper, code) is a graph neural network trained for de-noising of rigid transformations (rotation and translation) to predict protein-protein docking orientations between two rigid protein subunits.",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#why-use-diffdock-pp",
    "href": "monday/8-diffdock-pp.html#why-use-diffdock-pp",
    "title": "8. DiffDock-PP",
    "section": "Why Use DiffDock-PP?",
    "text": "Why Use DiffDock-PP?\n\nFast protein-protein docking: Predicts binding orientations without expensive sampling\nValidation tool: Orthogonally validate structure predictions from other methods\nEnsemble predictions: Generate multiple docking poses for uncertainty estimation\nRigid-body docking: Efficient for cases where backbone flexibility is minimal\n\nRelated Tools: For protein-ligand docking, see PLACER. For flexible ligand binding with ensemble generation, see PLACER. For structure prediction of complexes, see Chai-1 or Boltz-2.",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#resource-requirements",
    "href": "monday/8-diffdock-pp.html#resource-requirements",
    "title": "8. DiffDock-PP",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n8 GB\n16 GB\nScales with protein size\n\n\nCPU RAM\n8 GB\n16 GB\nFor preprocessing\n\n\nDisk Space\n2 GB\n5 GB\nModel weights\n\n\nCUDA\n11.6\n11.6-11.7\nSpecific version required",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#preparation",
    "href": "monday/8-diffdock-pp.html#preparation",
    "title": "8. DiffDock-PP",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nCUDA 11.6 or 11.7 available\n\nCheck CUDA availability:\nmodule avail cuda\n# Look for cuda/11.6 or cuda/11.7",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#installation",
    "href": "monday/8-diffdock-pp.html#installation",
    "title": "8. DiffDock-PP",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the DiffDock-PP repository:\n\ngit clone https://github.com/ketatam/DiffDock-PP.git\ncd DiffDock-PP\n\nCreate a new environment:\n\nmamba create -n diffdock_pp python=3.9\nmamba activate diffdock_pp\n\nInstall PyTorch with CUDA 11.6:\n\nmamba install pytorch=1.13.0 pytorch-cuda=11.6 -c pytorch -c nvidia\nWhy CUDA 11.6? DiffDock-PP was developed and tested with this version. Using different versions may cause compatibility issues with PyG.\n\nInstall PyTorch Geometric (PyG) packages:\n\nmamba install pytorch-scatter pytorch-sparse pytorch-cluster pytorch-spline-conv pyg -c pyg\n\nInstall remaining dependencies:\n\nmamba install mkl=2024.0 \"numpy&lt;2.0\" dill tqdm pyyaml pandas biopandas scikit-learn biopython e3nn wandb tensorboard tensorboardX matplotlib\nWhy numpy&lt;2.0? NumPy 2.0 introduced breaking changes that affect many scientific packages. Keeping NumPy below 2.0 ensures compatibility.",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#testing-the-installation",
    "href": "monday/8-diffdock-pp.html#testing-the-installation",
    "title": "8. DiffDock-PP",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\n\nCreate required directories:\n\nmkdir storage\n\nRun the test script on the DB5 benchmark:\n\nbash src/db5_inference.sh\nSuccess indicators:\n\nCommand completes without errors\nOutput folder visualization/epoch-0/ is created\nDirectory contains PDB files of docked complexes (multiple .pdb files)\n\nExpected runtime: 5-15 minutes depending on GPU.\nVerify output:\nls visualization/epoch-0/*.pdb | wc -l\n# Should show multiple PDB files",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#hpc-job-script",
    "href": "monday/8-diffdock-pp.html#hpc-job-script",
    "title": "8. DiffDock-PP",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=diffdock_pp\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/11.6\n\n# source ~/.bashrc\nmamba activate diffdock_pp\n\ncd /path/to/DiffDock-PP\n\n# Create output directory\nmkdir -p storage\n\n# Run inference\nbash src/db5_inference.sh",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#usage-examples",
    "href": "monday/8-diffdock-pp.html#usage-examples",
    "title": "8. DiffDock-PP",
    "section": "Usage Examples",
    "text": "Usage Examples\nRun DB5 benchmark (default test):\nbash src/db5_inference.sh\nCustom docking (requires understanding the codebase):\nDiffDock-PP requires input data in a specific format. For custom proteins:\n\nPrepare receptor and ligand PDB files\nCreate data configuration files\nRun inference script\n\nSee the repository documentation for detailed input format requirements.",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#understanding-the-output",
    "href": "monday/8-diffdock-pp.html#understanding-the-output",
    "title": "8. DiffDock-PP",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput structure:\nvisualization/\n└── epoch-0/\n    ├── complex_1_pose_0.pdb    # Docking pose 1\n    ├── complex_1_pose_1.pdb    # Docking pose 2\n    ├── complex_1_pose_2.pdb    # Docking pose 3\n    └── ...                      # More poses\nEach PDB file contains:\n\nBoth protein chains with predicted relative orientation\nMultiple poses represent different docking predictions\nCompare poses to assess uncertainty",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#use-cases",
    "href": "monday/8-diffdock-pp.html#use-cases",
    "title": "8. DiffDock-PP",
    "section": "Use Cases",
    "text": "Use Cases\n\nProtein-Protein Docking: Predict binding orientations between protein chains\nComplex Validation: Validate predicted protein-protein interfaces from other methods\nEnsemble Generation: Generate multiple docking poses to capture uncertainty\nBenchmarking: Compare against other docking methods",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#when-to-use-diffdock-pp-vs-other-tools",
    "href": "monday/8-diffdock-pp.html#when-to-use-diffdock-pp-vs-other-tools",
    "title": "8. DiffDock-PP",
    "section": "When to Use DiffDock-PP vs Other Tools",
    "text": "When to Use DiffDock-PP vs Other Tools\n\n\n\nTool\nBest For\n\n\n\n\nDiffDock-PP\nRigid protein-protein docking\n\n\nPLACER\nProtein-ligand docking with conformational sampling\n\n\nChai-1/Boltz-2\nAb initio complex structure prediction\n\n\nBindCraft\nDe novo binder design",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/8-diffdock-pp.html#troubleshooting",
    "href": "monday/8-diffdock-pp.html#troubleshooting",
    "title": "8. DiffDock-PP",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nPyG installation fails:\n\nEnsure CUDA toolkit is loaded before installing\nInstall PyTorch first, then PyG packages\nVerify versions match:\npython -c \"import torch; print(torch.__version__, torch.cuda.is_available())\"\n\nCUDA version mismatch:\nCheck your system’s CUDA version:\nnvcc --version\nThis should match the pytorch-cuda version (11.6). If not:\nmodule load cuda/11.6\n“ModuleNotFoundError” for PyG components:\n\nInstall all PyG packages together:\nmamba install pytorch-scatter pytorch-sparse pytorch-cluster pytorch-spline-conv pyg -c pyg\n\nNumPy errors:\n\nEnsure numpy&lt;2.0 is installed\nDon’t upgrade NumPy even if prompted\n\nGPU not detected:\n# Verify CUDA is available to PyTorch\npython -c \"import torch; print(torch.cuda.is_available())\"\n# Should print: True\nEmpty output directory:\n\nCheck for error messages in terminal output\nVerify input files exist and are formatted correctly\nEnsure storage/ directory was created",
    "crumbs": [
      "Monday",
      "8. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html",
    "href": "monday/prework-1-env-git.html",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "",
    "text": "This assignment ensures you have properly set up your development environment and can use GitHub for version control. By completing this assignment, you will:\n\nInstall Anaconda (if not already installed)\nCreate a conda environment with all required packages for the bootcamp\nVerify your installation with a Python script\nCommit and push your verification results to GitHub",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#overview",
    "href": "monday/prework-1-env-git.html#overview",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "",
    "text": "This assignment ensures you have properly set up your development environment and can use GitHub for version control. By completing this assignment, you will:\n\nInstall Anaconda (if not already installed)\nCreate a conda environment with all required packages for the bootcamp\nVerify your installation with a Python script\nCommit and push your verification results to GitHub",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#about-autograding",
    "href": "monday/prework-1-env-git.html#about-autograding",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "About Autograding",
    "text": "About Autograding\nNote: This assignment has automated tests that run when you push your code. You’ll see a score out of 100 points. These points are just for feedback - they help you know if you’ve completed everything correctly. This is a bootcamp focused on learning, not grades!",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#prerequisites",
    "href": "monday/prework-1-env-git.html#prerequisites",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nWindows Users: Install WSL (Windows Subsystem for Linux)\nIMPORTANT for Windows Users: This bootcamp requires PyRosetta, which only runs on Unix-based systems (Linux/Mac). Windows users must use Windows Subsystem for Linux (WSL) to complete this assignment.\nInstall WSL:\n\nOpen PowerShell or Windows Command Prompt as Administrator (right-click and “Run as administrator”)\nInstall WSL with Ubuntu:\nwsl --install\nRestart your computer when prompted\nAfter restart, Ubuntu will automatically open and ask you to create a username and password. Remember these credentials!\nUpdate your WSL installation:\nsudo apt update && sudo apt upgrade -y\n\nUsing WSL: - Launch Ubuntu from your Start menu, or type wsl in PowerShell/Command Prompt - All commands in this assignment should be run in the WSL Ubuntu terminal - Your Windows files are accessible at /mnt/c/ (C: drive), /mnt/d/ (D: drive), etc. - We recommend working in your Linux home directory: cd ~\nFor more detailed WSL documentation, see: https://learn.microsoft.com/en-us/windows/wsl/install\n\n\nGitHub Account\nIf you don’t have a GitHub account yet, create one at: https://github.com/signup\n\n\nGit Installation\nMac Users: Install Xcode Command Line Tools, which includes git:\nxcode-select --install\nThis will open a dialog to install the command line developer tools. Once installed, you’ll have git available.\nWindows Users (in WSL): Git should be pre-installed in WSL Ubuntu. If not, install it with:\nsudo apt install git\nLinux Users: Download and install git from: https://git-scm.com/downloads Or use your distribution’s package manager (e.g., sudo apt install git for Ubuntu/Debian)\nVerify git is installed by running:\ngit --version\n\n\nGitHub CLI (Recommended)\nWe recommend installing the GitHub CLI for easier GitHub integration: - Installation instructions: https://cli.github.com/ - After installing, authenticate with: gh auth login\n\n\nBasic Command Line Familiarity\nYou should be comfortable navigating directories and running commands in a terminal.\nHere is a popular GitHub and command line tutorial video: https://www.youtube.com/watch?v=HVsySz-h9r4",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#instructions",
    "href": "monday/prework-1-env-git.html#instructions",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "Instructions",
    "text": "Instructions\n\nStep 1: Install Anaconda (or Alternative)\nNote: If you already have Miniconda, Miniforge, Mambaforge, or Mamba installed, you can use those instead - they all work with this assignment!\nOption 1: Download Anaconda (Mac/Windows/Linux)\nFor Mac/Windows, download and install from: https://www.anaconda.com/download\nFor Windows users in WSL, use the Linux installer:\n# Download the Linux installer in WSL\nwget https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh\n\n# Run the installer\nbash Anaconda3-2024.10-1-Linux-x86_64.sh\n\n# Follow the prompts and accept the license\n# When asked to initialize conda, type 'yes'\n\n# Restart your terminal or run:\nsource ~/.bashrc\nOption 2: Install via Homebrew (Mac users only) If you have Homebrew installed:\nbrew install --cask anaconda\nThen add conda to your PATH (follow the instructions shown after installation).\nVerify installation by running:\nconda --version\n\n\nStep 2: Clone Your GitHub Classroom Repository\nWhen you accepted the GitHub Classroom assignment, a repository was created specifically for you. You need to clone this repository to your computer.\nFinding your repository URL: 1. After accepting the assignment, you should see a link to your repository 2. Go to your repository on GitHub 3. Click the green “Code” button 4. Copy the repository URL (HTTPS or SSH)\nClone the repository:\n# Replace YOUR_REPO_URL with your actual repository URL\ngit clone YOUR_REPO_URL\n\n# Navigate into the cloned directory\n# (The directory name will match your repository name)\ncd HW1-setup-verification-YOUR_USERNAME\nNote: Your repository URL will be unique to you and will look something like: - HTTPS: https://github.com/YOUR_ORGANIZATION/HW1-setup-verification-YOUR_USERNAME.git - SSH: git@github.com:YOUR_ORGANIZATION/HW1-setup-verification-YOUR_USERNAME.git\n\n\nStep 3: Create the Conda Environment\nNow that you’re in the repository directory (which contains this README and the environment.yml file), run:\nconda env create -f environment.yml\nThis will create an environment named bootcamp2025_HW1 with all required packages: - Python 3.11 - PyRosetta (for protein structure manipulation - channels configured in environment.yml) - NumPy (numerical computing) - Pandas (data analysis) - Matplotlib & Seaborn (visualization) - Jupyter (interactive notebooks) - SciPy (scientific computing) - scikit-learn (machine learning) - Biopython (bioinformatics tools)\nNote: The environment.yml file includes the RosettaCommons conda channel, so you don’t need to configure it manually.\n\n\nStep 4: Activate the Environment\nconda activate bootcamp2025_HW1\nYou should see (bootcamp2025_HW1) in your terminal prompt.\n\n\nStep 5: Run the Verification Script\npython verify_setup.py\nThis script will: - Check that all required packages are installed - Display version information for each package - Generate a verification_result.json file\nIf all checks pass, you should see:\n🎉 SUCCESS! All packages are installed correctly!\nIf some checks fail, the script will provide guidance on what went wrong.\n\n\nStep 6: Update the Verification File\nOpen verification_result.json in a text editor and replace \"REPLACE_WITH_YOUR_NAME\" with your actual name.\n\n\nStep 7: Commit and Push Your Verification\nNow that you have your verification file, commit and push it to your GitHub Classroom repository:\n# Add the verification file\ngit add verification_result.json\n\n# Create your commit\ngit commit -m \"Add environment verification\"\n\n# Push to GitHub\ngit push\n\n\nStep 8: Verify Your Submission\nGo to your GitHub repository (the one created when you accepted the assignment) and verify that: - The verification_result.json file is present - It contains your name (not “REPLACE_WITH_YOUR_NAME”) - The verification_passed field is true",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#submission",
    "href": "monday/prework-1-env-git.html#submission",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "Submission",
    "text": "Submission\nYour work is automatically submitted when you push to your GitHub Classroom repository. The instructor will be able to see your verification file and check that all requirements are met.",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#troubleshooting",
    "href": "monday/prework-1-env-git.html#troubleshooting",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nPyRosetta Installation Issues\nIf PyRosetta fails to install: 1. Make sure you’re using the environment.yml file which includes the RosettaCommons channel 2. Try creating the environment again with verbose output: conda env create -f environment.yml --verbose 3. If that fails, try installing PyRosetta separately: conda install -c https://conda.rosettacommons.org pyrosetta 4. Check that you’re using a compatible Python version (3.11 is specified in environment.yml)\n\n\nImport Errors\nIf packages are installed but imports fail: 1. Make sure you’ve activated the environment: conda activate bootcamp2025_HW1 2. Try reinstalling the problematic package: conda install --force-reinstall &lt;package-name&gt;\n\n\nGit Issues\nIf you have trouble with git: - Make sure git is installed: git --version - Configure your git identity if needed: bash   git config --global user.name \"Your Name\"   git config --global user.email \"your.email@example.com\"\n\n\nAuthentication with GitHub\nIf you have trouble pushing to GitHub, you may need to set up authentication: - Use GitHub’s personal access token: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token - Or set up SSH keys: https://docs.github.com/en/authentication/connecting-to-github-with-ssh",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#questions-and-getting-help",
    "href": "monday/prework-1-env-git.html#questions-and-getting-help",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "Questions and Getting Help",
    "text": "Questions and Getting Help\nWe strongly encourage you to use our Slack workspace for questions and collaboration! One of the best ways to learn is by discussing problems with your peers and seeing how others approach challenges.\nJoin the Bootcamp Slack channel: The Slack workspace is currently reserved for in-person bootcamp participants. We apologize for the inconvenience.\nIn the Slack channel, you can: - Ask questions and get help from fellow students - Share tips and solutions you’ve discovered - Learn from others’ questions and answers - Collaborate on troubleshooting issues\nThe TAs and I will be active in the Slack channel to help guide discussions and answer questions. Don’t hesitate to jump in - chances are if you’re stuck on something, others are too!\nIf you have questions, please open an issue on the GitHub repository (GitHub account required) instead of emailing directly.",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/6-chai1.html",
    "href": "monday/6-chai1.html",
    "title": "6. Chai-1",
    "section": "",
    "text": "Chai-1 (paper, code) is a multi-modal foundation model for molecular structure prediction that achieves state-of-the-art performance across diverse benchmarks. Chai-1 enables unified prediction of proteins, small molecules, DNA, RNA, glycosylations, and more.",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#why-use-chai-1",
    "href": "monday/6-chai1.html#why-use-chai-1",
    "title": "6. Chai-1",
    "section": "Why Use Chai-1?",
    "text": "Why Use Chai-1?\n\nMulti-modal: Predict proteins, nucleic acids, small molecules, and modifications in one model\nState-of-the-art: Top performance on structure prediction benchmarks\nFlexible inputs: Handles complex multi-component assemblies\nExperimental restraints: Can incorporate known distance constraints\n\nRelated Tools: For protein-only predictions, see ESMFold (faster) or LocalColabFold (MSA-based). For binding affinity predictions, see Boltz-2.",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#resource-requirements",
    "href": "monday/6-chai1.html#resource-requirements",
    "title": "6. Chai-1",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n24 GB\n80 GB\nA100 80GB or H100 ideal\n\n\nCPU RAM\n32 GB\n64 GB\nFor preprocessing\n\n\nDisk Space\n10 GB\n20 GB\nModel weights\n\n\nPython\n3.10+\n3.11\nRequired\n\n\n\nGPU Compatibility: Requires bfloat16 support. Compatible GPUs include:\n\nA100, H100, L40S (recommended)\nA10, A30, RTX 4090 (works)\nOlder GPUs may not support bfloat16",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#preparation",
    "href": "monday/6-chai1.html#preparation",
    "title": "6. Chai-1",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nGPU with bfloat16 support\nPython 3.10+\n\nVerify bfloat16 support:\nimport torch\nprint(torch.cuda.is_bf16_supported())  # Should print True",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#installation",
    "href": "monday/6-chai1.html#installation",
    "title": "6. Chai-1",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nCreate a conda environment:\n\nmamba create -n chailab python=3.11\nmamba activate chailab\n\nInstall Chai-1:\n\npip install chai_lab==0.6.1\nExpected download: ~5-10 GB of model weights (downloaded on first run).\nAlternative: Latest development version:\npip install git+https://github.com/chaidiscovery/chai-lab.git",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#testing-the-installation",
    "href": "monday/6-chai1.html#testing-the-installation",
    "title": "6. Chai-1",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test FASTA file test.fasta:\n&gt;protein|name=example\nMKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\nRun prediction:\nchai-lab fold test.fasta output_folder/\nSuccess indicators:\n\nCommand completes without errors\noutput_folder/ contains:\n\npred.model_idx_0.cif - Predicted structure\nscores.model_idx_0.npz - Confidence scores\n\n\nExpected runtime: 2-5 minutes for first run (includes model download), ~30 seconds for subsequent runs.\nNote: By default, this generates 5 sample predictions using embeddings without MSAs.",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#hpc-job-script",
    "href": "monday/6-chai1.html#hpc-job-script",
    "title": "6. Chai-1",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=chai\n#SBATCH --partition=gpu\n#SBATCH --gpus=a100:1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate chailab\n\n# Set custom download directory (avoid filling home)\nexport CHAI_DOWNLOADS_DIR=/scratch/$USER/chai_models\n\n# Run prediction with MSAs\nchai-lab fold --use-msa-server --use-templates-server \\\n    my_complex.fasta \\\n    predictions/",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#usage-examples",
    "href": "monday/6-chai1.html#usage-examples",
    "title": "6. Chai-1",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic prediction (no MSAs, fast):\nchai-lab fold input.fasta output/\nWith MSAs (higher accuracy, uses ColabFold server):\nchai-lab fold --use-msa-server --use-templates-server input.fasta output/\nUsing internal MSA server (if your HPC has one):\nchai-lab fold --use-msa-server \\\n    --msa-server-url \"https://internal.colabserver.edu\" \\\n    input.fasta output/\nGenerate more samples:\nchai-lab fold --num-trunk-recycles 5 --num-diffn-timesteps 200 \\\n    input.fasta output/",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#input-format",
    "href": "monday/6-chai1.html#input-format",
    "title": "6. Chai-1",
    "section": "Input Format",
    "text": "Input Format\nChai-1 uses a modified FASTA format with entity type headers:\nProtein:\n&gt;protein|name=my_protein\nMKTVRQERLKSIVRILERSKEPVSG...\nLigand (SMILES):\n&gt;ligand|name=my_drug\nCC(C)CC1=CC=C(C=C1)C(C)C(=O)O\nDNA:\n&gt;dna|name=promoter\nATGCATGCATGCATGC\nRNA:\n&gt;rna|name=aptamer\nAUGCAUGCAUGCAUGC\nProtein complex (multiple chains):\n&gt;protein|name=chain_A\nMKTVRQERLK...\n&gt;protein|name=chain_B\nMVKLTAEGSE...",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#python-api",
    "href": "monday/6-chai1.html#python-api",
    "title": "6. Chai-1",
    "section": "Python API",
    "text": "Python API\nfrom chai_lab.chai1 import run_inference\n\nresults = run_inference(\n    fasta_file=\"input.fasta\",\n    output_dir=\"output/\",\n    num_trunk_recycles=3,\n    num_diffn_timesteps=200,\n    seed=42\n)\nSee examples/predict_structure.py in the repository for more details.",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#advanced-features",
    "href": "monday/6-chai1.html#advanced-features",
    "title": "6. Chai-1",
    "section": "Advanced Features",
    "text": "Advanced Features\nCustom Templates:\nchai-lab fold --custom-template template.cif input.fasta output/\nExperimental Restraints: Specify inter-chain contacts:\n# See: github.com/chaidiscovery/chai-lab/tree/main/examples/restraints\nCovalent Bonds: Specify covalent modifications:\n# See: github.com/chaidiscovery/chai-lab/tree/main/examples/covalent_bonds",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#understanding-the-output",
    "href": "monday/6-chai1.html#understanding-the-output",
    "title": "6. Chai-1",
    "section": "Understanding the Output",
    "text": "Understanding the Output\n\n\n\nFile\nDescription\n\n\n\n\npred.model_idx_N.cif\nPredicted structure (mmCIF format)\n\n\nscores.model_idx_N.npz\nConfidence scores\n\n\nmsa_*.a3m\nGenerated MSAs (if using MSA server)\n\n\n\nConfidence metrics (in scores file):\n\npLDDT: Per-residue confidence\npTM: Predicted TM-score\npAE: Predicted aligned error\ninterface scores: For multi-chain predictions",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#web-server",
    "href": "monday/6-chai1.html#web-server",
    "title": "6. Chai-1",
    "section": "Web Server",
    "text": "Web Server\nFor quick tests without installation: lab.chaidiscovery.com",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/6-chai1.html#troubleshooting",
    "href": "monday/6-chai1.html#troubleshooting",
    "title": "6. Chai-1",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n“bfloat16 not supported”:\n\nYour GPU doesn’t support bfloat16\nTry a newer GPU (A100, H100, RTX 4090)\nOlder GPUs (V100, etc.) may not work\n\nOut of memory:\n\nRequest GPU with more memory\nReduce --num-diffn-timesteps\nFor very large complexes, split into smaller units\n\nModel download location:\n# Set before running\nexport CHAI_DOWNLOADS_DIR=/scratch/$USER/chai_models\nMSA server rate limits:\n\nThe public ColabFold MMseqs2 server is a shared resource\nFor batch jobs, space out requests\nConsider setting up a local MSA server for high-throughput\n\nSlow first run:\n\nFirst run downloads ~5-10 GB of model weights\nSubsequent runs are much faster\nSet CHAI_DOWNLOADS_DIR to avoid re-downloading",
    "crumbs": [
      "Monday",
      "6. Chai-1"
    ]
  },
  {
    "objectID": "monday/11-esm3.html",
    "href": "monday/11-esm3.html",
    "title": "11. ESM3 (Optional)",
    "section": "",
    "text": "ESM3 (paper, code) is a frontier generative model for biology that jointly reasons across three fundamental biological properties of proteins: sequence, structure, and function. It represents a multimodal generative masked language model.\nNote: This tool is marked as OPTIONAL. Install if you’re interested in protein generation and multimodal design beyond structure prediction.",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#why-use-esm3",
    "href": "monday/11-esm3.html#why-use-esm3",
    "title": "11. ESM3 (Optional)",
    "section": "Why Use ESM3?",
    "text": "Why Use ESM3?\n\nMultimodal generation: Jointly reason about sequence, structure, and function\nProtein generation: Create novel proteins with desired properties\nSequence completion: Fill in masked or missing regions\nEmbeddings: Extract rich protein representations (ESM C)\n\nRelated Tools: For structure prediction only, see ESMFold. For sequence design given structure, see LigandMPNN.",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#resource-requirements",
    "href": "monday/11-esm3.html#resource-requirements",
    "title": "11. ESM3 (Optional)",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n24+ GB\nFor esm3-small (1.4B params)\n\n\nCPU RAM\n16 GB\n32 GB\nFor preprocessing\n\n\nDisk Space\n10 GB\n20 GB\nModel weights\n\n\nPython\n3.10+\n3.10\nRequired\n\n\n\nModel sizes:\n\nesm3-small-2024-08 (1.4B params): Runs locally\nesm3-medium-2024-08 (7B params): Via Forge API\nesm3-large-2024-03 (98B params): Via Forge API",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#preparation",
    "href": "monday/11-esm3.html#preparation",
    "title": "11. ESM3 (Optional)",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nHuggingFace account (for model access)",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#installation",
    "href": "monday/11-esm3.html#installation",
    "title": "11. ESM3 (Optional)",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nCreate a conda environment:\n\nmamba create -n esm3 python=3.10\nmamba activate esm3\n\nInstall the ESM library:\n\npip install esm",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#huggingface-authentication",
    "href": "monday/11-esm3.html#huggingface-authentication",
    "title": "11. ESM3 (Optional)",
    "section": "HuggingFace Authentication",
    "text": "HuggingFace Authentication\nESM3 weights are stored on HuggingFace Hub. You need to authenticate:\n\nCreate a HuggingFace account at huggingface.co\nGenerate an API token with “Read” permission at huggingface.co/settings/tokens\nAuthenticate in Python:\n\nfrom huggingface_hub import login\nlogin()  # Follow prompts to enter your token\nOr set environment variable:\nexport HF_TOKEN=\"your_token_here\"",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#testing-the-installation",
    "href": "monday/11-esm3.html#testing-the-installation",
    "title": "11. ESM3 (Optional)",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test script test_esm3.py:\nimport os\nfrom huggingface_hub import login\nfrom esm.models.esm3 import ESM3\nfrom esm.sdk.api import ESMProtein, GenerationConfig\n\n# Authenticate\n# Method 1: Environment variable (Recommended for HPC jobs)\nif \"HF_TOKEN\" in os.environ:\n    login(token=os.environ[\"HF_TOKEN\"])\n# Method 2: Interactive login (Run once on login node)\nelse:\n    login()\n\n# Load the model (downloads weights on first run)\nmodel = ESM3.from_pretrained(\"esm3-small-2024-08\").to(\"cuda\")  # or \"cpu\"\n\n# Generate a protein sequence completion\nprompt = \"MKTVRQ_______________QLAEELSVSRQVIVQDIAYLRSLG\"\nprotein = ESMProtein(sequence=prompt)\n\n# Generate sequence\nprotein = model.generate(\n    protein,\n    GenerationConfig(track=\"sequence\", num_steps=8, temperature=0.7)\n)\n\nprint(\"Generated sequence:\")\nprint(protein.sequence)\n\n# Generate structure\nprotein = model.generate(\n    protein,\n    GenerationConfig(track=\"structure\", num_steps=8)\n)\n\n# Save structure\nprotein.to_pdb(\"./generated.pdb\")\nprint(\"Structure saved to generated.pdb\")\nRun the test:\npython test_esm3.py\nSuccess indicators:\n\nModel loads without errors\nSequence completion fills in the masked region\nStructure is generated and saved as PDB\n\nExpected runtime: 2-5 minutes (first run downloads ~3GB weights).",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#hpc-job-script",
    "href": "monday/11-esm3.html#hpc-job-script",
    "title": "11. ESM3 (Optional)",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=esm3\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate esm3\n\n# Set HuggingFace token\nexport HF_TOKEN=\"your_token_here\"\n\npython generate_protein.py",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#usage-examples",
    "href": "monday/11-esm3.html#usage-examples",
    "title": "11. ESM3 (Optional)",
    "section": "Usage Examples",
    "text": "Usage Examples\nSequence generation (fill masked regions):\nfrom esm.models.esm3 import ESM3\nfrom esm.sdk.api import ESMProtein, GenerationConfig\n\nmodel = ESM3.from_pretrained(\"esm3-small-2024-08\").to(\"cuda\")\n\n# Use underscores for masked positions\nprotein = ESMProtein(sequence=\"MKTVRQ_______________QLAEELSVSRQVIVQDIAYLRSLG\")\n\n# Generate\nprotein = model.generate(\n    protein,\n    GenerationConfig(track=\"sequence\", num_steps=8, temperature=0.7)\n)\nprint(protein.sequence)\nStructure prediction:\nprotein = ESMProtein(sequence=\"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLG\")\n\nprotein = model.generate(\n    protein,\n    GenerationConfig(track=\"structure\", num_steps=8)\n)\n\nprotein.to_pdb(\"predicted.pdb\")\nUsing ESM C for embeddings only (faster, smaller):\nfrom esm.models.esmc import ESMC\nfrom esm.sdk.api import ESMProtein, LogitsConfig\n\nprotein = ESMProtein(sequence=\"MKTVRQERLK\")\nclient = ESMC.from_pretrained(\"esmc_300m\").to(\"cuda\")\n\n# Get embeddings\nprotein_tensor = client.encode(protein)\nlogits_output = client.logits(\n    protein_tensor,\n    LogitsConfig(sequence=True, return_embeddings=True)\n)\n\nprint(f\"Embedding shape: {logits_output.embeddings.shape}\")",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#available-models",
    "href": "monday/11-esm3.html#available-models",
    "title": "11. ESM3 (Optional)",
    "section": "Available Models",
    "text": "Available Models\n\n\n\nModel\nParameters\nAvailability\n\n\n\n\nesm3-small-2024-08\n1.4B\nLocal (free)\n\n\nesmc_300m\n300M\nLocal (fast embeddings)\n\n\nesmc_600m\n600M\nLocal\n\n\nesm3-medium-2024-08\n7B\nForge API\n\n\nesm3-large-2024-03\n98B\nForge API",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#generation-tracks",
    "href": "monday/11-esm3.html#generation-tracks",
    "title": "11. ESM3 (Optional)",
    "section": "Generation Tracks",
    "text": "Generation Tracks\nESM3 can generate different “tracks”:\n\n\n\nTrack\nDescription\n\n\n\n\nsequence\nGenerate amino acid sequence\n\n\nstructure\nGenerate 3D coordinates\n\n\nfunction\nGenerate functional annotations",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#key-parameters",
    "href": "monday/11-esm3.html#key-parameters",
    "title": "11. ESM3 (Optional)",
    "section": "Key Parameters",
    "text": "Key Parameters\n\n\n\nParameter\nDescription\n\n\n\n\ntrack\nWhat to generate: sequence, structure, function\n\n\nnum_steps\nNumber of generation steps (more = better quality)\n\n\ntemperature\nSampling diversity (higher = more diverse)",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#use-cases",
    "href": "monday/11-esm3.html#use-cases",
    "title": "11. ESM3 (Optional)",
    "section": "Use Cases",
    "text": "Use Cases\n\nProtein generation: Create novel proteins\nSequence completion: Fill in missing regions\nStructure prediction: Generate 3D structures\nFunction prediction: Predict functional properties\nEmbeddings: Extract protein representations for ML",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/11-esm3.html#troubleshooting",
    "href": "monday/11-esm3.html#troubleshooting",
    "title": "11. ESM3 (Optional)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nHuggingFace authentication errors:\n\nVerify token has “Read” permission\nRun login() in Python and follow prompts\nOr set HF_TOKEN environment variable\n\nModel download issues:\n\nCheck network connectivity\nWeights are large (~3GB for small model)\nSet HF_HOME to location with space:\nexport HF_HOME=/scratch/$USER/huggingface\n\nGPU memory issues:\n\nUse CPU if GPU is insufficient: .to(\"cpu\")\nReduce batch size if processing multiple proteins\nesmc_300m is smaller and faster for embeddings\n\nSlow generation:\n\nGPU strongly recommended\nReduce num_steps for faster (lower quality) results\nUse ESM C for embeddings (no structure generation)",
    "crumbs": [
      "Monday",
      "11. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html",
    "href": "monday/12-rfdiffusion-aa.html",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "",
    "text": "RFdiffusion All Atom (code) is the predecessor to RFdiffusion2, enabling all-atom protein design with small molecule binding. It can design protein binders to ligands with all-atom precision.\nNote: This tool is marked as OPTIONAL because RFdiffusion2 is the newer, more capable version. Install this only if you need the earlier methodology or specific features.",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#why-use-rfdiffusion-all-atom",
    "href": "monday/12-rfdiffusion-aa.html#why-use-rfdiffusion-all-atom",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Why Use RFdiffusion All Atom?",
    "text": "Why Use RFdiffusion All Atom?\n\nSmall molecule binding: Design proteins that bind specific ligands\nMotif incorporation: Include functional motifs in designs\nHistorical reference: Understand the evolution of diffusion-based design\nSpecific workflows: Some published protocols may reference this version\n\nRelated Tools: For the newer version, see RFdiffusion2. For sequence design, see LigandMPNN.",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#resource-requirements",
    "href": "monday/12-rfdiffusion-aa.html#resource-requirements",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n8 GB\n16 GB\nScales with design size\n\n\nCPU RAM\n8 GB\n16 GB\nContainer-based\n\n\nDisk Space\n5 GB\n10 GB\nContainer + weights\n\n\nContainer\nApptainer/Singularity\nRequired\nNot Docker",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#preparation",
    "href": "monday/12-rfdiffusion-aa.html#preparation",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nApptainer/Singularity available\nGPU access recommended\n\nImportant: Like RFdiffusion2, this uses Apptainer/Singularity containers. Most academic HPCs do NOT support Docker.\nVerify container runtime:\nmodule load apptainer    # or: module load singularity\napptainer --version",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#installation",
    "href": "monday/12-rfdiffusion-aa.html#installation",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the repository:\n\ngit clone https://github.com/baker-laboratory/rf_diffusion_all_atom.git\ncd rf_diffusion_all_atom\n\nDownload the Singularity container:\n\nwget http://files.ipd.uw.edu/pub/RF-All-Atom/containers/rf_se3_diffusion.sif\nExpected download: ~2-3 GB.\n\nDownload the model weights:\n\nwget http://files.ipd.uw.edu/pub/RF-All-Atom/weights/RFDiffusionAA_paper_weights.pt\nExpected download: ~500 MB.\n\nInitialize git submodules:\n\ngit submodule init\ngit submodule update",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#testing-the-installation",
    "href": "monday/12-rfdiffusion-aa.html#testing-the-installation",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a ligand binder design example:\napptainer run --nv rf_se3_diffusion.sif -u run_inference.py \\\n    inference.deterministic=True \\\n    diffuser.T=100 \\\n    inference.output_prefix=output/ligand_test/sample \\\n    inference.input_pdb=input/7v11.pdb \\\n    contigmap.contigs=\"['150-150']\" \\\n    inference.ligand=OQO \\\n    inference.num_designs=1 \\\n    inference.design_startnum=0\nNote: Omit --nv flag if running without GPU.\nSuccess indicators:\n\nCommand completes without errors\nOutput files created:\n\noutput/ligand_test/sample_0.pdb - The designed structure\noutput/ligand_test/sample_0_Xt-1_traj.pdb - Denoising trajectory\noutput/ligand_test/sample_0_X0-1_traj.pdb - Predicted ground truth at each step\n\n\nExpected runtime: 5-10 minutes on GPU, 30+ minutes on CPU.",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#hpc-job-script",
    "href": "monday/12-rfdiffusion-aa.html#hpc-job-script",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=rfdaa\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load apptainer\nmodule load cuda/12.1\n\ncd /path/to/rf_diffusion_all_atom\n\napptainer run --nv rf_se3_diffusion.sif -u run_inference.py \\\n    inference.deterministic=True \\\n    diffuser.T=100 \\\n    inference.output_prefix=output/my_design/sample \\\n    inference.input_pdb=input/my_protein.pdb \\\n    contigmap.contigs=\"['150-150']\" \\\n    inference.ligand=HEM \\\n    inference.num_designs=10",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#usage-examples",
    "href": "monday/12-rfdiffusion-aa.html#usage-examples",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic ligand binder design:\napptainer run --nv rf_se3_diffusion.sif -u run_inference.py \\\n    inference.input_pdb=input/complex.pdb \\\n    inference.output_prefix=output/design \\\n    inference.ligand=LIG \\\n    contigmap.contigs=\"['100-100']\" \\\n    inference.num_designs=10\nMultiple designs with different lengths:\napptainer run --nv rf_se3_diffusion.sif -u run_inference.py \\\n    inference.input_pdb=input/complex.pdb \\\n    inference.output_prefix=output/design \\\n    inference.ligand=LIG \\\n    contigmap.contigs=\"['80-120']\" \\\n    inference.num_designs=20",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#key-parameters",
    "href": "monday/12-rfdiffusion-aa.html#key-parameters",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Key Parameters",
    "text": "Key Parameters\n\n\n\nParameter\nDescription\n\n\n\n\ninference.input_pdb\nInput PDB with ligand\n\n\ninference.output_prefix\nOutput path prefix\n\n\ninference.ligand\nLigand residue name\n\n\ncontigmap.contigs\nProtein length range (e.g., ['100-100'])\n\n\ninference.num_designs\nNumber of designs\n\n\ndiffuser.T\nDiffusion timesteps (100 typical)\n\n\ninference.deterministic\nReproducible results",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#docker-to-apptainer-translation",
    "href": "monday/12-rfdiffusion-aa.html#docker-to-apptainer-translation",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Docker to Apptainer Translation",
    "text": "Docker to Apptainer Translation\nThe official docs may show Docker commands. Translate as follows:\n\n\n\n\n\n\n\nDocker\nApptainer\n\n\n\n\ndocker run --gpus all\napptainer run --nv\n\n\ndocker run -v $(pwd):/workspace\napptainer run --bind $(pwd):/workspace\n\n\n-it (interactive)\nUse apptainer shell --nv",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#understanding-the-output",
    "href": "monday/12-rfdiffusion-aa.html#understanding-the-output",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Understanding the Output",
    "text": "Understanding the Output\n\n\n\n\n\n\n\nOutput File\nDescription\n\n\n\n\nsample_N.pdb\nFinal designed structure\n\n\nsample_N_Xt-1_traj.pdb\nDenoising trajectory (animation of design)\n\n\nsample_N_X0-1_traj.pdb\nModel predictions at each step\n\n\n\nThe trajectory files can be loaded into PyMOL to visualize the diffusion process.",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#comparison-with-rfdiffusion2",
    "href": "monday/12-rfdiffusion-aa.html#comparison-with-rfdiffusion2",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Comparison with RFdiffusion2",
    "text": "Comparison with RFdiffusion2\n\n\n\nFeature\nRFdiffusion AA\nRFdiffusion2\n\n\n\n\nAtomic precision\nYes\nYes (improved)\n\n\nLigand binding\nYes\nYes\n\n\nActive site scaffolding\nLimited\nAdvanced\n\n\nModel architecture\nEarlier\nUpdated\n\n\nRecommended\nLegacy workflows\nNew projects\n\n\n\nRecommendation: Use RFdiffusion2 for new projects unless you have specific reasons to use this version.",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/12-rfdiffusion-aa.html#troubleshooting",
    "href": "monday/12-rfdiffusion-aa.html#troubleshooting",
    "title": "12. RFdiffusion All Atom (Optional)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nContainer not found:\n\nEnsure .sif file is in current directory\nOr provide full path to container\n\nGPU errors:\n\nEnsure --nv flag is included for GPU\nLoad CUDA module: module load cuda/12.1\nVerify GPU availability: nvidia-smi\n\nPermission denied on container:\nchmod +x rf_se3_diffusion.sif\nInput PDB errors:\n\nVerify ligand is present in input PDB\nCheck ligand residue name matches inference.ligand\nEnsure PDB has proper formatting\n\nSubmodule errors:\ngit submodule init\ngit submodule update",
    "crumbs": [
      "Monday",
      "12. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/index.html",
    "href": "monday/index.html",
    "title": "Monday: Tool Installation",
    "section": "",
    "text": "Monday is dedicated to installing the essential ML-based protein design and structure prediction tools on your HPC cluster. Each module guides you through installing a specific tool, from structure predictors like LocalColabFold and ESMFold to design tools like LigandMPNN and BindCraft.\nGoal: By the end of Monday, you should have all major tools installed and tested on your HPC cluster.",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#overview",
    "href": "monday/index.html#overview",
    "title": "Monday: Tool Installation",
    "section": "",
    "text": "Monday is dedicated to installing the essential ML-based protein design and structure prediction tools on your HPC cluster. Each module guides you through installing a specific tool, from structure predictors like LocalColabFold and ESMFold to design tools like LigandMPNN and BindCraft.\nGoal: By the end of Monday, you should have all major tools installed and tested on your HPC cluster.",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#pre-work",
    "href": "monday/index.html#pre-work",
    "title": "Monday: Tool Installation",
    "section": "Pre-work",
    "text": "Pre-work\nBefore starting the main modules, complete these pre-work assignments to ensure your local environment is ready.\n\n\n\n#\nModule\nDescription\nStatus\n\n\n\n\nP1\nEnvironment & GitHub\nSetup conda, git, and GitHub\nRequired\n\n\nP2\nPyMOL & VS Code\nInstall visualization and coding tools\nRequired\n\n\nP3\nPython Refresher\nRefresh Python skills for bioinformatics\nRecommended",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#getting-started",
    "href": "monday/index.html#getting-started",
    "title": "Monday: Tool Installation",
    "section": "Getting Started",
    "text": "Getting Started\nBefore installing individual tools, complete the HPC Setup module to ensure your environment is properly configured:\n\n\n\n#\nModule\nDescription\nStatus\n\n\n\n\n0\nCommon HPC Setup\nCUDA, Conda, containers, and environment setup\nStart Here",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#tool-installation-modules",
    "href": "monday/index.html#tool-installation-modules",
    "title": "Monday: Tool Installation",
    "section": "Tool Installation Modules",
    "text": "Tool Installation Modules\n\n\n\n#\nTool\nDescription\nStatus\n\n\n\n\n1\nLocalColabFold\nFast AlphaFold2 structure prediction\nRequired\n\n\n2\nLigandMPNN\nContext-aware protein sequence design\nRequired\n\n\n3\nRFdiffusion2\nAtom-level active site scaffolding\nRequired\n\n\n4\nESMFold\nSingle-sequence structure prediction\nRequired\n\n\n5\nOpenFold\nOpen-source AlphaFold2 reproduction\nOptional\n\n\n6\nChai-1\nMulti-modal biomolecular structure prediction\nRequired\n\n\n7\nBoltz-2\nStructure + binding affinity prediction\nRequired\n\n\n8\nDiffDock-PP\nProtein-protein docking\nRequired\n\n\n9\nPLACER\nProtein-ligand conformational ensemble prediction\nRequired\n\n\n10\nBindCraft\nEnd-to-end binder design pipeline\nRequired\n\n\n11\nESM3\nMultimodal protein generation\nOptional\n\n\n12\nRFdiffusion All Atom\nAll-atom protein design (predecessor to RFd2)\nOptional",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#tool-categories",
    "href": "monday/index.html#tool-categories",
    "title": "Monday: Tool Installation",
    "section": "Tool Categories",
    "text": "Tool Categories\n\nStructure Prediction\n\n\n\n\n\n\n\n\n\n\nTool\nInput\nOutput\nSpeed\nBest For\n\n\n\n\nLocalColabFold\nSequence + MSA\nStructure\nMedium\nHigh-accuracy single proteins\n\n\nESMFold\nSequence only\nStructure\nFast\nQuick predictions, no MSA needed\n\n\nOpenFold\nSequence + MSA\nStructure\nMedium\nResearch, custom training\n\n\nChai-1\nMulti-modal\nComplex structures\nMedium\nProteins + ligands + nucleic acids\n\n\nBoltz-2\nMulti-modal\nStructure + affinity\nMedium\nDrug discovery\n\n\n\n\n\nProtein Design\n\n\n\n\n\n\n\n\n\nTool\nInput\nOutput\nBest For\n\n\n\n\nLigandMPNN\nBackbone\nSequence\nSequence design with ligand context\n\n\nRFdiffusion2\nConstraints\nBackbone\nActive site scaffolding\n\n\nBindCraft\nTarget structure\nBinder designs\nEnd-to-end binder design\n\n\n\n\n\nDocking\n\n\n\n\n\n\n\n\n\nTool\nInput\nOutput\nBest For\n\n\n\n\nDiffDock-PP\nTwo proteins\nDocked complex\nProtein-protein docking\n\n\nPLACER\nProtein + ligand\nEnsemble poses\nProtein-ligand docking",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#tips-for-success",
    "href": "monday/index.html#tips-for-success",
    "title": "Monday: Tool Installation",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nStart with HPC Setup - Complete Module 0 before installing any tools\nUse separate conda environments - Each tool should have its own environment to avoid dependency conflicts\nCheck GPU availability - Most tools require GPU access; make sure you can request GPU nodes on your cluster\nNote your paths - Keep track of where you install each tool; you’ll need these paths later\nTest each installation - Don’t move on until you’ve verified each tool works\nCheck shared resources - Your HPC may already have databases (AlphaFold, ColabFold) installed",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#resource-overview",
    "href": "monday/index.html#resource-overview",
    "title": "Monday: Tool Installation",
    "section": "Resource Overview",
    "text": "Resource Overview\nApproximate requirements across all tools:\n\n\n\nResource\nTotal Needed\n\n\n\n\nDisk Space\n50-100 GB (tools only), 2+ TB (with databases)\n\n\nGPU RAM\n16-80 GB depending on task\n\n\nCPU RAM\n32-64 GB",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#getting-help",
    "href": "monday/index.html#getting-help",
    "title": "Monday: Tool Installation",
    "section": "Getting Help",
    "text": "Getting Help\nIf you encounter issues:\n\nCheck the tool’s official documentation (linked in each module)\nSearch for existing GitHub issues on the tool’s repository\nReport an issue on the bootcamp site (GitHub account required)\n\n\n\n\n← Back to Home\n\n\nTuesday →",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html",
    "href": "monday/prework-2-pymol-vscode.html",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "",
    "text": "This assignment ensures you have PyMOL and VS Code installed and configured for the bootcamp. You will:\n\nInstall PyMOL (a molecular visualization tool)\nInstall VS Code (a powerful code editor/IDE)\nVerify your installations by completing hands-on tasks\nCommit your work to GitHub",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#overview",
    "href": "monday/prework-2-pymol-vscode.html#overview",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "",
    "text": "This assignment ensures you have PyMOL and VS Code installed and configured for the bootcamp. You will:\n\nInstall PyMOL (a molecular visualization tool)\nInstall VS Code (a powerful code editor/IDE)\nVerify your installations by completing hands-on tasks\nCommit your work to GitHub",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#about-autograding",
    "href": "monday/prework-2-pymol-vscode.html#about-autograding",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "About Autograding",
    "text": "About Autograding\nNote: This assignment has automated tests that run when you push your code. You’ll see a score out of 100 points. These points are just for feedback - they help you know if you’ve completed everything correctly. This is a bootcamp focused on learning, not grades!",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#prerequisites",
    "href": "monday/prework-2-pymol-vscode.html#prerequisites",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nHW1 completed",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#part-1-pymol-installation",
    "href": "monday/prework-2-pymol-vscode.html#part-1-pymol-installation",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Part 1: PyMOL Installation",
    "text": "Part 1: PyMOL Installation\nPyMOL is a molecular visualization tool that is super useful for visualizing, manipulating, and analyzing molecular structures, especially proteins. We will use PyMOL to explore protein structures during bootcamp. We recognize that it’s likely you are already quite familiar with PyMOL but if you have been using an alternative visualization software, this is a great opportunity to explore PyMOL. Please be ready to use this in the Bootcamp.\n\nInstallation Options\nOption 1: Official PyMOL (Requires Educational License) 1. Visit the PyMOL website: https://pymol.org/ 2. Obtain a free educational license: https://pymol.org/edu/ 3. Download and install PyMOL\nOption 2: Open-Source PyMOL via Homebrew (Mac users) If you have Homebrew installed:\nbrew install pymol\nVerify installation by launching PyMOL (GUI) from your applications or by running:\npymol\n\n\nOptional PyMOL Learning Resources\nThough a comprehensive understanding of the various features in PyMOL is not within the scope of this course, it may be a useful tool for your future work and presentations.\n\nPyMOL Tutorial Video - Complete this tutorial using a protein of interest\nPyMOL Guide - Comprehensive written guide\nPyMOL Wiki - Explore more PyMOL features and tools\n\n\n\nCustomizing PyMOL (Optional)\nIf you would like to alter your visual defaults, you may do so by editing your .pymolrc, which is a script that gets read every time PyMOL opens. To do so, go to File &gt; Edit pymolrc. Here are the defaults that Rosetta member Joey Lubin uses, which can be pasted into the window that opens.",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#part-2-vs-code-installation",
    "href": "monday/prework-2-pymol-vscode.html#part-2-vs-code-installation",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Part 2: VS Code Installation",
    "text": "Part 2: VS Code Installation\nVS Code is a powerful IDE that is largely customizable and can incorporate many different extensions and features. If you do not have VS Code or an alternative IDE, please install it before the bootcamp.\n\nInstallation Options\nOption 1: Download from Website Visit the VS Code website: https://code.visualstudio.com/\nOption 2: Install via Homebrew (Mac users) If you have Homebrew installed:\nbrew install --cask visual-studio-code\nVerify installation by launching VS Code or running:\ncode --version\n\n\nRecommended VS Code Extensions\nOnce VS Code is installed, we recommend installing the following extensions:\n\nPython (Microsoft) - Python language support\nPylance (Microsoft) - Fast Python language server\nJupyter (Microsoft) - Jupyter notebook support\n\nTo install extensions: 1. Open VS Code 2. Click the Extensions icon in the sidebar (or press Cmd+Shift+X / Ctrl+Shift+X) 3. Search for each extension and click “Install”",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#part-3-verification-tasks",
    "href": "monday/prework-2-pymol-vscode.html#part-3-verification-tasks",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Part 3: Verification Tasks",
    "text": "Part 3: Verification Tasks\nNow let’s verify that everything is working correctly!\n\nTask 1: PyMOL Verification\n\nFetch a protein structure:\n\nOpen PyMOL\nIn the PyMOL command line, type: fetch 1ubq\nThis fetches ubiquitin (PDB ID: 1UBQ), a small regulatory protein\n\nStyle your visualization:\n\nTry different representations (cartoon, sticks, surface, etc.)\nColor the structure in a way you find visually appealing\nFeel free to experiment!\n\nSave your work:\n\nSave the PyMOL session: File &gt; Save Session As...\nName it 1ubq_session.pse and save it in this assignment directory\nExport an image: File &gt; Export Image As &gt; PNG...\nName it 1ubq_visualization.png and save it in this assignment directory\n\n\n\n\nTask 2: VS Code Verification\n\nOpen VS Code\nCreate a Python script:\n\nOpen this assignment folder in VS Code: File &gt; Open Folder...\nCreate a new file called verify_setup.py\nCopy and paste the verification script (provided in this repository)\nRun the script to verify your setup\n\nRun the verification script:\n\nOpen the integrated terminal in VS Code: Terminal &gt; New Terminal\nActivate your conda environment from HW1: conda activate bootcamp2025_HW1\nRun: python verify_setup.py\nThe script will check your installations and create a verification_result.txt file",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#part-4-submission",
    "href": "monday/prework-2-pymol-vscode.html#part-4-submission",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Part 4: Submission",
    "text": "Part 4: Submission\nOnce you’ve completed both tasks, commit your work to your GitHub Classroom repository:\n# Add all the files you created\ngit add 1ubq_session.pse 1ubq_visualization.png verification_result.txt\n\n# Commit your work\ngit commit -m \"Complete PyMOL and VS Code setup verification\"\n\n# Push to GitHub\ngit push\n\nRequired Files for Submission\nMake sure your repository contains: - [ ] 1ubq_session.pse - Your PyMOL session file - [ ] 1ubq_visualization.png - Your exported PyMOL visualization - [ ] verification_result.txt - Output from the VS Code verification script",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#troubleshooting",
    "href": "monday/prework-2-pymol-vscode.html#troubleshooting",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nPyMOL Issues\nPyMOL won’t launch: - Make sure you’ve completed the installation fully - Mac users: Try the Homebrew installation method - Make sure you’re launching the GUI application, not just the Python package\nCan’t fetch PDB structures: - Check your internet connection - Try: fetch 1ubq, type=pdb1 if the default doesn’t work\n\n\nVS Code Issues\ncode command not found: - On Mac: Open VS Code, press Cmd+Shift+P, type “shell command”, and select “Install ‘code’ command in PATH” - On Windows: VS Code should be in your PATH automatically after installation\nPython extension not working: - Make sure Python is installed (you should have this from HW1) - Restart VS Code after installing extensions",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#questions-and-getting-help",
    "href": "monday/prework-2-pymol-vscode.html#questions-and-getting-help",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Questions and Getting Help",
    "text": "Questions and Getting Help\nWe strongly encourage you to use our Slack workspace for questions and collaboration! One of the best ways to learn is by discussing problems with your peers and seeing how others approach challenges.\nJoin the Bootcamp Slack channel: The Slack workspace is currently reserved for in-person bootcamp participants. We apologize for the inconvenience.\nIn the Slack channel, you can: - Ask questions and get help from fellow students - Share tips and solutions you’ve discovered - Learn from others’ questions and answers - Collaborate on troubleshooting issues\nThe TAs and I will be active in the Slack channel to help guide discussions and answer questions. Don’t hesitate to jump in - chances are if you’re stuck on something, others are too!\nIf you have questions, please open an issue on the GitHub repository (GitHub account required) instead of emailing directly.",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "tuesday/2-esmfold.html",
    "href": "tuesday/2-esmfold.html",
    "title": "2. ESMFold vs. AlphaFold2",
    "section": "",
    "text": "In this activity, you will use ESMFold to predict protein structures and systematically compare results with the AlphaFold2 predictions you generated in Activity 1. ESMFold uses a fundamentally different approach—a protein language model trained on sequences alone, without requiring Multiple Sequence Alignments (MSAs) or templates. By comparing ESMFold to AF2, you will understand the trade-offs between different prediction methods and learn when to use each approach.\nBuilding on Activity 1: You explored how AF2 parameters (model weights, MSA depth, templates, recycles) affect prediction quality. Now you’ll investigate how a completely different architecture performs on the same protein and develop intuition for choosing the right tool for your research needs."
  },
  {
    "objectID": "tuesday/2-esmfold.html#overview",
    "href": "tuesday/2-esmfold.html#overview",
    "title": "2. ESMFold vs. AlphaFold2",
    "section": "",
    "text": "In this activity, you will use ESMFold to predict protein structures and systematically compare results with the AlphaFold2 predictions you generated in Activity 1. ESMFold uses a fundamentally different approach—a protein language model trained on sequences alone, without requiring Multiple Sequence Alignments (MSAs) or templates. By comparing ESMFold to AF2, you will understand the trade-offs between different prediction methods and learn when to use each approach.\nBuilding on Activity 1: You explored how AF2 parameters (model weights, MSA depth, templates, recycles) affect prediction quality. Now you’ll investigate how a completely different architecture performs on the same protein and develop intuition for choosing the right tool for your research needs."
  },
  {
    "objectID": "tuesday/2-esmfold.html#learning-objectives",
    "href": "tuesday/2-esmfold.html#learning-objectives",
    "title": "2. ESMFold vs. AlphaFold2",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand ESMFold’s language model approach vs AF2’s MSA-based approach\nWrite Python code to use ESMFold following official documentation\nCompare prediction quality, speed, and confidence across different models\nMake informed decisions about model selection for different use cases\nCritically evaluate strengths and limitations of different prediction methods"
  },
  {
    "objectID": "tuesday/2-esmfold.html#setup",
    "href": "tuesday/2-esmfold.html#setup",
    "title": "2. ESMFold vs. AlphaFold2",
    "section": "Setup",
    "text": "Setup\n\nPrerequisites\n1. Create an ESMFold activity directory:\n\n\nActivity Directory Structure\nmkdir esmfold_activity\ncd esmfold_activity"
  },
  {
    "objectID": "tuesday/2-esmfold.html#part-1-running-esmfold-structure-prediction",
    "href": "tuesday/2-esmfold.html#part-1-running-esmfold-structure-prediction",
    "title": "2. ESMFold vs. AlphaFold2",
    "section": "Part 1: Running ESMFold Structure Prediction",
    "text": "Part 1: Running ESMFold Structure Prediction\nGoal: Generate an ESMFold prediction and understand the workflow.\n\n1.1 Quick Introduction to ESMFold\nVisit the ESM GitHub repository: - https://github.com/facebookresearch/esm\nKey points about ESMFold: 1. Uses esm.pretrained.esmfold_v1() to load the model 2. Runs inference with model.infer_pdb(sequence) - returns a PDB-formatted string 3. Stores confidence scores (pLDDT) in the B-factor column of the PDB file 4. No MSA required - just sequence in, structure out!\n\n\n1.2 Write Your ESMFold Prediction Script\nCreate a file predict_esmfold.py:\nYou can follow the official example from the ESM GitHub, or use this template:\nimport torch\nimport esm\nimport time\n\n# Load model\nprint(\"Loading ESMFold model...\")\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval()\n\n# Move to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\nprint(f\"Using device: {device}\")\n\n# Your sequence (GFP - same as Activity 1)\nsequence = \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n\n# Run prediction with timing\nprint(f\"Predicting structure for sequence of length {len(sequence)}...\")\nstart_time = time.time()\n\nwith torch.no_grad():\n    output = model.infer_pdb(sequence)\n\nelapsed = time.time() - start_time\nprint(f\"Prediction completed in {elapsed:.1f} seconds ({elapsed/60:.2f} minutes)\")\n\n# Save to file\noutput_file = \"esmfold_gfp.pdb\"\nwith open(output_file, \"w\") as f:\n    f.write(output)\n\nprint(f\"Structure saved to {output_file}\")\nQuestions to consider: 1. What does torch.no_grad() do? (Hint: disables gradient calculation - saves memory during inference) 2. Why don’t we need to generate MSAs like in AlphaFold2? 3. How long did your prediction take?\n\n\n1.3 Run Your First Prediction\npython predict_esmfold.py\nExpected output: - A PDB file named esmfold_gfp.pdb should be created - Note the prediction time - this is much faster than ColabFold from yesterday!"
  },
  {
    "objectID": "tuesday/2-esmfold.html#part-2-extracting-and-comparing-confidence-scores",
    "href": "tuesday/2-esmfold.html#part-2-extracting-and-comparing-confidence-scores",
    "title": "2. ESMFold vs. AlphaFold2",
    "section": "Part 2: Extracting and Comparing Confidence Scores",
    "text": "Part 2: Extracting and Comparing Confidence Scores\nGoal: Extract pLDDT scores from both ESMFold and AlphaFold2 predictions for comparison.\n\n2.1 Understanding pLDDT Storage in PDB Files\nBoth ESMFold and AlphaFold2 store pLDDT confidence scores in the B-factor column of PDB files.\nPDB format (ATOM lines): - Columns 1-6: “ATOM” - Columns 7-11: Atom serial number - Columns 12-16: Atom name - Columns 17-20: Residue name - Columns 23-26: Residue number - Columns 31-54: X, Y, Z coordinates - Columns 61-66: B-factor (where pLDDT is stored!)\n\n\n2.2 Extract pLDDT Scores\nWe’ve provided a script extract_plddt.py that works for both ESMFold and AlphaFold2 PDB files.\nDownload extract_plddt.py\nRun it on your ESMFold prediction:\npython extract_plddt.py esmfold_gfp.pdb\nRun it on your AlphaFold2 prediction from yesterday:\npython extract_plddt.py path/to/your/af2_prediction.pdb\nQuestions: 1. What is the average pLDDT for ESMFold vs AlphaFold2? 2. Which model has more high-confidence residues (&gt;90)? 3. Do both models identify the same low-confidence regions?\n\n\n2.3 Visualize Confidence Scores in PyMOL\nLoad your ESMFold structure and color by confidence:\nload esmfold_gfp.pdb\n# Color by B-factor (pLDDT): blue = high confidence, red = low confidence\nspectrum b, blue_white_red, minimum=50, maximum=100\nIdentify and display low confidence regions:\n# Select residues with low confidence\nselect low_conf, b &lt; 70\nshow sticks, low_conf\ncolor yellow, low_conf\nCompare to AlphaFold2 visually: - Do the same for your AF2 prediction - Are low-confidence regions in similar locations?"
  },
  {
    "objectID": "tuesday/2-esmfold.html#part-3-direct-comparison---esmfold-vs-alphafold2",
    "href": "tuesday/2-esmfold.html#part-3-direct-comparison---esmfold-vs-alphafold2",
    "title": "2. ESMFold vs. AlphaFold2",
    "section": "Part 3: Direct Comparison - ESMFold vs AlphaFold2",
    "text": "Part 3: Direct Comparison - ESMFold vs AlphaFold2\nGoal: Compare the two predictions structurally and assess accuracy.\n\n3.1 Prepare for Comparison\nYou need: 1. Your ESMFold prediction: esmfold_gfp.pdb (from Part 1) 2. Your AlphaFold2 prediction from yesterday (ColabFold output) 3. The experimental crystal structure of GFP from PDB\nDownload the crystal structure:\nwget https://files.rcsb.org/download/1GFL.pdb -O 1gfl_native.pdb\n\n\n3.2 Visual Comparison in PyMOL\nLoad and align all three structures:\n# Load all structures\nload 1gfl_native.pdb, native\nload path/to/af2_prediction.pdb, af2\nload esmfold_gfp.pdb, esmfold\n\n# Align both predictions to the native structure\nalign af2, native\nalign esmfold, native\n\n# Display with different colors\nhide everything\nshow cartoon\ncolor green, native\ncolor cyan, af2\ncolor magenta, esmfold\nAnalysis questions: 1. Do ESMFold and AF2 agree on the overall fold? 2. Where are the major differences (if any)? 3. Which one looks closer to the native structure? 4. Are differences in well-structured regions (beta-barrel) or loops?\n\n\n3.3 Calculate RMSD\nIn PyMOL, calculate RMSD for each comparison:\nUse the same approach you used in Activity 1. If you’re feeling confident, try writing a PyMOL script to automate this!\n# In PyMOL:\nrms_print af2, native\nrms_print esmfold, native\nrms_print af2, esmfold\nRecord your results:\n\n\n\nComparison\nRMSD (Å)\nInterpretation\n\n\n\n\nAF2 → Native\n\nBetter accuracy = lower RMSD\n\n\nESMFold → Native\n\n\n\n\nAF2 ↔︎ ESMFold\n\nAgreement &lt; 2Å is good\n\n\n\nDiscussion points: - Which model is more accurate for GFP? - Do the two models agree with each other (RMSD &lt; 2Å)? - Given that both are trained on different data, what does their agreement (or disagreement) tell you?"
  },
  {
    "objectID": "tuesday/2-esmfold.html#part-4-exploring-esmfold-features-optional---if-youre-ahead",
    "href": "tuesday/2-esmfold.html#part-4-exploring-esmfold-features-optional---if-youre-ahead",
    "title": "2. ESMFold vs. AlphaFold2",
    "section": "Part 4: Exploring ESMFold Features ⭐ OPTIONAL - If You’re Ahead",
    "text": "Part 4: Exploring ESMFold Features ⭐ OPTIONAL - If You’re Ahead\n⏰ Due to time constraints, this section is optional. Only do this if you finish the core activity early, or come back to it later if you want to explore ESMFold more deeply!\nGoal: Understand ESMFold-specific capabilities and parameters.\n\n4.1 Memory Optimization with Chunking\nESMFold supports chunking to reduce memory usage.\nLooking at the documentation, you’ll find model.set_chunk_size(size): - Reduces memory from O(L²) to O(L) - Useful for very long sequences - May be slightly slower\nModify your prediction script to test chunking:\n# Before inference:\nmodel.set_chunk_size(128)\nExperiment: 1. Run with no chunking 2. Run with chunk_size=128 3. Run with chunk_size=64\nQuestions: 1. Does chunking affect the output structure? 2. Does it affect runtime? 3. When would you use this?\n\n\n4.2 Understanding Model Determinism\nTest if ESMFold is deterministic:\nRun the same prediction twice:\npython predict_esmfold.py  # Run 1\nmv output.pdb output1.pdb\n\npython predict_esmfold.py  # Run 2\nmv output.pdb output2.pdb\nCompare in PyMOL:\nExpected result: RMSD = 0.0 Å (identical)\nThis tells us: - ESMFold is deterministic (same input → same output) - Unlike some methods with dropout, it’s reproducible - You can’t generate diversity by running multiple times\nCompare to AF2: - AF2 gives 5 different predictions (different model weights) - Each can be slightly different - Provides ensemble diversity\nDiscussion: - Advantages of determinism? - Disadvantages (no ensemble diversity)? - How do you assess uncertainty with a single prediction?"
  },
  {
    "objectID": "tuesday/2-esmfold.html#part-5-speed-comparison---esmfold-vs-alphafold2",
    "href": "tuesday/2-esmfold.html#part-5-speed-comparison---esmfold-vs-alphafold2",
    "title": "2. ESMFold vs. AlphaFold2",
    "section": "Part 5: Speed Comparison - ESMFold vs AlphaFold2",
    "text": "Part 5: Speed Comparison - ESMFold vs AlphaFold2\nGoal: Understand the practical speed differences between the two approaches.\n\n5.1 Compare Prediction Times\nYou already timed your ESMFold prediction in Part 1. Now compare it to yesterday’s ColabFold run.\nThink back to Activity 1 (ColabFold): - How long did the MSA search take? - How long did the actual structure prediction take (for 5 models)? - What was the total time?\nFill in your observations:\n\n\n\nModel\nApproximate Time\nNotes\n\n\n\n\nESMFold\n_____ seconds/minutes\nFrom Part 1 output\n\n\nColabFold MSA search\n_____ minutes\nFrom yesterday\n\n\nColabFold prediction (5 models)\n_____ minutes\nFrom yesterday\n\n\nTotal ColabFold\n_____ minutes\n\n\n\n\nKey observations: 1. ESMFold is typically 10-60x faster than full AlphaFold2 pipelines 2. Most of AF2’s time is spent on MSA generation (searching sequence databases) 3. ESMFold skips this entirely - it’s trained to predict from sequence alone\n\n\n5.2 When Does Speed Matter?\nConsider these scenarios and think about which model you’d choose:\nScenario 1: Screening 1,000 designed proteins for a protein engineering project - ESMFold could finish in hours - AF2 could take days or weeks - Which would you use first? Why?\nScenario 2: Final structure for an important publication - You need the highest accuracy possible - You have time for validation - Which would you use? Would you run both?\nScenario 3: Analyzing 50 point mutations of a therapeutic antibody - Need quick results to guide next experiments - Medium accuracy is acceptable for initial screening - Your strategy?\nRemember: In practice, many researchers use ESMFold for fast screening, then validate interesting hits with AlphaFold2 for higher accuracy."
  },
  {
    "objectID": "tuesday/2-esmfold.html#part-6-choosing-the-right-model-for-your-research",
    "href": "tuesday/2-esmfold.html#part-6-choosing-the-right-model-for-your-research",
    "title": "2. ESMFold vs. AlphaFold2",
    "section": "Part 6: Choosing the Right Model for Your Research",
    "text": "Part 6: Choosing the Right Model for Your Research\nGoal: Develop intuition for when to use ESMFold vs AlphaFold2.\n\n6.1 Key Differences Summary\n\n\n\n\n\n\n\n\nAspect\nESMFold\nAlphaFold2\n\n\n\n\nTraining approach\nLanguage model on sequences\nStructure prediction with MSA+templates\n\n\nInput required\nSequence only\nSequence (MSA search automatic)\n\n\nSpeed\nVery fast (seconds-minutes)\nSlower (minutes-hours with MSA)\n\n\nNumber of predictions\n1 (deterministic)\n5 (different model weights)\n\n\nBest for\nFast screening, novel proteins\nHigh-stakes accuracy, well-studied proteins\n\n\nNeeds homologs?\nNo\nPerforms better with many homologs\n\n\n\n\n\n6.2 Decision Guide\nUse this guide to help choose the right tool:\nSTART: I need to predict a protein structure\n    ↓\nQ: Is speed critical? (screening many sequences, quick answer needed)\n    YES → Use ESMFold first\n    NO ↓\n\nQ: Is this a designed/synthetic protein with no natural homologs?\n    YES → Use ESMFold (AF2 needs homologs to work well)\n    NO ↓\n\nQ: Is this high-stakes? (publication, drug design, experimental planning)\n    YES → Run BOTH models and compare\n          - Builds confidence if they agree\n          - Reveals uncertainty if they disagree\n    NO ↓\n\nQ: Do I need uncertainty estimates from multiple predictions?\n    YES → Use AlphaFold2 (5 models provide ensemble diversity)\n    NO → Use ESMFold (faster, single high-quality prediction)\n\n\n6.3 Practical Scenarios - Test Your Understanding\nFor each scenario below, think about which model(s) you would use and why:\nScenario 1: Screening 5,000 designed protein variants for stability - Consider: Time constraints, cost, accuracy needs - Your choice: _______ - Why? _______\nScenario 2: Structure for molecular replacement in X-ray crystallography - Consider: Accuracy is critical, you need the best possible model - Your choice: _______ - Why? _______\nScenario 3: Novel metagenomic protein (no known homologs) - Consider: No evolutionary information available - Your choice: _______ - Why? _______\nScenario 4: Well-studied enzyme with 100+ homologs in PDB - Consider: Rich evolutionary information available - Your choice: _______ - Why? _______\nScenario 5: 200 point mutations to assess effect on protein binding - Consider: Need balance between throughput and accuracy - Your choice: _______ - Why? _______\n\n\n6.4 What If the Models Disagree?\nIf you run both ESMFold and AlphaFold2 and they give different structures:\nStep 1: Check confidence scores - Which model has higher average pLDDT? - Where specifically do they disagree (structured regions or flexible loops)?\nStep 2: Understand the disagreement - Small differences (&lt;2Å RMSD): Likely both correct, minor variations - Large differences (&gt;4Å RMSD): Real uncertainty - investigate further\nStep 3: What to do about it - If ESMFold has higher confidence → May indicate AF2 lacks good templates - If AF2 has higher confidence → Evolutionary information may be providing key insights - If both have low confidence in same region → Likely disordered or genuinely uncertain - When in doubt: Use predictions to design experiments, not as final answers\nRemember: Predictions are computational hypotheses. They guide experiments but don’t replace them!"
  },
  {
    "objectID": "tuesday/2-esmfold.html#part-7-key-takeaways-best-practices",
    "href": "tuesday/2-esmfold.html#part-7-key-takeaways-best-practices",
    "title": "2. ESMFold vs. AlphaFold2",
    "section": "Part 7: Key Takeaways & Best Practices",
    "text": "Part 7: Key Takeaways & Best Practices\n\n7.1 What You’ve Learned\nESMFold: ✅ Very fast (no MSA generation needed) ✅ Works on designed/orphan proteins with no homologs ✅ Simple, streamlined workflow ✅ Great for high-throughput screening\n⚠️ Single prediction (no ensemble diversity) ⚠️ May miss evolutionary insights ⚠️ Less extensively validated than AF2\nAlphaFold2: ✅ Leverages evolutionary information (MSAs) ✅ 5-model ensemble for uncertainty estimation ✅ Extensively validated, highly accurate ✅ Best performance on proteins with many homologs\n⚠️ Slower (MSA search takes time) ⚠️ Needs homologs for best performance ⚠️ Higher computational cost\n\n\n7.2 Best Practices for Structure Prediction\nAlways do this: 1. ✅ Check pLDDT confidence scores - don’t trust low-confidence regions blindly 2. ✅ Inspect low-confidence regions carefully (may be disordered or uncertain) 3. ✅ Consider biological context (is this region expected to be structured?) 4. ✅ Validate critical predictions experimentally when possible\nSmart strategies: 1. 💡 Use ESMFold for rapid screening → follow up interesting hits with AF2 2. 💡 For high-stakes work, run both models and compare 3. 💡 When models disagree, use that as a signal to investigate further 4. 💡 Low pLDDT doesn’t always mean “wrong” - might indicate genuine flexibility\n\n\n7.3 Recommended Workflow for Most Projects\nNew protein structure question\n    ↓\nStart with ESMFold (fast initial prediction)\n    ↓\nCheck pLDDT scores\n    ↓\nHigh confidence (&gt;70) overall? ──NO→ Run AlphaFold2\n    ↓                                 (may need evolutionary info)\n    YES                               Compare results\n    ↓\nIs this critical? ──YES→ Validate with AlphaFold2\n    ↓                    (get ensemble diversity)\n    NO\n    ↓\nUse ESMFold prediction\n(fast, good enough for most purposes)\nThe bottom line: There’s no single “best” model. Choose based on your specific needs for speed, accuracy, and the biological context of your protein!"
  },
  {
    "objectID": "tuesday/2-esmfold.html#additional-resources",
    "href": "tuesday/2-esmfold.html#additional-resources",
    "title": "2. ESMFold vs. AlphaFold2",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nDocumentation\n\nESM GitHub: https://github.com/facebookresearch/esm\nESMFold paper: Lin et al. (2023) Science 379:1123-1130\n\n\n\nOnline Resources\n\nESMFold web server: https://esmatlas.com/resources?action=fold\nAlphaFold Database: https://alphafold.ebi.ac.uk/\nRCSB PDB: https://www.rcsb.org/"
  },
  {
    "objectID": "tuesday/index.html",
    "href": "tuesday/index.html",
    "title": "Tuesday",
    "section": "",
    "text": "Tuesday focuses on protein structure prediction using state-of-the-art tools. You’ll learn to visualize protein structures with PyMOL, predict structures using AlphaFold2, and compare different prediction methods (ESMFold vs AlphaFold2) to understand their strengths and trade-offs.",
    "crumbs": [
      "Tuesday"
    ]
  },
  {
    "objectID": "tuesday/index.html#overview",
    "href": "tuesday/index.html#overview",
    "title": "Tuesday",
    "section": "",
    "text": "Tuesday focuses on protein structure prediction using state-of-the-art tools. You’ll learn to visualize protein structures with PyMOL, predict structures using AlphaFold2, and compare different prediction methods (ESMFold vs AlphaFold2) to understand their strengths and trade-offs.",
    "crumbs": [
      "Tuesday"
    ]
  },
  {
    "objectID": "tuesday/index.html#modules",
    "href": "tuesday/index.html#modules",
    "title": "Tuesday",
    "section": "Modules",
    "text": "Modules\n\n\n\nModule\nTopic\nDescription\n\n\n\n\n0. PyMOL\nPyMOL Visualization\nLearn protein structure visualization, navigation, representation, and coloring using PyMOL\n\n\n1. AlphaFold2\nAlphaFold2 Structure Prediction\nRun ColabFold/AlphaFold2 predictions and explore how parameters affect prediction quality\n\n\n2. ESMFold vs. AlphaFold2\nComparing Prediction Methods\nCompare ESMFold’s language model approach vs AF2’s MSA-based approach. Learn when to use each tool\n\n\n\n\n\n\n← Monday\n\n\nBack to Home\n\n\nWednesday →",
    "crumbs": [
      "Tuesday"
    ]
  },
  {
    "objectID": "wednesday/2-rfdiffusion.html",
    "href": "wednesday/2-rfdiffusion.html",
    "title": "2. RFdiffusion: Advanced Design",
    "section": "",
    "text": "Setup for the activity.\n\nGo to the RFdiffusion installation and make a new directory for the activity: mkdir activity.\n\nGenerate unconditional monomers.\n\nMake new folder for this example: cd activity; mkdir 1_uncond; cd 1_uncond\nCopy design_unconditional.sh here: cp ../../examples/design_unconditional.sh .\nEdit the design script by using 1) 'contigmap.contigs=[75-150]', 2) inference.num_designs=5 , 3) ../../scripts/run_inference.py.\nRun the script: bash design_unconditional.sh\n\nWhat did the script create?\nLook at the structures. What are kinds of topologies did you get?\n\n\nGenerate monomers that scaffold a motif.\n\nMake new folder for this example: cd activity; mkdir 2_motifs; cd 2_motifs\nCopy design_motifscaffolding.sh here: cp ../../examples/design_motifscaffolding.sh .\nFor this example, we’ll be scaffolding a site from RSV-F protein. Let’s copy that .pdb here: cp ../../examples/input_pdbs/5TPN.pdb .\nEdit the design script by using 1) inference.input_pdb=5TPN.pdb, 2) 'contigmap.contigs=[10-40/A163-181/10-40]', 3) inference.num_designs=5 , 4) ../../scripts/run_inference.py.\n\nThe backbone we’re generating will have 10-40 residues (randomly sampled), the the motif residues 163-181 (inclusive) on chain A of the input, then 10-40 residues (randomly sampled).\n\nRun the script: bash design_motifscaffolding.sh\n\nLook at the structures. How well is the motif scaffolded?\n\n\nGenerate partially diffused structures.\n\nMake new folder for this example: cd activity; mkdir 3_partial; cd 3_partial\nCopy design_partialdiffusion.sh here: cp ../../examples/design_partialdiffusion.sh .\nFor this example, we’ll be partially noising and denoising a 79 residue protein 2KL8. Let’s copy that .pdb here: cp ../../examples/input_pdbs/2KL8.pdb .\nEdit the design script by using 1) inference.input_pdb=2KL8.pdb, 2) 'contigmap.contigs=[79-79]’, and 3) inference.num_designs=5.\n\nHere we’re generating diversity around a particular fold by noising and denoising 10 steps (20% of the full trajectory). We’re adding noise to the entire structure (all 79 residues), but part of the structure can also be held fixed.\n\nRun the script: bash design_partialdiffusion.sh\n\nLook at the structures. How similar are the outputs to the original structure?\n\n\nGenerate binders with hotspots.\n\nMake new folder for this example: cd activity; mkdir 4_hotspot; cd 4_hotspot\nCopy design_ppi.sh here: cp ../../examples/design_ppi.sh .\nFor this example, we’ll be designing binders to insulin receptor. Let’s copy that .pdb here: cp ../../examples/input_pdbs/insulin_target.pdb .\nEdit the design script by using 1) inference.input_pdb=insulin_target.pdb, 2) 'contigmap.contigs=[A1-150/0 70-100]’, 3) ’ppi.hotspot_res=[A59,A83,A91]’, 4) inference.num_designs=5, 5) denoiser.noise_scale_ca=0, and 6) denoiser.noise_scale_frame=0.\n\nHere, we’re designing binders to insulin receptor. The contig describes the protein we want: residues 1-150 of the A chain of the receptor, a chainbreak (we don’t want to fuse the binder and target!), a 70-100 residue binder to be diffused. We also tell diffusion to target three specific residues, specifically residues 59, 83 and 91 of chain A. Finally, we reduce the noise added during inference to 0 to improve the quality of the designs.\n\nRun the script: bash design_ppi.sh\n\nLook at the structures. What are the topologies of your binders?\n\n\nGenerate fold-conditioned structures.\n\nMake new folder for this example: cd activity; mkdir 5_fold_cond; cd 5_fold_cond\nCopy design_timbarrel.sh here: cp ../../examples/design_timbarrel.sh .\nFor this example, we’ll be diffusing a TIM barrel by providing course-grained specification of the fold. Let’s copy that fold information here: cp -r ../../examples/tim_barrel_scaffold/ .\n\nWhat do the files in this folder represent?\n\nEdit the design script by using inference.num_designs=5.\n\nHere, we’re making a TIM barrel by providing course-grained specification of the fold. We specify the output path. We tell RFdiffusion that we want to do scaffoldguided design, and that we are not making a binder to a target (just a monomer). We provide a path to a directory of TIM barrel scaffolds, generated with the helper script helper_scripts/make_secstruc_adj.py. We generate 5 designs, with a reduced noise scale during inference of 0.5. We also sample additional length to increase diversity of the outputs by masking the loops and inserting 0-5 residues into each loop. We also add 0-5 residues to the N and the C-terminus. These allow use to sample additional diversity in our designs.\nYou may need to edit rfdiffusion/inference/model_runners.py like 751 to self.blockadjacency = iu.BlockAdjacency(conf, conf.inference.num_designs)\n\nRun the script: bash design_timbarrel.sh\n\nLook at the structures. How do these generations compare to the TIM barrel structure used for conditioning (6WVS)?\n\n\nGenerate oligomers of various symmetries.\n\nMake new folder for this example: cd activity; mkdir 6_symmetry; cd 6_symmetry\nCopy design_cyclic_oligos.sh, design_dihedral_oligos.sh, and design_tetrahedral_oligos.sh here: cp ../../examples/design_cyclic_oligos.sh ., cp ../../examples/design_dihedral_oligos.sh ., and cp ../../examples/design_tetrahedral_oligos.sh .\nGenerate cyclic oligomers:\n\nEdit the design_cyclic_oligos.sh script by changing 1) inference.symmetry=\"C4”, 2) inference.num_designs=5, 3) inference.output_prefix=\"example_outputs/C4_oligo”, and 4) 'contigmap.contigs=[200-200]’.\n\nIn this example, we generate 5 designs of C4 symmetric oligomers. For symmetrical diffusion, we need the symmetry config. We also apply an external potential to promote contacts both within (with a relative weight of 1) and between chains (relative weight 0.1). We specify that we want to apply these potentials to all chains, with a guide scale of 2.0 (a sensible starting point). We decay this potential with quadratic form, so that it is applied more strongly initially. We specify a total length of 200 aa, so each chain is 50 residues long.\n\nRun the script: bash design_cyclic_oligos.sh\n\nDo the structures have the desired symmetry?\nWhat topologies are found in the individual subunits?\n\n\nGenerate dihedral oligomers:\n\nEdit the design_dihedral_oligos.sh script by changing inference.num_designs=2.\n\nIn this example, we generate 2 D2 symmetric oligomers using the symmetry config. We also apply an external potential to promote contacts both within (with a relative weight of 1) and between chains (relative weight 0.1). We specify that we want to apply these potentials to all chains, with a guide scale of 2.0 (a sensible starting point). We decay this potential with quadratic form, so that it is applied more strongly initially. We specify a total length of 320 aa, so each chain is 80 residues long.\n\nRun the script: bash design_dihedral_oligos.sh\n\nDo the structures have the desired symmetry?\nWhat topologies are found in the individual subunits?\n\n\nGenerate tetrahedral oligomers:\n\nEdit the design_tetrahedral_oligos.sh script by changing 1) inference.num_designs=2, 2) 'contigmap.contigs=[720-720]’.\n\nIn this example, we generate tetrahedral symmetric oligomers. We use the symmetry config, and specify we want a tetrahedral oligomer, with 2 designs generated. We specify the output prefix, and also the potential we want to apply. This external potential promotes contacts both within (with a relative weight of 1) and between chains (relative weight 0.1). We specify that we want to apply these potentials to all chains, with a guide scale of 2.0 (a sensible starting point). We decay this potential with quadratic form, so that it is applied more strongly initially. We specify a total length of 720 aa, so each chain is 60 residues long\n\nRun the script: bash design_tetrahedral_oligos.sh\n\nDo the structures have the desired symmetry?\nWhat topologies are found in the individual subunits?"
  },
  {
    "objectID": "wednesday/2-rfdiffusion.html#in-class-activity",
    "href": "wednesday/2-rfdiffusion.html#in-class-activity",
    "title": "2. RFdiffusion: Advanced Design",
    "section": "",
    "text": "Setup for the activity.\n\nGo to the RFdiffusion installation and make a new directory for the activity: mkdir activity.\n\nGenerate unconditional monomers.\n\nMake new folder for this example: cd activity; mkdir 1_uncond; cd 1_uncond\nCopy design_unconditional.sh here: cp ../../examples/design_unconditional.sh .\nEdit the design script by using 1) 'contigmap.contigs=[75-150]', 2) inference.num_designs=5 , 3) ../../scripts/run_inference.py.\nRun the script: bash design_unconditional.sh\n\nWhat did the script create?\nLook at the structures. What are kinds of topologies did you get?\n\n\nGenerate monomers that scaffold a motif.\n\nMake new folder for this example: cd activity; mkdir 2_motifs; cd 2_motifs\nCopy design_motifscaffolding.sh here: cp ../../examples/design_motifscaffolding.sh .\nFor this example, we’ll be scaffolding a site from RSV-F protein. Let’s copy that .pdb here: cp ../../examples/input_pdbs/5TPN.pdb .\nEdit the design script by using 1) inference.input_pdb=5TPN.pdb, 2) 'contigmap.contigs=[10-40/A163-181/10-40]', 3) inference.num_designs=5 , 4) ../../scripts/run_inference.py.\n\nThe backbone we’re generating will have 10-40 residues (randomly sampled), the the motif residues 163-181 (inclusive) on chain A of the input, then 10-40 residues (randomly sampled).\n\nRun the script: bash design_motifscaffolding.sh\n\nLook at the structures. How well is the motif scaffolded?\n\n\nGenerate partially diffused structures.\n\nMake new folder for this example: cd activity; mkdir 3_partial; cd 3_partial\nCopy design_partialdiffusion.sh here: cp ../../examples/design_partialdiffusion.sh .\nFor this example, we’ll be partially noising and denoising a 79 residue protein 2KL8. Let’s copy that .pdb here: cp ../../examples/input_pdbs/2KL8.pdb .\nEdit the design script by using 1) inference.input_pdb=2KL8.pdb, 2) 'contigmap.contigs=[79-79]’, and 3) inference.num_designs=5.\n\nHere we’re generating diversity around a particular fold by noising and denoising 10 steps (20% of the full trajectory). We’re adding noise to the entire structure (all 79 residues), but part of the structure can also be held fixed.\n\nRun the script: bash design_partialdiffusion.sh\n\nLook at the structures. How similar are the outputs to the original structure?\n\n\nGenerate binders with hotspots.\n\nMake new folder for this example: cd activity; mkdir 4_hotspot; cd 4_hotspot\nCopy design_ppi.sh here: cp ../../examples/design_ppi.sh .\nFor this example, we’ll be designing binders to insulin receptor. Let’s copy that .pdb here: cp ../../examples/input_pdbs/insulin_target.pdb .\nEdit the design script by using 1) inference.input_pdb=insulin_target.pdb, 2) 'contigmap.contigs=[A1-150/0 70-100]’, 3) ’ppi.hotspot_res=[A59,A83,A91]’, 4) inference.num_designs=5, 5) denoiser.noise_scale_ca=0, and 6) denoiser.noise_scale_frame=0.\n\nHere, we’re designing binders to insulin receptor. The contig describes the protein we want: residues 1-150 of the A chain of the receptor, a chainbreak (we don’t want to fuse the binder and target!), a 70-100 residue binder to be diffused. We also tell diffusion to target three specific residues, specifically residues 59, 83 and 91 of chain A. Finally, we reduce the noise added during inference to 0 to improve the quality of the designs.\n\nRun the script: bash design_ppi.sh\n\nLook at the structures. What are the topologies of your binders?\n\n\nGenerate fold-conditioned structures.\n\nMake new folder for this example: cd activity; mkdir 5_fold_cond; cd 5_fold_cond\nCopy design_timbarrel.sh here: cp ../../examples/design_timbarrel.sh .\nFor this example, we’ll be diffusing a TIM barrel by providing course-grained specification of the fold. Let’s copy that fold information here: cp -r ../../examples/tim_barrel_scaffold/ .\n\nWhat do the files in this folder represent?\n\nEdit the design script by using inference.num_designs=5.\n\nHere, we’re making a TIM barrel by providing course-grained specification of the fold. We specify the output path. We tell RFdiffusion that we want to do scaffoldguided design, and that we are not making a binder to a target (just a monomer). We provide a path to a directory of TIM barrel scaffolds, generated with the helper script helper_scripts/make_secstruc_adj.py. We generate 5 designs, with a reduced noise scale during inference of 0.5. We also sample additional length to increase diversity of the outputs by masking the loops and inserting 0-5 residues into each loop. We also add 0-5 residues to the N and the C-terminus. These allow use to sample additional diversity in our designs.\nYou may need to edit rfdiffusion/inference/model_runners.py like 751 to self.blockadjacency = iu.BlockAdjacency(conf, conf.inference.num_designs)\n\nRun the script: bash design_timbarrel.sh\n\nLook at the structures. How do these generations compare to the TIM barrel structure used for conditioning (6WVS)?\n\n\nGenerate oligomers of various symmetries.\n\nMake new folder for this example: cd activity; mkdir 6_symmetry; cd 6_symmetry\nCopy design_cyclic_oligos.sh, design_dihedral_oligos.sh, and design_tetrahedral_oligos.sh here: cp ../../examples/design_cyclic_oligos.sh ., cp ../../examples/design_dihedral_oligos.sh ., and cp ../../examples/design_tetrahedral_oligos.sh .\nGenerate cyclic oligomers:\n\nEdit the design_cyclic_oligos.sh script by changing 1) inference.symmetry=\"C4”, 2) inference.num_designs=5, 3) inference.output_prefix=\"example_outputs/C4_oligo”, and 4) 'contigmap.contigs=[200-200]’.\n\nIn this example, we generate 5 designs of C4 symmetric oligomers. For symmetrical diffusion, we need the symmetry config. We also apply an external potential to promote contacts both within (with a relative weight of 1) and between chains (relative weight 0.1). We specify that we want to apply these potentials to all chains, with a guide scale of 2.0 (a sensible starting point). We decay this potential with quadratic form, so that it is applied more strongly initially. We specify a total length of 200 aa, so each chain is 50 residues long.\n\nRun the script: bash design_cyclic_oligos.sh\n\nDo the structures have the desired symmetry?\nWhat topologies are found in the individual subunits?\n\n\nGenerate dihedral oligomers:\n\nEdit the design_dihedral_oligos.sh script by changing inference.num_designs=2.\n\nIn this example, we generate 2 D2 symmetric oligomers using the symmetry config. We also apply an external potential to promote contacts both within (with a relative weight of 1) and between chains (relative weight 0.1). We specify that we want to apply these potentials to all chains, with a guide scale of 2.0 (a sensible starting point). We decay this potential with quadratic form, so that it is applied more strongly initially. We specify a total length of 320 aa, so each chain is 80 residues long.\n\nRun the script: bash design_dihedral_oligos.sh\n\nDo the structures have the desired symmetry?\nWhat topologies are found in the individual subunits?\n\n\nGenerate tetrahedral oligomers:\n\nEdit the design_tetrahedral_oligos.sh script by changing 1) inference.num_designs=2, 2) 'contigmap.contigs=[720-720]’.\n\nIn this example, we generate tetrahedral symmetric oligomers. We use the symmetry config, and specify we want a tetrahedral oligomer, with 2 designs generated. We specify the output prefix, and also the potential we want to apply. This external potential promotes contacts both within (with a relative weight of 1) and between chains (relative weight 0.1). We specify that we want to apply these potentials to all chains, with a guide scale of 2.0 (a sensible starting point). We decay this potential with quadratic form, so that it is applied more strongly initially. We specify a total length of 720 aa, so each chain is 60 residues long\n\nRun the script: bash design_tetrahedral_oligos.sh\n\nDo the structures have the desired symmetry?\nWhat topologies are found in the individual subunits?"
  },
  {
    "objectID": "wednesday/2-rfdiffusion.html#project-time",
    "href": "wednesday/2-rfdiffusion.html#project-time",
    "title": "2. RFdiffusion: Advanced Design",
    "section": "Project Time",
    "text": "Project Time\n(Use your target protein)\n\nMilestone 1: Target Preparation & Initial Exploration\n\nSet up your project directory structure\nDownload and inspect your assigned target PDB\nIdentify the binding surface and hotspot residues from the table below\nRun 2-3 test designs to verify your setup is working correctly\n\n\n\nMilestone 2: Unconditional Binder Generation\n\nGenerate at least 10 unconditional binders of length 70-100 towards your target protein.\n\nIs there a particular epitope on the target that RFdiffusion prefers?\nIs there a particular binder topology that RFdiffusion prefers?\n\n\n\n\nMilestone 3: Hotspot-Guided Binder Design\n\nEach of the targets that were assigned were previously used in binder design experiments (1, 2). These experiments made use of hotspots located on the target proteins. Generate at least 10 binders towards your target protein using the hotspots below.\n\nIs there a particular binder topology that RFdiffusion prefers to generate?\n\n\n\n\n\nTarget\nUniProt ID\nPDB ID\nHotspot Residues (chain and residue index from PDB)\n\n\n\n\nPD-L1\nQ9NZQ7\n5O45\nA56, A115, A123\n\n\nIL-7Rɑ\nP16871\n3DI3\nB58, B80, B139\n\n\nTrkA receptor\nP04629\n1WWW\nX294, X296, X333\n\n\nIFNAR2\nP48551\n2LAG\nB52, B82, B98\n\n\nBet v 1-A\nP15494\n1BV1\nA24, A28, A43\n\n\n\n\n\nMilestone 4: Potential Optimization\n\nPotentials can be powerful ways to bias the generation process. Try at least 3 different combinations of potentials, generating 5 backbones with each. Be sure to use hotspots for these generations too!\n\nHow did the potentials change your outputs?\nDid you find a particularly useful configuration of potentials?\n\n\n\n\nMilestone 5: Analysis & Selection\n\nCompare your unconditional vs. hotspot-guided vs. potential-optimized designs\nIdentify your top 3-5 most promising binder designs\nDocument what makes these designs stand out (binding pose, topology, contact with hotspots, etc.)"
  },
  {
    "objectID": "wednesday/2-rfdiffusion.html#troubleshooting-guide",
    "href": "wednesday/2-rfdiffusion.html#troubleshooting-guide",
    "title": "2. RFdiffusion: Advanced Design",
    "section": "Troubleshooting Guide",
    "text": "Troubleshooting Guide\n\nCommon Issues & Solutions\n\n\n\n\n\n\n\n\nProblem\nLikely Cause\nSolution\n\n\n\n\n“CUDA out of memory”\nGPU memory exhausted\nReduce inference.num_designs to 1-2, or design smaller proteins\n\n\nScript hangs at “Initializing model”\nMissing model weights or incorrect paths\nVerify RFdiffusion installation and model checkpoint paths\n\n\n“FileNotFoundError” for input PDB\nIncorrect file path or missing file\nCheck that PDB file exists in current directory with ls *.pdb\n\n\nDesigns look extended/unfolded\nInsufficient denoising or inappropriate settings\nCheck inference.num_steps (should be ~50), verify contig syntax\n\n\nBinders don’t contact target\nIncorrect contig specification or chainbreak\nVerify /0 chainbreak in contigs, check residue numbering in target PDB\n\n\nBinders don’t contact hotspots\nHotspot residues too far apart or incorrect chain ID\nVerify chain IDs and residue numbers match your PDB file exactly\n\n\n“ImportError” or module not found\nConda environment not activated\nActivate RFdiffusion environment: conda activate RFdiffusion (or appropriate env name)\n\n\nScript runs but produces no outputs\nOutput directory doesn’t exist or permissions issue\nCheck that output directory exists, verify write permissions\n\n\nSymmetric oligomers don’t look symmetric\nIncorrect symmetry specification or potentials\nVerify symmetry string (e.g., “C4”, “D2”, “T”), check potential settings\n\n\n“BlockAdjacency” error in fold-conditioned\nKnown bug in model_runners.py\nEdit rfdiffusion/inference/model_runners.py line ~751 as noted in activity\n\n\nVery slow generation times\nNormal for large/complex designs\nTetrahedral oligomers can take 30-60 min. Consider using screen or tmux\n\n\nAll designs look very similar\nInsufficient diversity sampling\nIncrease contig length ranges (e.g., [60-100] instead of [80-80]), adjust noise scales\n\n\n\n\n\nTips for Success\n\nAlways check your PDB file first: Use PyMOL or ChimeraX to verify chain IDs and residue numbering before running\nStart small: Test with num_designs=1 first to verify your setup works\nSave your commands: Keep a log of successful parameter combinations\nUse descriptive output names: Include key parameters in output_prefix (e.g., hotspot_A59_A83_A91)\nCheck the logs: RFdiffusion creates log files - read them if something goes wrong"
  },
  {
    "objectID": "wednesday/index.html",
    "href": "wednesday/index.html",
    "title": "Wednesday",
    "section": "",
    "text": "Wednesday focuses on advanced structure prediction and protein design. You’ll compare AlphaFold3 and Chai-1 for complex predictions (multimers, ligands, ions), then dive into RFdiffusion for de novo protein design including binder generation and symmetric oligomers.",
    "crumbs": [
      "Wednesday"
    ]
  },
  {
    "objectID": "wednesday/index.html#overview",
    "href": "wednesday/index.html#overview",
    "title": "Wednesday",
    "section": "",
    "text": "Wednesday focuses on advanced structure prediction and protein design. You’ll compare AlphaFold3 and Chai-1 for complex predictions (multimers, ligands, ions), then dive into RFdiffusion for de novo protein design including binder generation and symmetric oligomers.",
    "crumbs": [
      "Wednesday"
    ]
  },
  {
    "objectID": "wednesday/index.html#modules",
    "href": "wednesday/index.html#modules",
    "title": "Wednesday",
    "section": "Modules",
    "text": "Modules\n\n\n\nModule\nTopic\nDescription\n\n\n\n\n1. AlphaFold3 vs. Chai-1\nComparing Next-Gen Predictors\nCompare AF3 and Chai-1 on monomers, multimers, and protein-ligand complexes\n\n\n2. RFdiffusion\nAdvanced Protein Design\nDesign unconditional monomers, motif scaffolds, binders, and symmetric oligomers using RFdiffusion\n\n\n\n\n\n\n← Tuesday\n\n\nBack to Home\n\n\nThursday →",
    "crumbs": [
      "Wednesday"
    ]
  },
  {
    "objectID": "wednesday/1-alphafold3.html",
    "href": "wednesday/1-alphafold3.html",
    "title": "1. AlphaFold3 vs. Chai-1",
    "section": "",
    "text": "Setup for the activity\n\nIn order to make a prediction with the AlphaFold server, you’ll need a Google account to sign in.\nIn order to make a prediction with Chai-Lab, you’ll need to create a free account.\n\nMake a monomer prediction for 1QYS with both AF3 and Chai-Lab.\n\nAlphaFold 3:\n\nPaste in the sequence for 1QYS: MGDIQVQVNIDDNGKNFDYTYTVTTESELQKVLNELMDYIKKQGAKRVRISITARTKKEAEKFAAILIKVFAELGYNDINVTFDGDTVTVEGQLEGGSLEHHHHHH\nPress the “continue and preview job” button.\nPress “confirm and submit job”.\n\nChai-Lab:\n\nPaste in the same sequence for 1QYS.\nClick “Run Job” to submit the prediction.\n\nAnalysis:\n\nHow do the predicted structures compare to the native?\nHow does the AF3 prediction compare to Chai-Lab?\nHow confident is each model about its prediction?\n\n\nMake a multimer prediction for 8AJY with both AF3 and Chai-Lab.\n\nAlphaFold 3:\n\nPaste in the sequence for 8AJY_1 (chain A): MLTDRGMTYDLDPKDGSSAATKPVLEVTKKVFDTAADAAGQTVTVEFKVSGAEGKYATTGYHIYWDERLEVVATKTGAYAKKGAALEDSSLAKAENNGNGVFVASGADDDFGADGVMWTVELKVPADAKAGDVYPIDVAYQWDPSKGDLFTDNKDSAQGKLMQAYFFTQGIKSSSNPSTDEYLVKANATYADGYIAIKAGEPE\nClick “add entity”, make sure the molecule type to “protein”, and paste in the sequence of 8AJY_2 (chain B): MGSSHHHHHHSSGLVPRGSHMASKPQYRLGDVDFNGIIDGRDATAVLTEYARISTGKPAEFVGNTALAADVNKDNMIDAADATHILTYYAISSTRDDITSDDYFALHQPLRG\nThe native structure was originally solved in the presence of 3 Ca2+ ions. Click “add entity”, change the molecule type to “ion”, and select Ca2+ to add each.\nPress the “continue and preview job” button.\nPress “confirm and submit job”.\n\nChai-Lab:\n\nPaste in the sequence for 8AJY_1 (chain A).\nClick “Add Chain” and paste in the sequence for 8AJY_2 (chain B).\nClick “Add Ligand” and select “Ca2+” from the ion options. Repeat to add all 3 Ca2+ ions.\nClick “Run Job” to submit the prediction.\n\nAnalysis:\n\nHow do the predicted structures compare to the native?\nHow does the AF3 prediction compare to Chai-Lab?\nHow confident is each model about its prediction?\nWhat about the placement of the ions and the coordinating side chains?\n\n\nMake a protein-ligand prediction with Chai-Lab for 2JIE (beta-glucosidase with 2-fluoro-glucose).\n\nChai-Lab:\n\nPaste in the sequence for 2JIE (beta-glucosidase B): MHHHHHHSENTFIFPATFMWGTSTSSYQIEGGTDEGGRTPSIWDTFCQIPGKVIGGDCGDVACDHFHHFKEDVQLMKQLGFLHYRFSVAWPRIMPAAGIINEEGLLFYEHLLDEIELAGLIPMLTLYHWDLPQWIEDEGGWTQRETIQHFKTYASVIMDRFGERINWWNTINEPYCASILGYGTGEHAPGHENWREAFTAAHHILMCHGIASNLHKEKGLTGKIGITLNMEHVDAASERPEDVAAAIRRDGFINRWFAEPLFNGKYPEDMVEWYGTYLNGLDFVQPGDMELIQQPGDFLGINYYTRSIIRSTNDASLLQVEQVHMEEPVTDMGWEIHPESFYKLLTRIEKDFSKGLPILITENGAAMRDELVNGQIEDTGRHGYIEEHLKACHRFIEEGGQLKGYFVWSFLDNFEWAWGYSKRFGIVHINYETQERTPKQSALWFKQMMAKNGF\nClick “Add Ligand” and select “Custom SMILES”.\nEnter the SMILES string for 2-fluoro-glucose (G2F): OCC1OC(O)C(F)C(O)C1O\nClick “Run Job” to submit the prediction.\n\nAnalysis:\n\nHow does the predicted structure compare to the crystal structure (2JIE)?\nIs the ligand positioned correctly in the active site?\nHow well does Chai-Lab predict the protein-ligand interactions?\nWhat is the confidence score for the ligand placement?"
  },
  {
    "objectID": "wednesday/1-alphafold3.html#in-class-activity",
    "href": "wednesday/1-alphafold3.html#in-class-activity",
    "title": "1. AlphaFold3 vs. Chai-1",
    "section": "",
    "text": "Setup for the activity\n\nIn order to make a prediction with the AlphaFold server, you’ll need a Google account to sign in.\nIn order to make a prediction with Chai-Lab, you’ll need to create a free account.\n\nMake a monomer prediction for 1QYS with both AF3 and Chai-Lab.\n\nAlphaFold 3:\n\nPaste in the sequence for 1QYS: MGDIQVQVNIDDNGKNFDYTYTVTTESELQKVLNELMDYIKKQGAKRVRISITARTKKEAEKFAAILIKVFAELGYNDINVTFDGDTVTVEGQLEGGSLEHHHHHH\nPress the “continue and preview job” button.\nPress “confirm and submit job”.\n\nChai-Lab:\n\nPaste in the same sequence for 1QYS.\nClick “Run Job” to submit the prediction.\n\nAnalysis:\n\nHow do the predicted structures compare to the native?\nHow does the AF3 prediction compare to Chai-Lab?\nHow confident is each model about its prediction?\n\n\nMake a multimer prediction for 8AJY with both AF3 and Chai-Lab.\n\nAlphaFold 3:\n\nPaste in the sequence for 8AJY_1 (chain A): MLTDRGMTYDLDPKDGSSAATKPVLEVTKKVFDTAADAAGQTVTVEFKVSGAEGKYATTGYHIYWDERLEVVATKTGAYAKKGAALEDSSLAKAENNGNGVFVASGADDDFGADGVMWTVELKVPADAKAGDVYPIDVAYQWDPSKGDLFTDNKDSAQGKLMQAYFFTQGIKSSSNPSTDEYLVKANATYADGYIAIKAGEPE\nClick “add entity”, make sure the molecule type to “protein”, and paste in the sequence of 8AJY_2 (chain B): MGSSHHHHHHSSGLVPRGSHMASKPQYRLGDVDFNGIIDGRDATAVLTEYARISTGKPAEFVGNTALAADVNKDNMIDAADATHILTYYAISSTRDDITSDDYFALHQPLRG\nThe native structure was originally solved in the presence of 3 Ca2+ ions. Click “add entity”, change the molecule type to “ion”, and select Ca2+ to add each.\nPress the “continue and preview job” button.\nPress “confirm and submit job”.\n\nChai-Lab:\n\nPaste in the sequence for 8AJY_1 (chain A).\nClick “Add Chain” and paste in the sequence for 8AJY_2 (chain B).\nClick “Add Ligand” and select “Ca2+” from the ion options. Repeat to add all 3 Ca2+ ions.\nClick “Run Job” to submit the prediction.\n\nAnalysis:\n\nHow do the predicted structures compare to the native?\nHow does the AF3 prediction compare to Chai-Lab?\nHow confident is each model about its prediction?\nWhat about the placement of the ions and the coordinating side chains?\n\n\nMake a protein-ligand prediction with Chai-Lab for 2JIE (beta-glucosidase with 2-fluoro-glucose).\n\nChai-Lab:\n\nPaste in the sequence for 2JIE (beta-glucosidase B): MHHHHHHSENTFIFPATFMWGTSTSSYQIEGGTDEGGRTPSIWDTFCQIPGKVIGGDCGDVACDHFHHFKEDVQLMKQLGFLHYRFSVAWPRIMPAAGIINEEGLLFYEHLLDEIELAGLIPMLTLYHWDLPQWIEDEGGWTQRETIQHFKTYASVIMDRFGERINWWNTINEPYCASILGYGTGEHAPGHENWREAFTAAHHILMCHGIASNLHKEKGLTGKIGITLNMEHVDAASERPEDVAAAIRRDGFINRWFAEPLFNGKYPEDMVEWYGTYLNGLDFVQPGDMELIQQPGDFLGINYYTRSIIRSTNDASLLQVEQVHMEEPVTDMGWEIHPESFYKLLTRIEKDFSKGLPILITENGAAMRDELVNGQIEDTGRHGYIEEHLKACHRFIEEGGQLKGYFVWSFLDNFEWAWGYSKRFGIVHINYETQERTPKQSALWFKQMMAKNGF\nClick “Add Ligand” and select “Custom SMILES”.\nEnter the SMILES string for 2-fluoro-glucose (G2F): OCC1OC(O)C(F)C(O)C1O\nClick “Run Job” to submit the prediction.\n\nAnalysis:\n\nHow does the predicted structure compare to the crystal structure (2JIE)?\nIs the ligand positioned correctly in the active site?\nHow well does Chai-Lab predict the protein-ligand interactions?\nWhat is the confidence score for the ligand placement?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "",
    "text": "Welcome back! You were last on:   Resume where you left off\nWelcome to the ML Protein Design Bootcamp 2025! This self-paced course covers machine learning tools for protein structure prediction and design."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "Course Overview",
    "text": "Course Overview\nThis bootcamp is organized into 4 days of content plus a capstone project. Each day contains sequential modules that build on each other.\n\n\nMonday\nTool Installation: Install essential ML protein design tools on your HPC cluster including LocalColabFold, ESMFold, LigandMPNN, RFdiffusion2, Chai-1, Boltz-2, and BindCraft.\n\n\nTuesday\nStructure Prediction: Learn protein visualization with PyMOL, predict structures with AlphaFold2, and compare prediction methods (ESMFold vs AF2) to understand their trade-offs.\n\n\nWednesday\nAdvanced Prediction & Design: Compare AlphaFold3 and Chai-1 on complex predictions, then design proteins with RFdiffusion including binders and symmetric oligomers.\n\n\nThursday\nComing soon\nModules for Thursday.\n\n\nCapstone\nCapstone project bringing together everything you’ve learned."
  },
  {
    "objectID": "index.html#your-progress",
    "href": "index.html#your-progress",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "Your Progress",
    "text": "Your Progress\n\n\nCompleted modules: 0\n\n\n&lt;div id=\"progress-bar\" class=\"progress-bar\" role=\"progressbar\" style=\"width: 0%;\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"&gt;0%&lt;/div&gt;\n\n\n\nReset Progress"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "Getting Started",
    "text": "Getting Started\n\nChoose a day from the navigation above or the cards below\nWork through modules in order - each builds on the previous\nCheck off sections as you complete them - your progress is saved automatically\nReturn anytime - use the “Resume” button to pick up where you left off\n\n\nReport an Issue / Ask a Question (GitHub account required)"
  },
  {
    "objectID": "tuesday/1-alphafold2.html",
    "href": "tuesday/1-alphafold2.html",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "",
    "text": "–max-msa 1:1",
    "crumbs": [
      "Tuesday",
      "1-alphafold2.html"
    ]
  },
  {
    "objectID": "tuesday/0-pymol.html",
    "href": "tuesday/0-pymol.html",
    "title": "0. PyMOL Visualization",
    "section": "",
    "text": "This activity covers the basics of protein structure visualization using PyMOL.",
    "crumbs": [
      "Tuesday",
      "0. PyMOL Visualization"
    ]
  },
  {
    "objectID": "tuesday/0-pymol.html#resources",
    "href": "tuesday/0-pymol.html#resources",
    "title": "0. PyMOL Visualization",
    "section": "Resources",
    "text": "Resources\n\nPyMOL Tutorial Slides (PDF)\nAesthetic Script (Python) - Script to recreate RFdiffusion figure aesthetics.",
    "crumbs": [
      "Tuesday",
      "0. PyMOL Visualization"
    ]
  },
  {
    "objectID": "tuesday/0-pymol.html#instructions",
    "href": "tuesday/0-pymol.html#instructions",
    "title": "0. PyMOL Visualization",
    "section": "Instructions",
    "text": "Instructions\n\nDownload and install PyMOL (or use the version on your cluster/classroom computers).\nFollow the tutorial slides to learn about navigation, representation, and coloring.\nUse the aesthetic.py script to apply custom styling to your structures.\n\n\nUsing the Aesthetic Script\nTo use the provided script in PyMOL:\n\nDownload aesthetic.py.\nIn PyMOL, go to File -&gt; Run Script… and select the file.\nOr run it from the PyMOL command line: bash     run /path/to/aesthetic.py\nThis will load new commands get_colors and get_lighting which you can use to beautify your protein renders.",
    "crumbs": [
      "Tuesday",
      "0. PyMOL Visualization"
    ]
  },
  {
    "objectID": "monday/9-placer.html",
    "href": "monday/9-placer.html",
    "title": "9. PLACER",
    "section": "",
    "text": "PLACER (paper, code) stands for Protein-Ligand Atomistic Conformational Ensemble Resolver. It’s a graph neural network that operates entirely at the atomic level to generate conformational ensembles of protein-ligand complexes.",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#why-use-placer",
    "href": "monday/9-placer.html#why-use-placer",
    "title": "9. PLACER",
    "section": "Why Use PLACER?",
    "text": "Why Use PLACER?\n\nEnsemble predictions: Generates multiple conformations to capture binding uncertainty\nAll-atom accuracy: Operates at atomic level for precise interactions\nSide chain flexibility: Predicts side chain conformations alongside ligand poses\nConfidence scores: Multiple metrics for ranking and validating predictions\n\nRelated Tools: For protein-protein docking, see DiffDock-PP. For structure prediction of complexes, see Chai-1 or Boltz-2.",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#resource-requirements",
    "href": "monday/9-placer.html#resource-requirements",
    "title": "9. PLACER",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n8 GB\n16 GB\n1-3 sec/model on GPU\n\n\nCPU RAM\n16 GB\n32 GB\n~1 min/model on 8 cores\n\n\nDisk Space\n2 GB\n5 GB\nModel weights included\n\n\nPython\n3.9+\n3.10\nRequired\n\n\n\nPerformance:\n\nGPU: 1-3 seconds per model\nCPU (8 cores): ~1 minute per model\nLigands with many symmetric groups take longer",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#preparation",
    "href": "monday/9-placer.html#preparation",
    "title": "9. PLACER",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nGPU recommended for reasonable throughput",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#installation",
    "href": "monday/9-placer.html#installation",
    "title": "9. PLACER",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the repository:\n\ngit clone https://github.com/baker-laboratory/PLACER.git\ncd PLACER\nThe repository includes model weights - no separate download needed.\n\nCreate the conda environment from the provided file:\n\nmamba env create -f envs/placer_env.yml\n\nActivate the environment:\n\nmamba activate placer_env",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#testing-the-installation",
    "href": "monday/9-placer.html#testing-the-installation",
    "title": "9. PLACER",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a simple heme docking prediction:\npython run_PLACER.py \\\n    --ifile examples/inputs/dnHEM1.pdb \\\n    --odir test_output \\\n    --rerank prmsd \\\n    -n 10 \\\n    --ligand_file HEM:examples/ligands/HEM.mol2\nSuccess indicators:\n\nCommand completes without errors\ntest_output/ directory is created\nContains ranked PDB files of docked complexes\n\nExpected runtime: 30-60 seconds on GPU, 10-15 minutes on CPU.",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#hpc-job-script",
    "href": "monday/9-placer.html#hpc-job-script",
    "title": "9. PLACER",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=placer\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate placer_env\n\ncd /path/to/PLACER\n\n# Predict ligand binding with 100 samples\npython run_PLACER.py \\\n    --ifile my_complex.pdb \\\n    --odir results/ \\\n    --predict_ligand LIG-501 \\\n    --rerank prmsd \\\n    -n 100 \\\n    --ligand_file LIG:ligand.sdf",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#usage-examples",
    "href": "monday/9-placer.html#usage-examples",
    "title": "9. PLACER",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic ligand docking:\npython run_PLACER.py \\\n    -f INPUT.pdb \\\n    -o OUTPUT_DIR \\\n    -n 50\nLigand docking with cofactor fixed:\npython run_PLACER.py \\\n    --ifile 4dtz.cif \\\n    --odir output/ \\\n    --predict_ligand D-LDP-501 \\\n    --fixed_ligand C-HEM-500 \\\n    -n 100 \\\n    --rerank prmsd\nSide chain prediction (apo mode, no ligand):\npython run_PLACER.py \\\n    --ifile protein.pdb \\\n    --odir output/ \\\n    --target_res A-149 \\\n    -n 50 \\\n    --no-use_sm\nMultiple ligands simultaneously:\npython run_PLACER.py \\\n    --ifile complex.pdb \\\n    --odir output/ \\\n    --predict_multi \\\n    --predict_ligand LIG1 LIG2 \\\n    -n 100",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#key-parameters",
    "href": "monday/9-placer.html#key-parameters",
    "title": "9. PLACER",
    "section": "Key Parameters",
    "text": "Key Parameters\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\n-f or --ifile\nInput PDB/mmCIF file\n\n\n-o or --odir\nOutput directory\n\n\n-n or --nsamples\nNumber of ensemble samples (50-100 recommended)\n\n\n--rerank\nRank by confidence: prmsd, plddt, or plddt_pde\n\n\n--predict_ligand\nSpecify which ligand(s) to predict\n\n\n--fixed_ligand\nKeep certain ligands fixed in place\n\n\n--ligand_file\nProvide SDF/MOL2 for correct atom typing\n\n\n--target_res\nSpecific residue(s) for side chain prediction\n\n\n--no-use_sm\nApo mode - predict without small molecules",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#understanding-confidence-scores",
    "href": "monday/9-placer.html#understanding-confidence-scores",
    "title": "9. PLACER",
    "section": "Understanding Confidence Scores",
    "text": "Understanding Confidence Scores\nPLACER provides multiple confidence metrics:\n\n\n\n\n\n\n\n\nMetric\nDescription\nGood Values\n\n\n\n\nprmsd\nPredicted RMSD to true pose\n&lt;2.0 Å (excellent), &lt;4.0 Å (acceptable)\n\n\nplddt\nPer-residue confidence (1D track)\n&gt;0.8\n\n\nplddt_pde\nPer-residue confidence (2D track)\n&gt;0.8\n\n\nfape\nAll-atom FAPE loss\nLower is better\n\n\nrmsd\nActual RMSD to reference (if available)\n&lt;2.0 Å\n\n\nkabsch\nSuperimposed RMSD\nMeasures conformation accuracy\n\n\n\nRecommendation: Use --rerank prmsd for docking tasks.",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#python-api",
    "href": "monday/9-placer.html#python-api",
    "title": "9. PLACER",
    "section": "Python API",
    "text": "Python API\nPLACER can be imported as a Python module:\nimport sys\nsys.path.append(\"/path/to/PLACER\")\nimport PLACER\n\n# Load model\nplacer = PLACER.PLACER()\n\n# Set up input\npl_input = PLACER.PLACERinput()\npl_input.pdb(\"complex.pdb\")\npl_input.name(\"my_prediction\")\npl_input.ligand_reference({\"HEM\": \"heme.mol2\"})\n\n# Run 50 predictions\noutputs = placer.run(pl_input, 50)\n\n# Access results\nfor out in outputs:\n    print(f\"pRMSD: {out.prmsd:.2f}, pLDDT: {out.plddt:.2f}\")",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#understanding-the-output",
    "href": "monday/9-placer.html#understanding-the-output",
    "title": "9. PLACER",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\noutput/\n├── ranked_0.pdb          # Best pose by confidence\n├── ranked_1.pdb          # Second best\n├── ranked_2.pdb          # ...\n├── scores.csv            # All confidence metrics\n└── ensemble/             # All generated samples\nInterpreting ensemble results:\n\nMultiple similar poses = high confidence\nDiverse poses = uncertain binding mode\nCompare top-ranked poses to assess convergence",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#input-format-requirements",
    "href": "monday/9-placer.html#input-format-requirements",
    "title": "9. PLACER",
    "section": "Input Format Requirements",
    "text": "Input Format Requirements\nLigand must be in input structure:\n\nPLACER requires the ligand to be present in the PDB\nSMILES-only input is not supported\nUse --ligand_file to provide correct bonding information\n\nLigand file formats:\n\nSDF files: Best for drug-like molecules\nMOL2 files: Good for cofactors\nHelps with: aromatic rings, stereochemistry, bond orders",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/9-placer.html#troubleshooting",
    "href": "monday/9-placer.html#troubleshooting",
    "title": "9. PLACER",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nNon-planar aromatic rings:\n\nProvide SDF/MOL2 file with --ligand_file\nThis ensures correct bonding information\n\nMissing ligands in PDB:\n\nLigands must be in the input structure\nSMILES-only input is not currently supported\n\nCustom/non-canonical residues:\npython run_PLACER.py \\\n    --ifile input.pdb \\\n    --residue_json custom_residues.json \\\n    --odir output/\nSlow predictions:\n\nSymmetric ligands (many equivalent atoms) are slower\nUse GPU for production runs\nReduce -n for initial testing\n\nOut of memory:\n\nPLACER is generally memory-efficient\nIf issues persist, try reducing ensemble size\nOr use CPU with multiple cores",
    "crumbs": [
      "Monday",
      "9. PLACER"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html",
    "href": "monday/4-esmfold.html",
    "title": "4. ESMFold",
    "section": "",
    "text": "ESMFold (paper, code) is an end-to-end single-sequence structure predictor that uses the ESM-2 language model to generate accurate 3D protein structures directly from sequence, without requiring multiple sequence alignments (MSAs).",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#why-use-esmfold",
    "href": "monday/4-esmfold.html#why-use-esmfold",
    "title": "4. ESMFold",
    "section": "Why Use ESMFold?",
    "text": "Why Use ESMFold?\n\nSpeed: Significantly faster than AlphaFold2 (seconds vs minutes)\nNo MSA required: Works directly from sequence alone\nCompetitive accuracy: Often comparable to AlphaFold2 for well-folded domains\nLower resource usage: Can run on smaller GPUs\n\nRelated Tools: For MSA-based prediction with potentially higher accuracy, see LocalColabFold or OpenFold. For protein language model embeddings only, see ESM3.",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#resource-requirements",
    "href": "monday/4-esmfold.html#resource-requirements",
    "title": "4. ESMFold",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n40+ GB\nLarger proteins need more memory\n\n\nCPU RAM\n16 GB\n32 GB\nCPU-only is possible but slow\n\n\nDisk Space\n5 GB\n10 GB\nModel weights\n\n\nPython\n≤3.9\n3.9\nImportant: Python 3.10+ may have issues\n\n\n\nWhy Python ≤3.9? ESMFold depends on OpenFold, which has compatibility issues with newer Python versions.",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#preparation",
    "href": "monday/4-esmfold.html#preparation",
    "title": "4. ESMFold",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nnvcc available (for compiling OpenFold dependencies)\n\nVerify your environment:\nnvcc --version      # Required for OpenFold compilation\nmodule load cuda    # If nvcc not found",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#installation",
    "href": "monday/4-esmfold.html#installation",
    "title": "4. ESMFold",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nCreate a conda environment with Python 3.9:\n\nmamba create -n esmfold python=3.9\nmamba activate esmfold\n\nInstall PyTorch (adjust CUDA version to match your cluster):\n\nmamba install pytorch pytorch-cuda=12.1 -c pytorch -c nvidia\n\nInstall ESM with ESMFold dependencies:\n\npip install \"fair-esm[esmfold]\"\n\nInstall OpenFold dependencies:\n\npip install 'dllogger @ git+https://github.com/NVIDIA/dllogger.git'\npip install 'openfold @ git+https://github.com/aqlaboratory/openfold.git@4b41059694619831a7db195b7e0988fc4ff3a307'\nNote: OpenFold compilation requires nvcc. If it fails, verify CUDA toolkit is loaded.\nAlternative method (using environment file):\nwget https://raw.githubusercontent.com/facebookresearch/esm/main/environment.yml\nmamba env create -f environment.yml\nmamba activate esmfold",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#testing-the-installation",
    "href": "monday/4-esmfold.html#testing-the-installation",
    "title": "4. ESMFold",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test script test_esmfold.py:\nimport torch\nimport esm\n\n# Load ESMFold model\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()  # Remove .cuda() if using CPU\n\n# Test sequence (65 residues)\nsequence = \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"\n\n# Run prediction\nwith torch.no_grad():\n    output = model.infer_pdb(sequence)\n\n# Save output\nwith open(\"test_result.pdb\", \"w\") as f:\n    f.write(output)\n\nprint(\"Structure prediction successful!\")\nprint(f\"Output saved to test_result.pdb\")\nprint(f\"Sequence length: {len(sequence)} residues\")\nRun the test:\npython test_esmfold.py\nSuccess indicators:\n\nCommand completes without errors\ntest_result.pdb file is created\nFile contains valid PDB coordinates\n\nExpected runtime: ~10-30 seconds on GPU for this small protein.",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#hpc-job-script",
    "href": "monday/4-esmfold.html#hpc-job-script",
    "title": "4. ESMFold",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=esmfold\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate esmfold\n\n# Predict structures for all sequences in FASTA file\nesm-fold -i my_proteins.fasta -o predictions/ \\\n    --num-recycles 4 \\\n    --max-tokens-per-batch 1024",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#usage-examples",
    "href": "monday/4-esmfold.html#usage-examples",
    "title": "4. ESMFold",
    "section": "Usage Examples",
    "text": "Usage Examples\nCommand line interface:\nesm-fold -i sequences.fasta -o output_pdbs/\nKey CLI options:\n\n\n\nOption\nDescription\n\n\n\n\n-i\nInput FASTA file\n\n\n-o\nOutput directory for PDB files\n\n\n--num-recycles\nNumber of recycles (default: 4)\n\n\n--max-tokens-per-batch\nBatch shorter sequences together\n\n\n--chunk-size\nReduce memory (values: 128, 64, 32)\n\n\n--cpu-only\nRun on CPU only\n\n\n--cpu-offload\nOffload to CPU RAM for long sequences\n\n\n\nReduce memory for large proteins:\nesm-fold -i large_proteins.fasta -o output/ --chunk-size 64\nProcess very long sequences:\nesm-fold -i long_sequences.fasta -o output/ --cpu-offload\nPython API:\nimport torch\nimport esm\n\n# Load model\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\n# Predict structure\nsequence = \"MVKLTAEGSEVSRQVIVQDIAYLRSLG\"\nwith torch.no_grad():\n    pdb_string = model.infer_pdb(sequence)\n\n# Save\nwith open(\"prediction.pdb\", \"w\") as f:\n    f.write(pdb_string)\nGet confidence scores:\nimport torch\nimport esm\n\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\nsequence = \"MVKLTAEGSEVSRQVIVQDIAYLRSLG\"\nwith torch.no_grad():\n    output = model.infer(sequence)\n\n# Per-residue confidence (pLDDT)\nplddt = output[\"plddt\"]  # Shape: (1, L)\nprint(f\"Mean pLDDT: {plddt.mean().item():.2f}\")\n\n# Predicted TM-score\nptm = output[\"ptm\"]\nprint(f\"pTM: {ptm.item():.3f}\")",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#understanding-the-output",
    "href": "monday/4-esmfold.html#understanding-the-output",
    "title": "4. ESMFold",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nPDB output:\n\nStandard PDB format with predicted coordinates\nB-factor column contains pLDDT confidence scores (0-100)\nHigher pLDDT = higher confidence\n\nConfidence score interpretation:\n\n\n\npLDDT Range\nInterpretation\n\n\n\n\n90-100\nVery high confidence\n\n\n70-90\nConfident\n\n\n50-70\nLow confidence (may be disordered)\n\n\n&lt;50\nVery low confidence (likely disordered)",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#memory-usage-guide",
    "href": "monday/4-esmfold.html#memory-usage-guide",
    "title": "4. ESMFold",
    "section": "Memory Usage Guide",
    "text": "Memory Usage Guide\nApproximate GPU memory by sequence length:\n\n\n\nSequence Length\nGPU Memory Needed\n\n\n\n\n&lt;200 aa\n8-16 GB\n\n\n200-400 aa\n16-24 GB\n\n\n400-600 aa\n24-40 GB\n\n\n600-1000 aa\n40-80 GB\n\n\n&gt;1000 aa\nUse --cpu-offload",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/4-esmfold.html#troubleshooting",
    "href": "monday/4-esmfold.html#troubleshooting",
    "title": "4. ESMFold",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nOpenFold installation fails:\n\nVerify nvcc is available:\nnvcc --version\n# If not found:\nmodule load cuda\nEnsure PyTorch CUDA version matches system CUDA\n\n“CUDA out of memory”:\n# Use chunking to reduce memory\nesm-fold -i input.fasta -o output/ --chunk-size 64\n\n# Or use CPU offloading for very long sequences\nesm-fold -i input.fasta -o output/ --cpu-offload\nSlow on GPU (should be fast):\n# Verify CUDA is detected\npython -c \"import torch; print(torch.cuda.is_available())\"\n# Should print: True\nPython version errors:\n\nESMFold requires Python ≤3.9 due to OpenFold dependencies\nCreate a new environment with Python 3.9 if needed\n\nModel download hangs:\n\nFirst run downloads ~2GB of weights\nSet custom cache location:\nexport TORCH_HOME=/scratch/$USER/torch_cache",
    "crumbs": [
      "Monday",
      "4. ESMFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html",
    "href": "monday/1-localcolabfold.html",
    "title": "1. LocalColabFold",
    "section": "",
    "text": "LocalColabFold (code) is a local installation of ColabFold, which provides an efficient implementation of AlphaFold2 protein structure prediction. ColabFold combines fast MSA generation from MMseqs2 with AlphaFold2’s structure prediction capabilities, making it significantly faster than the original AlphaFold2 implementation.",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#why-use-localcolabfold",
    "href": "monday/1-localcolabfold.html#why-use-localcolabfold",
    "title": "1. LocalColabFold",
    "section": "Why Use LocalColabFold?",
    "text": "Why Use LocalColabFold?\n\nHigh-throughput predictions: Run batch jobs without Colab time limits\nNo internet dependency: All computations run locally after setup\nHPC integration: Leverage your cluster’s GPUs for faster predictions\nMSA flexibility: Use pre-computed MSAs or generate them on-the-fly\n\nRelated Tools: For structure prediction without MSAs, see ESMFold. For multi-modal complexes, see Chai-1 or Boltz-2.",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#resource-requirements",
    "href": "monday/1-localcolabfold.html#resource-requirements",
    "title": "1. LocalColabFold",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n40+ GB\nA100 recommended for proteins &gt;500 aa\n\n\nCPU RAM\n32 GB\n64 GB\nMSA generation is memory-intensive\n\n\nDisk Space\n15 GB\n100+ GB\nModel weights + optional databases\n\n\nCUDA\n11.1+\n12.1+\nCheck compatibility",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#preparation",
    "href": "monday/1-localcolabfold.html#preparation",
    "title": "1. LocalColabFold",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\n\n\n\n\n\n\nImportantPrerequisites\n\n\n\n\nCompleted HPC Setup guide\nAccess to a GPU node for testing\n~15 GB disk space for installation\n\n\n\n\n\n\n\n\n\nNoteVerify your environment\n\n\n\nnvidia-smi          # Check GPU is available\nnvcc --version      # Check CUDA version",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#installation",
    "href": "monday/1-localcolabfold.html#installation",
    "title": "1. LocalColabFold",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nDownload the installation script: Navigate to the directory where you want to install LocalColabFold (e.g., your scratch directory or apps folder).\n\nwget https://raw.githubusercontent.com/YoshitakaMo/localcolabfold/main/install_colabbatch_linux.sh\n\nMake the script executable and run it:\n\nchmod +x install_colabbatch_linux.sh\n./install_colabbatch_linux.sh\nThis creates a localcolabfold directory containing: - A conda environment (colabfold_batch) - ColabFold and all dependencies - Model weights (~10-15 GB, downloaded automatically)\nExpected install time: 15-30 minutes depending on network speed.\n\nAdd the environment to your PATH (add to ~/.bashrc for permanent access):\n\nexport PATH=\"/path/to/your/localcolabfold/colabfold-conda/bin:$PATH\"",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#testing-the-installation",
    "href": "monday/1-localcolabfold.html#testing-the-installation",
    "title": "1. LocalColabFold",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\n\nActivate the ColabFold environment:\n\nsource localcolabfold/colabfold-conda/bin/activate\n\nCreate a directory for testing and a test FASTA file:\n\nmkdir -p tests\necho \"&gt;test_protein\nMKFLKFSLLTAVLLSVVFAFSSCGDDDDTYPYDVPDYAGTCGDDDDTYPYDVPDYA\" &gt; tests/test.fasta\n\nRun prediction:\n\ncolabfold_batch tests/test.fasta tests/test_output/\nSuccess indicators:\n\nCommand completes without errors\ntests/test_output/ directory contains:\n\ntest_protein_relaxed_rank_001_*.pdb (predicted structure)\ntest_protein_scores_rank_001_*.json (confidence scores)\ntest_protein_coverage.png (MSA coverage plot)\n\n\nExpected runtime: 2-5 minutes for this small test protein.\nVerify GPU is being used:\n# In another terminal while prediction runs:\nnvidia-smi\n# Look for python process using GPU memory",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#hpc-job-script",
    "href": "monday/1-localcolabfold.html#hpc-job-script",
    "title": "1. LocalColabFold",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=colabfold\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\n# Activate environment\nsource /path/to/localcolabfold/colabfold-conda/bin/activate\n\n# Optional: Use shared database location\nexport COLABFOLD_DOWNLOAD_DIR=/shared/colabfold_db\n\n# Run prediction\ncolabfold_batch input.fasta output_dir/",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#usage-examples",
    "href": "monday/1-localcolabfold.html#usage-examples",
    "title": "1. LocalColabFold",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic prediction:\ncolabfold_batch sequences.fasta predictions/\nWith custom MSA server (if your HPC has one):\ncolabfold_batch --msa-server \"https://internal.server.edu\" sequences.fasta predictions/\nMultimer prediction (protein complexes):\n# Separate chains with : in the FASTA file\n# &gt;complex\n# SEQUENCEA:SEQUENCEB\ncolabfold_batch complex.fasta complex_output/\nBatch with templates:\ncolabfold_batch --templates sequences.fasta predictions/\nReduce memory usage (for large proteins):\ncolabfold_batch --amber --num-recycle 3 large_protein.fasta output/",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#understanding-the-output",
    "href": "monday/1-localcolabfold.html#understanding-the-output",
    "title": "1. LocalColabFold",
    "section": "Understanding the Output",
    "text": "Understanding the Output\n\n\n\n\n\n\n\nFile\nDescription\n\n\n\n\n*_relaxed_rank_001_*.pdb\nBest predicted structure (Amber-relaxed)\n\n\n*_unrelaxed_rank_001_*.pdb\nBest prediction before relaxation\n\n\n*_scores_rank_001_*.json\npLDDT and pTM scores\n\n\n*_coverage.png\nMSA coverage visualization\n\n\n*_pae.png\nPredicted Aligned Error heatmap\n\n\n\nConfidence scores:\n\npLDDT (per-residue): &gt;90 high confidence, 70-90 confident, 50-70 low, &lt;50 very low\npTM (overall): &gt;0.8 high confidence for whole structure\nPAE (pairwise): Lower is better, indicates domain organization confidence",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/1-localcolabfold.html#troubleshooting",
    "href": "monday/1-localcolabfold.html#troubleshooting",
    "title": "1. LocalColabFold",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n\n\n\n\n\nWarningCommon Issues\n\n\n\n“CUDA out of memory”:\n\nRequest GPU with more memory (#SBATCH --gpus=a100:1)\nUse --amber flag to reduce peak memory\nFor very large proteins (&gt;1000 aa), use Chai-1 or Boltz-2 instead\n\nMSA generation is slow:\n\nUse the MMseqs2 server option for faster MSA generation\nPre-compute MSAs for frequently used sequences\n\nDatabase location filling home directory:\n# Set in ~/.bashrc before running\nexport COLABFOLD_DOWNLOAD_DIR=/scratch/$USER/colabfold_db\nModel weights download fails:\n\nCheck network connectivity\nManually download from: https://storage.googleapis.com/alphafold/\nPlace in ~/.cache/colabfold/params/\n\nGPU not being used (slow prediction):\n# Verify CUDA is detected\npython -c \"import torch; print(torch.cuda.is_available())\"\n# Should print: True",
    "crumbs": [
      "Monday",
      "1. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html",
    "href": "monday/prework-3-python.html",
    "title": "Pre-work 3: Python Refresher",
    "section": "",
    "text": "This assignment is designed as a Python refresher for biologists and bioinformaticians who may be a bit rusty. You’ll complete a series of small exercises that build up to creating a simple sequence analysis tool. This assignment should take approximately 30-45 minutes to complete.\nIMPORTANT: Each Python file contains function templates with TODO comments. Your job is to fill in the missing code where you see # TODO: comments. The scripts won’t work until you complete the TODO sections!",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#overview",
    "href": "monday/prework-3-python.html#overview",
    "title": "Pre-work 3: Python Refresher",
    "section": "",
    "text": "This assignment is designed as a Python refresher for biologists and bioinformaticians who may be a bit rusty. You’ll complete a series of small exercises that build up to creating a simple sequence analysis tool. This assignment should take approximately 30-45 minutes to complete.\nIMPORTANT: Each Python file contains function templates with TODO comments. Your job is to fill in the missing code where you see # TODO: comments. The scripts won’t work until you complete the TODO sections!",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#prerequisites",
    "href": "monday/prework-3-python.html#prerequisites",
    "title": "Pre-work 3: Python Refresher",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nHW1 completed (Python environment set up)",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#download-assignment-files",
    "href": "monday/prework-3-python.html#download-assignment-files",
    "title": "Pre-work 3: Python Refresher",
    "section": "Download Assignment Files",
    "text": "Download Assignment Files\n  Download Assignment Files (ZIP) \n\nDownload the ZIP file above.\nUnzip the folder on your computer.\nOpen the folder in VS Code.",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#how-to-complete-this-assignment",
    "href": "monday/prework-3-python.html#how-to-complete-this-assignment",
    "title": "Pre-work 3: Python Refresher",
    "section": "How to Complete This Assignment",
    "text": "How to Complete This Assignment\nEach Python file is a template with incomplete functions. Look for # TODO: comments - these mark where you need to write code. Read the docstrings and hints carefully, then implement the missing functionality.\nThe scripts will not work until you complete the TODO sections!\n\nPart 1: Python Basics Warmup (basics.py)\nOpen basics.py and fill in the TODO sections for these exercises: - reverse_string() - Write a function to reverse a string - count_characters() - Use a for loop to count characters in a string - amino_acid_composition() - Create a dictionary to calculate amino acid percentages - filter_sequences_by_length() - Use a list comprehension to filter sequences\nTo test your work:\n# Navigate to the unzipped directory first!\ncd path/to/python-refresher\n\nconda activate bootcamp2025_HW1\npython basics.py\nIf you see None or errors, you haven’t completed all the TODOs yet!\n\n\nPart 2: Sequence Utilities (sequence_utils.py)\nOpen sequence_utils.py and implement the TODO sections for these bioinformatics functions: - molecular_weight() - Calculate the molecular weight of a protein - count_hydrophobic() - Count hydrophobic amino acids - find_motif() - Find all positions of a motif in a sequence - count_charged_residues() - Count positive and negative charges\nTo test your work:\npython sequence_utils.py\nThe test cases at the bottom will only work once you’ve filled in all the TODOs!\n\n\nPart 3: FASTA File Parsing (read_fasta.py)\nOpen read_fasta.py and complete the TODO sections to: - read_fasta() - Read and parse a FASTA file, storing sequences in a dictionary (header → sequence) - print_fasta_stats() - Print basic statistics about the sequences\nTo test your work:\npython read_fasta.py sample.fasta\nYou should see information about each protein sequence. If not, check your TODOs!\n\n\nPart 4: Sequence Analysis Tool (analyze_sequence.py)\nOpen analyze_sequence.py and fill in the TODO sections to create a complete analysis script: - analyze_sequences() - Use your sequence_utils functions to analyze each protein - write_results() - Write the analysis results to a file\nThis part brings together everything from Parts 1-3!\nTo run your completed analysis:\npython analyze_sequence.py sample.fasta\nThis should create an analysis_results.txt file with your results. If it doesn’t work, you’re missing some TODOs!",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#tips",
    "href": "monday/prework-3-python.html#tips",
    "title": "Pre-work 3: Python Refresher",
    "section": "Tips",
    "text": "Tips\n\nTest frequently: Run your code after each function to catch errors early\nUse print statements: Debug by printing intermediate values\nRead the TODO comments carefully: They contain hints and requirements\nDon’t worry about edge cases: Focus on getting the basic functionality working\nAsk for help: If you’re stuck, reach out!",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#expected-output",
    "href": "monday/prework-3-python.html#expected-output",
    "title": "Pre-work 3: Python Refresher",
    "section": "Expected Output",
    "text": "Expected Output\nWhen you run analyze_sequence.py, you should see output similar to:\nAnalyzing sequences from sample.fasta...\n\nSequence: gene_example_1\n  Length: 120 bp\n  GC Content: 45.83%\n  Reverse Complement: CGATCG...\n\nResults written to analysis_results.txt",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#questions-and-getting-help",
    "href": "monday/prework-3-python.html#questions-and-getting-help",
    "title": "Pre-work 3: Python Refresher",
    "section": "Questions and Getting Help",
    "text": "Questions and Getting Help\nWe strongly encourage you to use our Slack workspace for questions and collaboration! One of the best ways to learn is by discussing problems with your peers and seeing how others approach challenges.\nJoin the Bootcamp Slack channel: The Slack workspace is currently reserved for in-person bootcamp participants. We apologize for the inconvenience.\nIn the Slack channel, you can: - Ask questions and get help from fellow students - Share tips and solutions you’ve discovered - Learn from others’ questions and answers - Collaborate on troubleshooting issues\nThe TAs and I will be active in the Slack channel to help guide discussions and answer questions. Don’t hesitate to jump in - chances are if you’re stuck on something, others are too!\nIf you have questions, please open an issue on the GitHub repository (GitHub account required) instead of emailing directly.",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html",
    "href": "monday/7-boltz2.html",
    "title": "7. Boltz-2",
    "section": "",
    "text": "Boltz-2 (paper, code) is a biomolecular foundation model that jointly models complex structures and binding affinities. It’s the first deep learning model to approach the accuracy of physics-based free-energy perturbation (FEP) methods while running 1000x faster.",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#why-use-boltz-2",
    "href": "monday/7-boltz2.html#why-use-boltz-2",
    "title": "7. Boltz-2",
    "section": "Why Use Boltz-2?",
    "text": "Why Use Boltz-2?\n\nStructure + Affinity: Predict both binding pose and binding strength\nDrug discovery ready: Affinity predictions useful for hit-to-lead optimization\nMulti-modal: Handles proteins, nucleic acids, small molecules, covalent modifications\nSpeed: 1000x faster than FEP methods for affinity prediction\n\nRelated Tools: For structure prediction only, see Chai-1. For protein-ligand docking, see PLACER or DiffDock-PP.",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#resource-requirements",
    "href": "monday/7-boltz2.html#resource-requirements",
    "title": "7. Boltz-2",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n32+ GB\nScales with complex size\n\n\nCPU RAM\n16 GB\n32 GB\nFor preprocessing\n\n\nDisk Space\n5 GB\n10 GB\nModel weights\n\n\nPython\n3.9+\n3.11\nRequired",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#preparation",
    "href": "monday/7-boltz2.html#preparation",
    "title": "7. Boltz-2",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nCUDA-capable GPU (recommended) or CPU\n\nImportant: Install Boltz in a fresh Python environment to avoid dependency conflicts.",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#installation",
    "href": "monday/7-boltz2.html#installation",
    "title": "7. Boltz-2",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nCreate a fresh environment:\n\nmamba create -n boltz python=3.11\nmamba activate boltz\n\nInstall Boltz with CUDA support:\n\npip install boltz[cuda] -U\nFor CPU-only or non-CUDA GPUs:\npip install boltz -U\nAlternative: Install from GitHub (for latest updates):\ngit clone https://github.com/jwohlwend/boltz.git\ncd boltz\npip install -e .[cuda]",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#testing-the-installation",
    "href": "monday/7-boltz2.html#testing-the-installation",
    "title": "7. Boltz-2",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test YAML file test_input.yaml:\nversion: 1\nsequences:\n  - protein:\n      id: [A, B]\n      sequence: MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\nRun prediction:\nboltz predict test_input.yaml --use_msa_server\nSuccess indicators:\n\nCommand completes without errors\nOutput directory contains:\n\nPredicted structure files (CIF format)\nConfidence scores\n\n\nExpected runtime: 1-3 minutes for this small test.",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#hpc-job-script",
    "href": "monday/7-boltz2.html#hpc-job-script",
    "title": "7. Boltz-2",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=boltz\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate boltz\n\n# Run prediction\nboltz predict my_complex.yaml --use_msa_server --out_dir results/",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#usage-examples",
    "href": "monday/7-boltz2.html#usage-examples",
    "title": "7. Boltz-2",
    "section": "Usage Examples",
    "text": "Usage Examples\nStructure prediction only:\nboltz predict structure.yaml\nWith MSA server (higher accuracy):\nboltz predict input.yaml --use_msa_server\nWith affinity prediction:\n# input.yaml\nversion: 1\nsequences:\n  - protein:\n      id: A\n      sequence: MKTVRQERLK...\n  - ligand:\n      id: L\n      smiles: \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\"\nproperties:\n  - affinity\nboltz predict input.yaml",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#input-format-yaml",
    "href": "monday/7-boltz2.html#input-format-yaml",
    "title": "7. Boltz-2",
    "section": "Input Format (YAML)",
    "text": "Input Format (YAML)\nBoltz uses YAML files to describe biomolecules:\nSimple protein:\nversion: 1\nsequences:\n  - protein:\n      id: A\n      sequence: MKTVRQERLKSIVRILERSKEPVSG...\nProtein-ligand complex:\nversion: 1\nsequences:\n  - protein:\n      id: A\n      sequence: MKTVRQERLK...\n  - ligand:\n      id: L\n      smiles: \"CCO\"\nProtein complex (homodimer):\nversion: 1\nsequences:\n  - protein:\n      id: [A, B]  # Same sequence for both chains\n      sequence: MKTVRQERLK...\nWith affinity prediction:\nversion: 1\nsequences:\n  - protein:\n      id: A\n      sequence: MKTVRQERLK...\n  - ligand:\n      id: L\n      smiles: \"CC(=O)NC1=CC=C(O)C=C1\"\nproperties:\n  - affinity\nSee prediction documentation for full format details.",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#binding-affinity-predictions",
    "href": "monday/7-boltz2.html#binding-affinity-predictions",
    "title": "7. Boltz-2",
    "section": "Binding Affinity Predictions",
    "text": "Binding Affinity Predictions\nBoltz-2 provides two affinity metrics:\n\n\n\n\n\n\n\n\nMetric\nRange\nUse Case\n\n\n\n\naffinity_probability_binary\n0-1\nHit discovery - probability that ligand is a binder\n\n\naffinity_pred_value\nlog10(IC50) in μM\nLead optimization - compare binding strengths\n\n\n\nInterpretation:\n\naffinity_probability_binary: Higher = more likely to bind\naffinity_pred_value: Lower = stronger binding (lower IC50)",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#msa-server-authentication",
    "href": "monday/7-boltz2.html#msa-server-authentication",
    "title": "7. Boltz-2",
    "section": "MSA Server Authentication",
    "text": "MSA Server Authentication\nFor servers requiring authentication:\nexport BOLTZ_MSA_TOKEN=\"your_token_here\"\nboltz predict input.yaml --use_msa_server",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#understanding-the-output",
    "href": "monday/7-boltz2.html#understanding-the-output",
    "title": "7. Boltz-2",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\nboltz_results_&lt;input&gt;/\n├── predictions/\n│   ├── model_0.cif      # Predicted structure\n│   └── confidence.json  # Confidence scores\n├── msa/                 # Generated MSAs (if using server)\n└── affinity/            # Affinity predictions (if requested)\nConfidence metrics:\n\npLDDT: Per-residue confidence\npTM: Predicted TM-score\ninterface pTM: For complexes",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#performance-comparison",
    "href": "monday/7-boltz2.html#performance-comparison",
    "title": "7. Boltz-2",
    "section": "Performance Comparison",
    "text": "Performance Comparison\n\n\n\nMethod\nSpeed\nAffinity Accuracy\n\n\n\n\nFEP (physics-based)\nHours-days\nGold standard\n\n\nBoltz-2\nSeconds-minutes\nComparable to FEP\n\n\nTraditional docking\nSeconds\nLower accuracy",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/7-boltz2.html#troubleshooting",
    "href": "monday/7-boltz2.html#troubleshooting",
    "title": "7. Boltz-2",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nInstallation issues:\n\nUse a fresh environment\nTry removing [cuda] if CUDA issues arise\nVerify CUDA version compatibility\n\n“MSA server error”:\n\nCheck network connectivity\nVerify authentication token if required\nTry without --use_msa_server for testing\n\nOut of memory:\n\nRequest more GPU memory\nReduce complex size\nTry CPU-only mode for testing\n\nSlow without GPU:\n\nCPU mode is functional but significantly slower\nAlways use GPU for production runs\n\nYAML parsing errors:\n\nCheck YAML syntax (indentation matters)\nEnsure SMILES strings are quoted\nVerify sequence format",
    "crumbs": [
      "Monday",
      "7. Boltz-2"
    ]
  },
  {
    "objectID": "monday/5-openfold.html",
    "href": "monday/5-openfold.html",
    "title": "5. OpenFold (Optional)",
    "section": "",
    "text": "OpenFold (paper, code) is a faithful, trainable PyTorch reproduction of DeepMind’s AlphaFold2. It achieves performance comparable to AlphaFold2 and provides a fully open-source implementation for protein structure prediction.",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#why-use-openfold",
    "href": "monday/5-openfold.html#why-use-openfold",
    "title": "5. OpenFold (Optional)",
    "section": "Why Use OpenFold?",
    "text": "Why Use OpenFold?\n\nFull transparency: Open-source model architecture and training code\nTrainable: Can be fine-tuned or retrained on custom data\nResearch-friendly: Ideal for understanding how structure prediction works\nMSA-based accuracy: Uses evolutionary information for high-accuracy predictions\n\nRelated Tools: For faster predictions without MSAs, see ESMFold. For a more user-friendly MSA-based option, see LocalColabFold.",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#resource-requirements",
    "href": "monday/5-openfold.html#resource-requirements",
    "title": "5. OpenFold (Optional)",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n40+ GB\nA100 for large proteins\n\n\nCPU RAM\n32 GB\n64+ GB\nMSA generation is memory-intensive\n\n\nDisk Space\n500 GB\n2+ TB\nSequence databases are large\n\n\nCUDA\n11.3+\n12.1+\nRequired for compilation\n\n\n\nNote: OpenFold requires significant disk space for sequence databases if generating MSAs locally. Check if your HPC already has AlphaFold/OpenFold databases installed.",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#preparation",
    "href": "monday/5-openfold.html#preparation",
    "title": "5. OpenFold (Optional)",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nnvcc available for CUDA compilation\nSignificant disk space (or access to shared databases)\n\nCheck for existing databases:\n# Ask your HPC admins or check common locations\nls /shared/databases/alphafold/\nls /shared/databases/openfold/\nMany HPCs have pre-installed AlphaFold databases that OpenFold can use.",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#installation",
    "href": "monday/5-openfold.html#installation",
    "title": "5. OpenFold (Optional)",
    "section": "Installation",
    "text": "Installation\n Mark as complete\nImportant: OpenFold installation can be complex. The official documentation at openfold.readthedocs.io has the most current instructions.\n\nClone the repository:\n\ngit clone https://github.com/aqlaboratory/openfold.git\ncd openfold\n\nCreate the conda environment:\n\nmamba env create -f environment.yml\nmamba activate openfold_venv\nExpected time: 10-20 minutes for environment creation.\n\nInstall OpenFold:\n\npip install -e .\n\nDownload model weights:\n\nbash scripts/download_openfold_params.sh openfold/resources\nExpected download: ~1-2 GB of model weights.\n\n(Optional) Download sequence databases for MSA generation:\n\n# This downloads ~2TB of data - skip if using HPC shared databases\nbash scripts/download_alphafold_dbs.sh /path/to/database/directory",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#testing-the-installation",
    "href": "monday/5-openfold.html#testing-the-installation",
    "title": "5. OpenFold (Optional)",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test FASTA file test.fasta:\n&gt;test_protein\nMKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\nRun a prediction (using pre-computed MSAs or without MSAs for testing):\npython run_pretrained_openfold.py \\\n    test.fasta \\\n    /path/to/database/directory \\\n    --output_dir predictions/ \\\n    --config_preset model_1_ptm \\\n    --model_device cuda:0\nNote: For testing without databases, you can use --use_precomputed_alignments with a directory containing pre-computed MSA files.\nSuccess indicators:\n\nCommand completes without errors\npredictions/ directory contains PDB files\nOutput includes confidence metrics (pLDDT, pTM)\n\nExpected runtime: 5-30 minutes depending on MSA availability and protein size.",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#hpc-job-script",
    "href": "monday/5-openfold.html#hpc-job-script",
    "title": "5. OpenFold (Optional)",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=openfold\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --time=08:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc # Source shell profile if needed\nmamba activate openfold_venv\n\ncd /path/to/openfold\n\n# Using HPC shared databases\nDATABASE_DIR=/shared/databases/alphafold\n\npython run_pretrained_openfold.py \\\n    my_protein.fasta \\\n    $DATABASE_DIR \\\n    --output_dir predictions/ \\\n    --config_preset model_1_ptm \\\n    --model_device cuda:0",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#usage-examples",
    "href": "monday/5-openfold.html#usage-examples",
    "title": "5. OpenFold (Optional)",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic prediction with local databases:\npython run_pretrained_openfold.py \\\n    input.fasta \\\n    /path/to/databases \\\n    --output_dir output/ \\\n    --config_preset model_1_ptm\nUsing pre-computed MSAs:\npython run_pretrained_openfold.py \\\n    input.fasta \\\n    /path/to/databases \\\n    --use_precomputed_alignments /path/to/msas/ \\\n    --output_dir output/\nMultiple model presets (ensemble):\nfor preset in model_1_ptm model_2_ptm model_3_ptm; do\n    python run_pretrained_openfold.py \\\n        input.fasta \\\n        /path/to/databases \\\n        --config_preset $preset \\\n        --output_dir output_${preset}/\ndone",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#model-presets",
    "href": "monday/5-openfold.html#model-presets",
    "title": "5. OpenFold (Optional)",
    "section": "Model Presets",
    "text": "Model Presets\n\n\n\nPreset\nDescription\n\n\n\n\nmodel_1_ptm\nStandard model with pTM head\n\n\nmodel_2_ptm\nAlternative model with pTM\n\n\nmodel_3_ptm\nThird model variant\n\n\nmodel_1_multimer_v3\nFor protein complexes",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#understanding-the-output",
    "href": "monday/5-openfold.html#understanding-the-output",
    "title": "5. OpenFold (Optional)",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\npredictions/\n├── test_protein_model_1_ptm_unrelaxed.pdb    # Predicted structure\n├── test_protein_model_1_ptm_confidences.json # Confidence scores\n└── test_protein_model_1_ptm_timings.json     # Runtime statistics\nConfidence metrics:\n\npLDDT: Per-residue confidence (0-100, higher is better)\npTM: Predicted TM-score (0-1, &gt;0.8 is confident)\nPAE: Predicted Aligned Error matrix",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#database-requirements",
    "href": "monday/5-openfold.html#database-requirements",
    "title": "5. OpenFold (Optional)",
    "section": "Database Requirements",
    "text": "Database Requirements\nIf generating MSAs locally, OpenFold needs these databases:\n\n\n\nDatabase\nSize\nPurpose\n\n\n\n\nBFD\n~1.7 TB\nSequence alignments\n\n\nMGnify\n~120 GB\nMetagenomic sequences\n\n\nUniRef90\n~100 GB\nSequence clustering\n\n\nUniRef30\n~200 GB\nHHblits templates\n\n\nPDB70\n~60 GB\nStructure templates\n\n\n\nTotal: ~2+ TB\nCheck HPC shared databases first - most research HPCs have these pre-installed.",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#troubleshooting",
    "href": "monday/5-openfold.html#troubleshooting",
    "title": "5. OpenFold (Optional)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nCompilation errors during install:\n# Ensure CUDA toolkit is loaded\nmodule load cuda/12.1\nnvcc --version\n\n# Clean and retry\npip uninstall openfold\npip install -e .\n“Database not found” errors:\n\nVerify database paths exist\nCheck HPC documentation for shared database locations\nContact HPC admins about AlphaFold database availability\n\nOut of memory:\n\nRequest more GPU memory\nReduce --max_recycling_iters\nUse gradient checkpointing for training\n\nSlow MSA generation:\n\nMSA generation is CPU-bound and can take hours\nUse pre-computed MSAs when possible\nConsider using ColabFold’s MMseqs2 server instead\n\nModel weights not found:\n# Re-download weights\nbash scripts/download_openfold_params.sh openfold/resources\n\n# Verify files exist\nls openfold/resources/*.pt",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/5-openfold.html#for-researchers-training",
    "href": "monday/5-openfold.html#for-researchers-training",
    "title": "5. OpenFold (Optional)",
    "section": "For Researchers: Training",
    "text": "For Researchers: Training\nOpenFold can be retrained or fine-tuned:\npython train_openfold.py \\\n    /path/to/training/data \\\n    /path/to/template_mmcif \\\n    /path/to/output \\\n    --config_preset initial_training\nSee the training documentation for details.",
    "crumbs": [
      "Monday",
      "5. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html",
    "href": "monday/0-hpc-setup.html",
    "title": "0. Common HPC Setup",
    "section": "",
    "text": "Before installing individual ML tools, ensure your HPC environment is properly configured. This guide covers the foundational setup that all subsequent modules depend on.",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#resource-requirements-overview",
    "href": "monday/0-hpc-setup.html#resource-requirements-overview",
    "title": "0. Common HPC Setup",
    "section": "Resource Requirements Overview",
    "text": "Resource Requirements Overview\nMost ML protein tools share similar computational requirements. Here’s a general guide:\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n40+ GB\nA100 80GB ideal for large proteins\n\n\nCPU RAM\n32 GB\n64 GB\nMore for MSA generation\n\n\nDisk Space\n50 GB\n200+ GB\nModel weights + databases\n\n\nCUDA\n11.6+\n12.1+\nCheck tool-specific requirements",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#checking-your-hpc-environment",
    "href": "monday/0-hpc-setup.html#checking-your-hpc-environment",
    "title": "0. Common HPC Setup",
    "section": "Checking Your HPC Environment",
    "text": "Checking Your HPC Environment\n Mark as complete\n\n\n\n\n\n\nTipInternet Access on HPC\n\n\n\nMany HPC clusters do not have internet access on compute nodes (the nodes where your heavy jobs run). They often only have internet on “login” or “head” nodes.\n\nDownloads: Always run installation and download commands on a login node.\nExecution: When running jobs, ensure your tools don’t try to download models on the fly. Pre-download all weights and databases.\n\n\n\n\n1. Check Available CUDA Modules\nmodule avail cuda\nThis shows all CUDA versions installed on your cluster. Note the versions - you’ll need to match them to tool requirements.\n\n\n2. Check GPU Availability\nRequest an interactive GPU session:\n# SLURM example\nsrun --gpus=1 --pty bash\nThen check GPU status:\nnvidia-smi\nThis shows: - GPU model (A100, V100, RTX 4090, etc.) - GPU memory (important for large models) - Current CUDA driver version\n\n\n3. Check CUDA Toolkit Version\nnvcc --version\nIf this fails, load a CUDA module first:\nmodule load cuda/12.1\nnvcc --version",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#condamamba-setup",
    "href": "monday/0-hpc-setup.html#condamamba-setup",
    "title": "0. Common HPC Setup",
    "section": "Conda/Mamba Setup",
    "text": "Conda/Mamba Setup\n Mark as complete\nMost tools use Conda environments. Mamba is recommended as it’s significantly faster than Conda for dependency resolution.\n\nInstalling Mamba (if not available)\nIf your HPC doesn’t have Mamba, install Miniforge:\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\nbash Miniforge3-Linux-x86_64.sh\nFollow prompts, then restart your shell or run:\nsource ~/.bashrc\n\n\nBest Practices for HPC Conda Usage\n\nUse dedicated environment directories: Set environment location to avoid filling home directory quota:\n\n# Add to ~/.condarc\nenvs_dirs:\n  - /scratch/$USER/conda_envs\npkgs_dirs:\n  - /scratch/$USER/conda_pkgs\n\nOne environment per tool: Don’t try to install all tools in one environment - dependency conflicts are common.\nExport environments for reproducibility:\n\nmamba env export &gt; environment.yml",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#docker-vs-singularityapptainer",
    "href": "monday/0-hpc-setup.html#docker-vs-singularityapptainer",
    "title": "0. Common HPC Setup",
    "section": "Docker vs Singularity/Apptainer",
    "text": "Docker vs Singularity/Apptainer\n Mark as complete\nIMPORTANT: Most academic HPCs do NOT support Docker for security reasons. Use Singularity or Apptainer instead.\n\nLoading Container Runtime\nmodule load apptainer\n# or on older systems:\nmodule load singularity\n\n\nConverting Docker Commands to Apptainer\nMany tool READMEs show Docker commands. Here’s how to translate them:\n\n\n\nDocker Command\nApptainer Equivalent\n\n\n\n\ndocker run\napptainer run\n\n\ndocker run --gpus all\napptainer run --nv\n\n\ndocker run -v /path:/path\napptainer run --bind /path:/path\n\n\ndocker pull image:tag\napptainer pull docker://image:tag\n\n\n\nExample conversion:\n# Docker (won't work on HPC):\ndocker run --gpus all -v $(pwd):/workspace myimage:latest python script.py\n\n# Apptainer (works on HPC):\napptainer run --nv --bind $(pwd):/workspace myimage.sif python script.py\n\n\nPulling Docker Images as Singularity Files\napptainer pull docker://nvcr.io/nvidia/pytorch:23.10-py3\n# Creates: pytorch_23.10-py3.sif",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#slurm-job-submission-basics",
    "href": "monday/0-hpc-setup.html#slurm-job-submission-basics",
    "title": "0. Common HPC Setup",
    "section": "SLURM Job Submission Basics",
    "text": "SLURM Job Submission Basics\n Mark as complete\nMost HPCs use SLURM for job scheduling. Here’s a template for ML jobs:\n#!/bin/bash\n#SBATCH --job-name=my_ml_job\n#SBATCH --partition=gpu          # GPU partition name (varies by cluster)\n#SBATCH --gpus=1                  # Number of GPUs\n#SBATCH --cpus-per-task=8         # CPUs for data loading\n#SBATCH --mem=64G                 # RAM\n#SBATCH --time=04:00:00           # Wall time (HH:MM:SS)\n#SBATCH --output=%x_%j.out        # Output file (%x=job name, %j=job ID)\n#SBATCH --error=%x_%j.err         # Error file\n\n# Load required modules\nmodule load cuda/12.1\nmodule load apptainer\n\n# Activate conda environment\n# source ~/.bashrc  # Source your shell profile if needed\nsource /path/to/your/miniconda3/etc/profile.d/conda.sh # Better: source conda.sh directly\nmamba activate my_env\n\n# Run your command\npython my_script.py\n\nCommon SLURM Commands\n\n\n\nCommand\nDescription\n\n\n\n\nsbatch script.sh\nSubmit job\n\n\nsqueue -u $USER\nCheck your jobs\n\n\nscancel JOB_ID\nCancel a job\n\n\nsinfo\nShow partition info\n\n\nsacct -j JOB_ID\nJob accounting info\n\n\n\n\n\nGPU Partition Names\nGPU partition names vary by cluster. Common names:\n\ngpu, gpus, gpu-shared\na100, v100, rtx\ngpu-debug (for testing)\n\nCheck your cluster’s documentation or run sinfo to see available partitions.",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#environment-variables",
    "href": "monday/0-hpc-setup.html#environment-variables",
    "title": "0. Common HPC Setup",
    "section": "Environment Variables",
    "text": "Environment Variables\n Mark as complete\nSeveral tools use environment variables. Add these to your ~/.bashrc:\n# Model weight storage (prevents filling home directory)\nexport TORCH_HOME=/scratch/$USER/torch_cache\nexport HF_HOME=/scratch/$USER/huggingface_cache\nexport TRANSFORMERS_CACHE=/scratch/$USER/transformers_cache\n\n# ColabFold databases\nexport COLABFOLD_DOWNLOAD_DIR=/scratch/$USER/colabfold_db\n\n# Chai-1 models\nexport CHAI_DOWNLOADS_DIR=/scratch/$USER/chai_models\n\n# General cache\nexport XDG_CACHE_HOME=/scratch/$USER/.cache\nReplace /scratch/$USER with your cluster’s scratch or work directory path.",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#verifying-gpu-works-with-pytorch",
    "href": "monday/0-hpc-setup.html#verifying-gpu-works-with-pytorch",
    "title": "0. Common HPC Setup",
    "section": "Verifying GPU Works with PyTorch",
    "text": "Verifying GPU Works with PyTorch\n Mark as complete\nAfter setting up an environment with PyTorch, verify GPU access:\nimport torch\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"GPU count: {torch.cuda.device_count()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n    # Quick computation test\n    x = torch.randn(1000, 1000, device='cuda')\n    y = torch.matmul(x, x)\n    print(\"GPU computation test: PASSED\")\nSave as test_gpu.py and run:\npython test_gpu.py\nExpected output (example):\nPyTorch version: 2.1.0\nCUDA available: True\nCUDA version: 12.1\nGPU count: 1\nGPU name: NVIDIA A100-SXM4-80GB\nGPU memory: 84.9 GB\nGPU computation test: PASSED",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#understanding-gpu-memory-requirements",
    "href": "monday/0-hpc-setup.html#understanding-gpu-memory-requirements",
    "title": "0. Common HPC Setup",
    "section": "Understanding GPU Memory Requirements",
    "text": "Understanding GPU Memory Requirements\nDifferent tasks require different GPU memory:\n\n\n\nTask\nTypical GPU Memory\n\n\n\n\nStructure prediction (small protein &lt;200 aa)\n8-16 GB\n\n\nStructure prediction (large protein &gt;500 aa)\n32-80 GB\n\n\nProtein design (RFdiffusion2)\n16-32 GB\n\n\nDocking (DiffDock-PP, PLACER)\n8-16 GB\n\n\nLanguage model inference (ESM3)\n16-40 GB\n\n\nBinder design (BindCraft)\n32-80 GB\n\n\n\nIf you get out-of-memory errors: 1. Request a GPU with more memory 2. Reduce batch size or sequence length 3. Use CPU offloading if available 4. Process sequences in chunks",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/0-hpc-setup.html#troubleshooting-common-issues",
    "href": "monday/0-hpc-setup.html#troubleshooting-common-issues",
    "title": "0. Common HPC Setup",
    "section": "Troubleshooting Common Issues",
    "text": "Troubleshooting Common Issues\n\n“CUDA out of memory”\n\nRequest more GPU memory\nReduce batch size\nUse gradient checkpointing if training\n\n\n\n“No CUDA runtime found”\nmodule load cuda/12.1  # Load CUDA module\nnvcc --version         # Verify it loaded\n\n\n“Singularity: command not found”\nmodule load apptainer  # or: module load singularity\n\n\nConda environment activation fails in SLURM\nAdd to your job script:\n# Source conda.sh directly (adjust path to your installation)\nsource /path/to/miniforge3/etc/profile.d/conda.sh\nmamba activate my_env\n\n\nPermission denied on container\nchmod +x container.sif",
    "crumbs": [
      "Monday",
      "0. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html",
    "href": "monday/3-rfdiffusion2.html",
    "title": "3. RFdiffusion2",
    "section": "",
    "text": "RFdiffusion2 (paper, code) is a protein design model capable of atom-level active site scaffolding. It extends the original RFdiffusion to enable precise control over protein-ligand interactions at the atomic level.",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#why-use-rfdiffusion2",
    "href": "monday/3-rfdiffusion2.html#why-use-rfdiffusion2",
    "title": "3. RFdiffusion2",
    "section": "Why Use RFdiffusion2?",
    "text": "Why Use RFdiffusion2?\n\nAtomic-level control: Design proteins with precise active site geometries\nLigand scaffolding: Build proteins around small molecules with atomic accuracy\nMotif grafting: Incorporate functional motifs into new scaffolds\nFlexible backbone design: Generate novel folds with specific functional constraints\n\nRelated Tools: Use with LigandMPNN for sequence design after backbone generation. For the earlier version without atomic control, see RFdiffusion All Atom (Optional).",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#resource-requirements",
    "href": "monday/3-rfdiffusion2.html#resource-requirements",
    "title": "3. RFdiffusion2",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\n\n\n\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n32+ GB\nA100 for larger designs\n\n\nCPU RAM\n16 GB\n32 GB\nContainer-based execution\n\n\nDisk Space\n10 GB\n20 GB\nContainer + weights\n\n\nContainer\nApptainer/Singularity\nRequired\nNo native Docker on HPC",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#preparation",
    "href": "monday/3-rfdiffusion2.html#preparation",
    "title": "3. RFdiffusion2",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nApptainer/Singularity available on your cluster\nCUDA-capable GPU\n\nVerify your environment:\nmodule load apptainer    # or: module load singularity\napptainer --version\nnvidia-smi\nImportant: RFdiffusion2 uses containers. Most academic HPCs do NOT support Docker for security reasons - use Apptainer/Singularity instead.",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#installation",
    "href": "monday/3-rfdiffusion2.html#installation",
    "title": "3. RFdiffusion2",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the repository:\n\ngit clone https://github.com/RosettaCommons/RFdiffusion2.git\ncd RFdiffusion2\n\nAdd the repo to your PYTHONPATH (add to ~/.bashrc):\n\nexport PYTHONPATH=\"/path/to/your/RFdiffusion2:$PYTHONPATH\"\n\nDownload the model weights and container:\n\npython setup.py\nExpected download: ~5-10 GB (container + weights). This can take 30+ minutes.\nIf download is interrupted:\npython setup.py overwrite\n\nVerify Apptainer/Singularity is available:\n\nmodule load apptainer\n# or: module load singularity\nThe downloaded .sif file in rf_diffusion/exec/ is the Singularity container.",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#testing-the-installation",
    "href": "monday/3-rfdiffusion2.html#testing-the-installation",
    "title": "3. RFdiffusion2",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a demo case:\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=open_source_demo \\\n    sweep.benchmarks=active_site_unindexed_atomic_partial_ligand\nNote: Omit --nv flag if running without GPU (will be very slow).\nSuccess indicators:\n\nCommand completes without errors\nOutput directory created at pipeline_outputs/&lt;timestamp&gt;_open_source_demo/\nContains PDB files with designed structures\n\nExpected runtime: 5-15 minutes on GPU, 30+ minutes on CPU.",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#hpc-job-script",
    "href": "monday/3-rfdiffusion2.html#hpc-job-script",
    "title": "3. RFdiffusion2",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=rfdiff2\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load apptainer\nmodule load cuda/12.1\n\ncd /path/to/RFdiffusion2\n\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=open_source_demo",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#usage-examples",
    "href": "monday/3-rfdiffusion2.html#usage-examples",
    "title": "3. RFdiffusion2",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic backbone design:\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=my_config\nWith custom output directory:\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=open_source_demo \\\n    sweep.output_dir=/path/to/output\nMultiple design benchmarks:\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=open_source_demo \\\n    sweep.benchmarks=\"[benchmark1,benchmark2]\"",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#docker-to-apptainer-translation",
    "href": "monday/3-rfdiffusion2.html#docker-to-apptainer-translation",
    "title": "3. RFdiffusion2",
    "section": "Docker to Apptainer Translation",
    "text": "Docker to Apptainer Translation\nThe official documentation may show Docker commands. Here’s how to translate:\n\n\n\nDocker Command\nApptainer Equivalent\n\n\n\n\ndocker run --gpus all image\napptainer exec --nv image.sif\n\n\ndocker run -v /path:/path\napptainer exec --bind /path:/path\n\n\n-it (interactive)\napptainer shell --nv\n\n\n\nExample conversion:\n# Docker (won't work on HPC):\ndocker run --gpus all -v $(pwd):/workspace rfdiffusion:latest python script.py\n\n# Apptainer (works on HPC):\napptainer exec --nv --bind $(pwd):/workspace rfdiffusion.sif python script.py",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#understanding-the-output",
    "href": "monday/3-rfdiffusion2.html#understanding-the-output",
    "title": "3. RFdiffusion2",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput structure:\npipeline_outputs/\n└── &lt;timestamp&gt;_&lt;config_name&gt;/\n    ├── designs/\n    │   ├── design_0.pdb    # Designed backbone\n    │   ├── design_1.pdb\n    │   └── ...\n    ├── logs/\n    │   └── run.log         # Execution log\n    └── config.yaml         # Configuration used",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#configuration-system",
    "href": "monday/3-rfdiffusion2.html#configuration-system",
    "title": "3. RFdiffusion2",
    "section": "Configuration System",
    "text": "Configuration System\nRFdiffusion2 uses Hydra for configuration. Key config options:\n\n\n\nParameter\nDescription\n\n\n\n\nsweep.benchmarks\nWhich design task(s) to run\n\n\nsweep.output_dir\nOutput directory\n\n\ndiffuser.T\nNumber of diffusion timesteps\n\n\ninference.num_designs\nNumber of designs to generate",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/3-rfdiffusion2.html#troubleshooting",
    "href": "monday/3-rfdiffusion2.html#troubleshooting",
    "title": "3. RFdiffusion2",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n“No GPU available” / extremely slow:\n\nEnsure --nv flag is included\nVerify GPU allocation: nvidia-smi\nLoad CUDA module: module load cuda/12.1\n\nContainer permission errors:\nchmod +x rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif\n“FileNotFoundError” for weights:\n\nRe-run python setup.py to ensure all files downloaded\nCheck rf_diffusion/weights/ directory exists\n\nContainer not found:\n\nProvide full path to .sif file\nOr run from the RFdiffusion2 directory\n\nSetup script hangs during download:\n\nLarge files may take 30+ minutes\nCheck network connectivity\nIf interrupted, run python setup.py overwrite\n\nModule not found errors inside container:\n\nEnsure PYTHONPATH is set correctly\nContainer may need --bind for additional paths",
    "crumbs": [
      "Monday",
      "3. RFdiffusion2"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This bootcamp covers machine learning tools for protein structure prediction and design.\nMore information coming soon."
  },
  {
    "objectID": "capstone/targets/beta-glucosidase.html",
    "href": "capstone/targets/beta-glucosidase.html",
    "title": "Beta-Glucosidase Deep Dive",
    "section": "",
    "text": "Beta-Glucosidase is an enzyme that breaks down complex sugars (glucosides) into glucose. It is relevant in both human metabolism (Gaucher’s disease) and industrial biofuel production.\nWhy it matters: In industrial applications, thermostable or stabilized versions of this enzyme are highly valuable. In health, chaperones (binders) that help fold unstable mutants can be therapeutic.\nThe Goal: Design a binder that stabilizes the enzyme without blocking its active site, or conversely, an inhibitor that blocks it.",
    "crumbs": [
      "Capstone",
      "Beta-Glucosidase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/beta-glucosidase.html#biological-context",
    "href": "capstone/targets/beta-glucosidase.html#biological-context",
    "title": "Beta-Glucosidase Deep Dive",
    "section": "",
    "text": "Beta-Glucosidase is an enzyme that breaks down complex sugars (glucosides) into glucose. It is relevant in both human metabolism (Gaucher’s disease) and industrial biofuel production.\nWhy it matters: In industrial applications, thermostable or stabilized versions of this enzyme are highly valuable. In health, chaperones (binders) that help fold unstable mutants can be therapeutic.\nThe Goal: Design a binder that stabilizes the enzyme without blocking its active site, or conversely, an inhibitor that blocks it.",
    "crumbs": [
      "Capstone",
      "Beta-Glucosidase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/beta-glucosidase.html#interactive-structure",
    "href": "capstone/targets/beta-glucosidase.html#interactive-structure",
    "title": "Beta-Glucosidase Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nThe viewer below shows Beta-Glucosidase A.\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "Beta-Glucosidase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/beta-glucosidase.html#design-mission",
    "href": "capstone/targets/beta-glucosidase.html#design-mission",
    "title": "Beta-Glucosidase Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nTarget a surface patch on Beta-Glucosidase.\n\nTarget Specifications\n\n\n\n\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nBeta-Glucosidase\n\n\nPDB ID\n2JIE\n\n\nTarget Chain\nChain A\n\n\nInterface / Hotspot\nSurface patches away from the active site (for stabilization) or the active site (for inhibition).\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 2JIE.\nDefine your Goal: Decide if you want to inhibit or stabilize.\nSelect Hotspot: Choose residues accordingly.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "Beta-Glucosidase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/ifnar2.html",
    "href": "capstone/targets/ifnar2.html",
    "title": "IFNAR2 Target Deep Dive",
    "section": "",
    "text": "Interferon Alpha/Beta Receptor 2 (IFNAR2) is one of the two receptor subunits that recognize Type I interferons (like IFN-α and IFN-β). These cytokines are powerful antivirals and immune regulators.\nWhy it matters: Modulating this pathway is crucial for treating viral infections (activating it) or autoimmune diseases like Lupus (blocking it).\nThe Goal: Design a protein that binds to the extracellular domain of IFNAR2. Depending on the design goal, this could either compete with natural Interferon (blocking the signal) or potentially act as a synthetic agonist.",
    "crumbs": [
      "Capstone",
      "IFNAR2 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/ifnar2.html#biological-context",
    "href": "capstone/targets/ifnar2.html#biological-context",
    "title": "IFNAR2 Target Deep Dive",
    "section": "",
    "text": "Interferon Alpha/Beta Receptor 2 (IFNAR2) is one of the two receptor subunits that recognize Type I interferons (like IFN-α and IFN-β). These cytokines are powerful antivirals and immune regulators.\nWhy it matters: Modulating this pathway is crucial for treating viral infections (activating it) or autoimmune diseases like Lupus (blocking it).\nThe Goal: Design a protein that binds to the extracellular domain of IFNAR2. Depending on the design goal, this could either compete with natural Interferon (blocking the signal) or potentially act as a synthetic agonist.",
    "crumbs": [
      "Capstone",
      "IFNAR2 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/ifnar2.html#interactive-structure",
    "href": "capstone/targets/ifnar2.html#interactive-structure",
    "title": "IFNAR2 Target Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nThe viewer below shows IFNAR2 (Chain B) in complex with Interferon alpha-2 (Chain A) and IFNAR1 (Chain C).\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "IFNAR2 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/ifnar2.html#design-mission",
    "href": "capstone/targets/ifnar2.html#design-mission",
    "title": "IFNAR2 Target Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nCreate a binder that targets the Interferon-binding interface of IFNAR2.\n\nTarget Specifications\n\n\n\n\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nIFNAR2 (Interferon alpha/beta receptor 2)\n\n\nPDB ID\n3SE3\n\n\nTarget Chain\nChain B (IFNAR2)\n\n\nBinder to Mimic\nChain A (IFN-alpha2)\n\n\nInterface / Hotspot\nResidues interacting with Chain A (approx 40-50, 70-80 regions)\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 3SE3.\nClean the structure: Isolate Chain B (IFNAR2) as your target.\nAnalyze the Interface: Load the full complex in PyMOL. Select Chain B residues that are within 5Å of Chain A to define your hotspot for BindCraft or RFdiffusion.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "IFNAR2 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/bet-v-1.html",
    "href": "capstone/targets/bet-v-1.html",
    "title": "Bet v 1 Target Deep Dive",
    "section": "",
    "text": "Bet v 1 is the major allergen found in birch pollen. It is responsible for seasonal allergies in millions of people worldwide.\nWhy it matters: The allergic reaction is caused when the patient’s antibodies (IgE) recognize and bind to epitopes on the surface of Bet v 1.\nThe Goal: Design a “masking” protein (similar to a neutralizing antibody) that binds tightly to the surface of Bet v 1. If your designed binder covers the IgE binding sites, it could theoretically prevent the allergic reaction.",
    "crumbs": [
      "Capstone",
      "Bet v 1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/bet-v-1.html#biological-context",
    "href": "capstone/targets/bet-v-1.html#biological-context",
    "title": "Bet v 1 Target Deep Dive",
    "section": "",
    "text": "Bet v 1 is the major allergen found in birch pollen. It is responsible for seasonal allergies in millions of people worldwide.\nWhy it matters: The allergic reaction is caused when the patient’s antibodies (IgE) recognize and bind to epitopes on the surface of Bet v 1.\nThe Goal: Design a “masking” protein (similar to a neutralizing antibody) that binds tightly to the surface of Bet v 1. If your designed binder covers the IgE binding sites, it could theoretically prevent the allergic reaction.",
    "crumbs": [
      "Capstone",
      "Bet v 1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/bet-v-1.html#interactive-structure",
    "href": "capstone/targets/bet-v-1.html#interactive-structure",
    "title": "Bet v 1 Target Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nThe viewer below shows the structure of Bet v 1.\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "Bet v 1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/bet-v-1.html#design-mission",
    "href": "capstone/targets/bet-v-1.html#design-mission",
    "title": "Bet v 1 Target Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nCreate a protein that binds to a solvent-exposed patch on Bet v 1. Unlike the other targets, you aren’t necessarily mimicking a natural partner—you are creating a blocker.\n\nTarget Specifications\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nBet v 1\n\n\nPDB ID\n4A88\n\n\nTarget Chain\nChain A\n\n\nInterface / Hotspot\nChoose a large exposed surface patch.\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 4A88.\nSurface Mapping: Since there is no single “natural ligand” to mimic, you have more freedom. Pick a surface patch that looks accessible (convex or flat surfaces are easier than deep pockets for protein-protein interactions).\nPyligner/BindCraft: These tools are excellent for identifying bindable patches on an antigen.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "Bet v 1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/il-7r.html",
    "href": "capstone/targets/il-7r.html",
    "title": "IL-7R Target Deep Dive",
    "section": "",
    "text": "Interleukin-7 Receptor alpha (IL-7R) is crucial for the development and survival of T-cells.\nWhy it matters: Dysregulation of IL-7 signaling is implicated in T-cell acute lymphoblastic leukemia (T-ALL) and various autoimmune conditions like Multiple Sclerosis.\nThe Goal: Design a binder that blocks the interaction between IL-7 and IL-7R. A high-affinity binder could serve as a therapeutic antagonist to dampen the immune response in autoimmune settings.",
    "crumbs": [
      "Capstone",
      "IL-7R Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/il-7r.html#biological-context",
    "href": "capstone/targets/il-7r.html#biological-context",
    "title": "IL-7R Target Deep Dive",
    "section": "",
    "text": "Interleukin-7 Receptor alpha (IL-7R) is crucial for the development and survival of T-cells.\nWhy it matters: Dysregulation of IL-7 signaling is implicated in T-cell acute lymphoblastic leukemia (T-ALL) and various autoimmune conditions like Multiple Sclerosis.\nThe Goal: Design a binder that blocks the interaction between IL-7 and IL-7R. A high-affinity binder could serve as a therapeutic antagonist to dampen the immune response in autoimmune settings.",
    "crumbs": [
      "Capstone",
      "IL-7R Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/il-7r.html#interactive-structure",
    "href": "capstone/targets/il-7r.html#interactive-structure",
    "title": "IL-7R Target Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nThe viewer below shows IL-7R alpha (Chain B) bound to its cytokine IL-7 (Chain A).\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "IL-7R Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/il-7r.html#design-mission",
    "href": "capstone/targets/il-7r.html#design-mission",
    "title": "IL-7R Target Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nCreate a binder that occupies the ligand-binding groove of IL-7R, preventing native IL-7 from binding.\n\nTarget Specifications\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nIL-7R alpha\n\n\nPDB ID\n3DI2\n\n\nTarget Chain\nChain B\n\n\nBinder to Mimic\nChain A (IL-7)\n\n\nInterface / Hotspot\nThe groove where Chain A sits.\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 3DI2.\nClean the structure: Keep Chain B.\nHotspot Definition: Use the residues on Chain B that contact Chain A to define the binding site.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "IL-7R Target Deep Dive"
    ]
  },
  {
    "objectID": "thursday/index.html",
    "href": "thursday/index.html",
    "title": "Thursday",
    "section": "",
    "text": "Thursday content coming soon.",
    "crumbs": [
      "Thursday"
    ]
  },
  {
    "objectID": "thursday/index.html#overview",
    "href": "thursday/index.html#overview",
    "title": "Thursday",
    "section": "",
    "text": "Thursday content coming soon.",
    "crumbs": [
      "Thursday"
    ]
  },
  {
    "objectID": "thursday/index.html#modules",
    "href": "thursday/index.html#modules",
    "title": "Thursday",
    "section": "Modules",
    "text": "Modules\n\n\n\nModule\nTopic\nStatus\n\n\n\n\n4.1\nPlaceholder Module\nComing soon\n\n\n\nMore modules will be added here.\n\n\n\n← Wednesday\n\n\nBack to Home\n\n\nCapstone →",
    "crumbs": [
      "Thursday"
    ]
  }
]