[
  {
    "objectID": "thursday/4.1-placeholder.html",
    "href": "thursday/4.1-placeholder.html",
    "title": "4.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon.",
    "crumbs": [
      "Thursday",
      "4.1 Placeholder Module"
    ]
  },
  {
    "objectID": "thursday/4.1-placeholder.html#learning-objectives",
    "href": "thursday/4.1-placeholder.html#learning-objectives",
    "title": "4.1 Placeholder Module",
    "section": "",
    "text": "Content coming soon.",
    "crumbs": [
      "Thursday",
      "4.1 Placeholder Module"
    ]
  },
  {
    "objectID": "thursday/4.1-placeholder.html#section-1",
    "href": "thursday/4.1-placeholder.html#section-1",
    "title": "4.1 Placeholder Module",
    "section": "Section 1",
    "text": "Section 1\n Mark Section 1 as complete\nPlaceholder content for Section 1.",
    "crumbs": [
      "Thursday",
      "4.1 Placeholder Module"
    ]
  },
  {
    "objectID": "thursday/4.1-placeholder.html#section-2",
    "href": "thursday/4.1-placeholder.html#section-2",
    "title": "4.1 Placeholder Module",
    "section": "Section 2",
    "text": "Section 2\n Mark Section 2 as complete\nPlaceholder content for Section 2.",
    "crumbs": [
      "Thursday",
      "4.1 Placeholder Module"
    ]
  },
  {
    "objectID": "capstone/targets/trka.html",
    "href": "capstone/targets/trka.html",
    "title": "TrkA Receptor Deep Dive",
    "section": "",
    "text": "Tropomyosin receptor kinase A (TrkA) is the high-affinity catalytic receptor for Nerve Growth Factor (NGF).\nWhy it matters: The TrkA pathway is essential for the development and survival of neurons. However, it is also implicated in chronic pain and some cancers.\nThe Goal: Design a binder that interacts with the extracellular domain of TrkA. This could be an antagonist to treat pain (blocking NGF) or a tool to study receptor activation.",
    "crumbs": [
      "Capstone",
      "TrkA Receptor Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/trka.html#biological-context",
    "href": "capstone/targets/trka.html#biological-context",
    "title": "TrkA Receptor Deep Dive",
    "section": "",
    "text": "Tropomyosin receptor kinase A (TrkA) is the high-affinity catalytic receptor for Nerve Growth Factor (NGF).\nWhy it matters: The TrkA pathway is essential for the development and survival of neurons. However, it is also implicated in chronic pain and some cancers.\nThe Goal: Design a binder that interacts with the extracellular domain of TrkA. This could be an antagonist to treat pain (blocking NGF) or a tool to study receptor activation.",
    "crumbs": [
      "Capstone",
      "TrkA Receptor Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/trka.html#interactive-structure",
    "href": "capstone/targets/trka.html#interactive-structure",
    "title": "TrkA Receptor Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nThe viewer below shows the extracellular domain of TrkA (Chain A) in complex with NGF (Chain B).\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "TrkA Receptor Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/trka.html#design-mission",
    "href": "capstone/targets/trka.html#design-mission",
    "title": "TrkA Receptor Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nDesign a protein that binds to the NGF-binding face of TrkA.\n\nTarget Specifications\n\n\n\n\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nTrkA Receptor\n\n\nPDB ID\n1WWC\n\n\nTarget Chain\nChain A (TrkA)\n\n\nBinder to Mimic\nChain B (NGF)\n\n\nInterface / Hotspot\nThe residues on Chain A that contact Chain B.\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 1WWC.\nClean the structure: Isolate Chain A.\nInterface Definition: Use PyMOL to find residues on Chain A within 5Å of Chain B.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "TrkA Receptor Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/gm3.html",
    "href": "capstone/targets/gm3.html",
    "title": "GM2 Activator Protein Deep Dive",
    "section": "",
    "text": "GM2 Activator Protein (GM2AP) is a small glycoprotein required for the degradation of gangliosides (types of lipids) in the lysosome. You may also see it referenced in the context of GM3 pathway interactions.\nWhy it matters: Defects in this protein cause GM2 gangliosidosis (AB variant), a severe neurodegenerative storage disease.\nThe Goal: Design a binder that can stabilize this protein or mimic its interaction partners.",
    "crumbs": [
      "Capstone",
      "GM2 Activator Protein Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/gm3.html#biological-context",
    "href": "capstone/targets/gm3.html#biological-context",
    "title": "GM2 Activator Protein Deep Dive",
    "section": "",
    "text": "GM2 Activator Protein (GM2AP) is a small glycoprotein required for the degradation of gangliosides (types of lipids) in the lysosome. You may also see it referenced in the context of GM3 pathway interactions.\nWhy it matters: Defects in this protein cause GM2 gangliosidosis (AB variant), a severe neurodegenerative storage disease.\nThe Goal: Design a binder that can stabilize this protein or mimic its interaction partners.",
    "crumbs": [
      "Capstone",
      "GM2 Activator Protein Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/gm3.html#interactive-structure",
    "href": "capstone/targets/gm3.html#interactive-structure",
    "title": "GM2 Activator Protein Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nThe viewer below shows the GM2 Activator Protein.\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "GM2 Activator Protein Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/gm3.html#design-mission",
    "href": "capstone/targets/gm3.html#design-mission",
    "title": "GM2 Activator Protein Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nCreate a binder that interacts with the lipid-binding cavity or surface loops of GM2AP.\n\nTarget Specifications\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nGM2 Activator Protein\n\n\nPDB ID\n1G13\n\n\nTarget Chain\nChain A\n\n\nInterface / Hotspot\nThe hydrophobic cavity or surface loops.\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 1G13.\nSurface Analysis: Identify the opening of the lipid-binding cavity. This is a challenging but interesting target for “pocket” binding.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "GM2 Activator Protein Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/tem-1.html",
    "href": "capstone/targets/tem-1.html",
    "title": "TEM-1 Beta-Lactamase Deep Dive",
    "section": "",
    "text": "TEM-1 Beta-Lactamase is an enzyme produced by bacteria that confers resistance to penicillin and other beta-lactam antibiotics. It works by breaking the antibiotic molecule apart.\nWhy it matters: This is a classic target for “Binder Design” as a means of inhibition. If you can design a protein that binds tightly to the active site or an allosteric site, you might stop the enzyme from working, making the bacteria susceptible to antibiotics again.\nThe Goal: Design a binder that inhibits TEM-1 function. This is often done by targeting the active site groove.",
    "crumbs": [
      "Capstone",
      "TEM-1 Beta-Lactamase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/tem-1.html#biological-context",
    "href": "capstone/targets/tem-1.html#biological-context",
    "title": "TEM-1 Beta-Lactamase Deep Dive",
    "section": "",
    "text": "TEM-1 Beta-Lactamase is an enzyme produced by bacteria that confers resistance to penicillin and other beta-lactam antibiotics. It works by breaking the antibiotic molecule apart.\nWhy it matters: This is a classic target for “Binder Design” as a means of inhibition. If you can design a protein that binds tightly to the active site or an allosteric site, you might stop the enzyme from working, making the bacteria susceptible to antibiotics again.\nThe Goal: Design a binder that inhibits TEM-1 function. This is often done by targeting the active site groove.",
    "crumbs": [
      "Capstone",
      "TEM-1 Beta-Lactamase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/tem-1.html#interactive-structure",
    "href": "capstone/targets/tem-1.html#interactive-structure",
    "title": "TEM-1 Beta-Lactamase Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nThe viewer below shows TEM-1 Beta-Lactamase.\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "TEM-1 Beta-Lactamase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/tem-1.html#design-mission",
    "href": "capstone/targets/tem-1.html#design-mission",
    "title": "TEM-1 Beta-Lactamase Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nCreate a binder that targets the active site cleft of TEM-1.\n\nTarget Specifications\n\n\n\n\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nTEM-1 Beta-Lactamase\n\n\nPDB ID\n1FQG\n\n\nTarget Chain\nChain A\n\n\nInterface / Hotspot\nThe active site pocket (look for where inhibitors typically bind).\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 1FQG.\nIdentify the Pocket: Use tools like PyMOL to locate the concave active site.\nTargeting: When using RFdiffusion, you may want to specify “binder”contigs that fill this specific pocket.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "TEM-1 Beta-Lactamase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/pd-l1.html",
    "href": "capstone/targets/pd-l1.html",
    "title": "PD-L1 Target Deep Dive",
    "section": "",
    "text": "Programmed Death-Ligand 1 (PD-L1) is a critical protein expressed on the surface of many cells, including cancer cells. Under normal conditions, it acts as a “brake” on the immune system. When PD-L1 binds to the PD-1 receptor on T-cells, it signals the T-cell to become inactive.\nWhy it matters: Cancer cells often hijack this mechanism by over-expressing PD-L1. This allows them to “trick” the immune system into ignoring the tumor.\nThe Goal: By designing a protein binder that sticks tightly to PD-L1, we can block it from interacting with PD-1. This releases the “brake,” allowing T-cells to recognize and attack the cancer.",
    "crumbs": [
      "Capstone",
      "PD-L1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/pd-l1.html#biological-context",
    "href": "capstone/targets/pd-l1.html#biological-context",
    "title": "PD-L1 Target Deep Dive",
    "section": "",
    "text": "Programmed Death-Ligand 1 (PD-L1) is a critical protein expressed on the surface of many cells, including cancer cells. Under normal conditions, it acts as a “brake” on the immune system. When PD-L1 binds to the PD-1 receptor on T-cells, it signals the T-cell to become inactive.\nWhy it matters: Cancer cells often hijack this mechanism by over-expressing PD-L1. This allows them to “trick” the immune system into ignoring the tumor.\nThe Goal: By designing a protein binder that sticks tightly to PD-L1, we can block it from interacting with PD-1. This releases the “brake,” allowing T-cells to recognize and attack the cancer.",
    "crumbs": [
      "Capstone",
      "PD-L1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/pd-l1.html#interactive-structure",
    "href": "capstone/targets/pd-l1.html#interactive-structure",
    "title": "PD-L1 Target Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nExplore the native interaction between PD-L1 (the target) and PD-1 (the natural binder) below. * Target (PD-L1): The surface we want to bind to. * Binder (PD-1): The natural partner we want to compete with.\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "PD-L1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/pd-l1.html#design-mission",
    "href": "capstone/targets/pd-l1.html#design-mission",
    "title": "PD-L1 Target Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nYour objective is to create a de novo protein binder that binds to the same interface on PD-L1 that PD-1 currently occupies.\n\nTarget Specifications\n\n\n\n\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nPD-L1 (Programmed Death-Ligand 1)\n\n\nPDB ID\n4ZQK\n\n\nTarget Chain\nChain B\n\n\nInterface / Hotspot\nResidues 54-66 and 113-123 (approximate binding loop area)\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 4ZQK.\nClean the structure: Keep Chain B (PD-L1) as the target. Remove Chain A (PD-1) and any water molecules.\nDefine the Interface: When running tools like RFdiffusion or BindCraft, specify the hotspot residues listed above to guide the diffusion process to the correct face of the protein.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "PD-L1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/index.html",
    "href": "capstone/index.html",
    "title": "Capstone Project",
    "section": "",
    "text": "Binder design is often formulated as “given a target protein (and optionally a specific epitope on that protein), design a (often smaller) protein capable of binding the target”. It is one of the most ubiquitous protein design problems and has many potential applications, largely functioning by blocking or competing with natural interactions involving the target protein.\nIn this bootcamp, you’ll have the opportunity to explore the computational binder design problem across three different categories of targets. Students will choose from a curated list of targets that have been validated with BindCraft and other computational binder design tools.",
    "crumbs": [
      "Capstone",
      "Capstone Project"
    ]
  },
  {
    "objectID": "capstone/index.html#overview",
    "href": "capstone/index.html#overview",
    "title": "Capstone Project",
    "section": "",
    "text": "Binder design is often formulated as “given a target protein (and optionally a specific epitope on that protein), design a (often smaller) protein capable of binding the target”. It is one of the most ubiquitous protein design problems and has many potential applications, largely functioning by blocking or competing with natural interactions involving the target protein.\nIn this bootcamp, you’ll have the opportunity to explore the computational binder design problem across three different categories of targets. Students will choose from a curated list of targets that have been validated with BindCraft and other computational binder design tools.",
    "crumbs": [
      "Capstone",
      "Capstone Project"
    ]
  },
  {
    "objectID": "capstone/index.html#target-categories-and-options",
    "href": "capstone/index.html#target-categories-and-options",
    "title": "Capstone Project",
    "section": "Target Categories and Options",
    "text": "Target Categories and Options\n\nImmune Checkpoint / Receptor Targets\n\n\n\nTarget\nUniProt\nDescription\n\n\n\n\nPD-L1\nQ9NZQ7\nImmune Checkpoint\n\n\nIFNAR2\nP48551\nInterferon receptor\n\n\nIL-7Rα\nP16871\nInterleukin receptor\n\n\n\n\n\nAntibody-like Targets\n\n\n\nTarget\nUniProt\nDescription\n\n\n\n\nBet v 1\nP15494\nBirch pollen allergen\n\n\n\n\n\nEnzyme / Small Molecule Binders\n\n\n\nTarget\nPDB\nDescription\n\n\n\n\nTrkA receptor\n1WWC\nNerve Growth Factor Receptor\n\n\nTEM-1 Beta-Lactamase\n1FQG\nAntibiotic Resistance Enzyme\n\n\nGM2 Activator Protein\n1G13\nLipid Transport Protein\n\n\nBeta-Glucosidase\n2JIE\nEnzymatic Catalyst",
    "crumbs": [
      "Capstone",
      "Capstone Project"
    ]
  },
  {
    "objectID": "capstone/index.html#project-structure",
    "href": "capstone/index.html#project-structure",
    "title": "Capstone Project",
    "section": "Project Structure",
    "text": "Project Structure\nSelect a target from the list above and work through the binder design process. As you learn about each tool in the preceding lessons, explore how that tool works for your particular protein. Be sure to make note of any quirks or nuances related to your protein, e.g. any particular settings used or any issues encountered.\n\n\n\n\n\n\nTipDocumentation Tip\n\n\n\nKeep a lab notebook (digital or physical) to document your predictions, results, settings tried, and commands used. This will be invaluable when synthesizing your findings at the end of the project.",
    "crumbs": [
      "Capstone",
      "Capstone Project"
    ]
  },
  {
    "objectID": "capstone/index.html#goals",
    "href": "capstone/index.html#goals",
    "title": "Capstone Project",
    "section": "Goals",
    "text": "Goals\n\nGain hands-on experience running DL-based tools on specific proteins\nTroubleshoot and address any issues related to your target protein\nLearn about tips and tricks used to get more favorable outputs\nGain experience communicating about computational tools and settings\nGain experience interpreting results and exploring various settings",
    "crumbs": [
      "Capstone",
      "Capstone Project"
    ]
  },
  {
    "objectID": "capstone/index.html#synthesizing-your-work",
    "href": "capstone/index.html#synthesizing-your-work",
    "title": "Capstone Project",
    "section": "Synthesizing Your Work",
    "text": "Synthesizing Your Work\nA key part of learning is synthesizing and communicating what you’ve done. In the live workshop, students gave 20-minute group presentations. For self-paced learners, consider one of these formats to consolidate your learning:\n\nWritten report summarizing your process and findings\nVideo walkthrough explaining your approach and results\nJupyter notebook with narrative text alongside code and figures\nGitHub repository with a comprehensive README documenting your project\n\n\n\n\n\n\n\nTip\n\n\n\nEven if you’re not presenting to anyone, creating slides is a great way to learn! The process of organizing your work into a clear narrative helps solidify your understanding.\n\n\nHowever you choose to document your work, aim to cover the following:\n\nBackground and introduction to target protein\nStructure prediction results\nBackbone generation results\nSequence design results\nDesign rationale - explain the specific choices, settings, and approaches used and why\n\n\n\n\n\n\n\nNote\n\n\n\nI find the design rationale section to be the most helpful to my learning and understanding.\n\n\n\nResults from miscellaneous tools\nSummary and takeaways\n\n\n\n← Thursday\nBack to Home",
    "crumbs": [
      "Capstone",
      "Capstone Project"
    ]
  },
  {
    "objectID": "monday/13-rfdiffusion-aa.html",
    "href": "monday/13-rfdiffusion-aa.html",
    "title": "13. RFdiffusion All Atom (Optional)",
    "section": "",
    "text": "RFdiffusion All Atom (code) is the predecessor to RFdiffusion2, enabling all-atom protein design with small molecule binding. It can design protein binders to ligands with all-atom precision.\nNote: This tool is marked as OPTIONAL because RFdiffusion2 is the newer, more capable version. Install this only if you need the earlier methodology or specific features.",
    "crumbs": [
      "Monday",
      "13. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/13-rfdiffusion-aa.html#why-use-rfdiffusion-all-atom",
    "href": "monday/13-rfdiffusion-aa.html#why-use-rfdiffusion-all-atom",
    "title": "13. RFdiffusion All Atom (Optional)",
    "section": "Why Use RFdiffusion All Atom?",
    "text": "Why Use RFdiffusion All Atom?\n\nSmall molecule binding: Design proteins that bind specific ligands\nMotif incorporation: Include functional motifs in designs\nHistorical reference: Understand the evolution of diffusion-based design\nSpecific workflows: Some published protocols may reference this version\n\nRelated Tools: For the newer version, see RFdiffusion2. For sequence design, see LigandMPNN.",
    "crumbs": [
      "Monday",
      "13. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/13-rfdiffusion-aa.html#resource-requirements",
    "href": "monday/13-rfdiffusion-aa.html#resource-requirements",
    "title": "13. RFdiffusion All Atom (Optional)",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n8 GB\n16 GB\nScales with design size\n\n\nCPU RAM\n8 GB\n16 GB\nContainer-based\n\n\nDisk Space\n5 GB\n10 GB\nContainer + weights\n\n\nContainer\nApptainer/Singularity\nRequired\nNot Docker",
    "crumbs": [
      "Monday",
      "13. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/13-rfdiffusion-aa.html#preparation",
    "href": "monday/13-rfdiffusion-aa.html#preparation",
    "title": "13. RFdiffusion All Atom (Optional)",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nApptainer/Singularity available\nGPU access recommended\n\nImportant: Like RFdiffusion2, this uses Apptainer/Singularity containers. Most academic HPCs do NOT support Docker.\nVerify container runtime:\nmodule load apptainer    # or: module load singularity\napptainer --version",
    "crumbs": [
      "Monday",
      "13. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/13-rfdiffusion-aa.html#installation",
    "href": "monday/13-rfdiffusion-aa.html#installation",
    "title": "13. RFdiffusion All Atom (Optional)",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the repository:\n\ngit clone https://github.com/baker-laboratory/rf_diffusion_all_atom.git\ncd rf_diffusion_all_atom\n\nDownload the Singularity container:\n\nwget http://files.ipd.uw.edu/pub/RF-All-Atom/containers/rf_se3_diffusion.sif\nExpected download: ~2-3 GB.\n\nDownload the model weights:\n\nwget http://files.ipd.uw.edu/pub/RF-All-Atom/weights/RFDiffusionAA_paper_weights.pt\nExpected download: ~500 MB.\n\nInitialize git submodules:\n\ngit submodule init\ngit submodule update",
    "crumbs": [
      "Monday",
      "13. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/13-rfdiffusion-aa.html#testing-the-installation",
    "href": "monday/13-rfdiffusion-aa.html#testing-the-installation",
    "title": "13. RFdiffusion All Atom (Optional)",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a ligand binder design example:\napptainer run --nv rf_se3_diffusion.sif -u run_inference.py \\\n    inference.deterministic=True \\\n    diffuser.T=100 \\\n    inference.output_prefix=output/ligand_test/sample \\\n    inference.input_pdb=input/7v11.pdb \\\n    contigmap.contigs=\"['150-150']\" \\\n    inference.ligand=OQO \\\n    inference.num_designs=1 \\\n    inference.design_startnum=0\nNote: Omit --nv flag if running without GPU.\nSuccess indicators:\n\nCommand completes without errors\nOutput files created:\n\noutput/ligand_test/sample_0.pdb - The designed structure\noutput/ligand_test/sample_0_Xt-1_traj.pdb - Denoising trajectory\noutput/ligand_test/sample_0_X0-1_traj.pdb - Predicted ground truth at each step\n\n\nExpected runtime: 5-10 minutes on GPU, 30+ minutes on CPU.",
    "crumbs": [
      "Monday",
      "13. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/13-rfdiffusion-aa.html#hpc-job-script",
    "href": "monday/13-rfdiffusion-aa.html#hpc-job-script",
    "title": "13. RFdiffusion All Atom (Optional)",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=rfdaa\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load apptainer\nmodule load cuda/12.1\n\ncd /path/to/rf_diffusion_all_atom\n\napptainer run --nv rf_se3_diffusion.sif -u run_inference.py \\\n    inference.deterministic=True \\\n    diffuser.T=100 \\\n    inference.output_prefix=output/my_design/sample \\\n    inference.input_pdb=input/my_protein.pdb \\\n    contigmap.contigs=\"['150-150']\" \\\n    inference.ligand=HEM \\\n    inference.num_designs=10",
    "crumbs": [
      "Monday",
      "13. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/13-rfdiffusion-aa.html#usage-examples",
    "href": "monday/13-rfdiffusion-aa.html#usage-examples",
    "title": "13. RFdiffusion All Atom (Optional)",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic ligand binder design:\napptainer run --nv rf_se3_diffusion.sif -u run_inference.py \\\n    inference.input_pdb=input/complex.pdb \\\n    inference.output_prefix=output/design \\\n    inference.ligand=LIG \\\n    contigmap.contigs=\"['100-100']\" \\\n    inference.num_designs=10\nMultiple designs with different lengths:\napptainer run --nv rf_se3_diffusion.sif -u run_inference.py \\\n    inference.input_pdb=input/complex.pdb \\\n    inference.output_prefix=output/design \\\n    inference.ligand=LIG \\\n    contigmap.contigs=\"['80-120']\" \\\n    inference.num_designs=20",
    "crumbs": [
      "Monday",
      "13. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/13-rfdiffusion-aa.html#key-parameters",
    "href": "monday/13-rfdiffusion-aa.html#key-parameters",
    "title": "13. RFdiffusion All Atom (Optional)",
    "section": "Key Parameters",
    "text": "Key Parameters\n\n\n\nParameter\nDescription\n\n\n\n\ninference.input_pdb\nInput PDB with ligand\n\n\ninference.output_prefix\nOutput path prefix\n\n\ninference.ligand\nLigand residue name\n\n\ncontigmap.contigs\nProtein length range (e.g., ['100-100'])\n\n\ninference.num_designs\nNumber of designs\n\n\ndiffuser.T\nDiffusion timesteps (100 typical)\n\n\ninference.deterministic\nReproducible results",
    "crumbs": [
      "Monday",
      "13. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/13-rfdiffusion-aa.html#docker-to-apptainer-translation",
    "href": "monday/13-rfdiffusion-aa.html#docker-to-apptainer-translation",
    "title": "13. RFdiffusion All Atom (Optional)",
    "section": "Docker to Apptainer Translation",
    "text": "Docker to Apptainer Translation\nThe official docs may show Docker commands. Translate as follows:\n\n\n\n\n\n\n\nDocker\nApptainer\n\n\n\n\ndocker run --gpus all\napptainer run --nv\n\n\ndocker run -v $(pwd):/workspace\napptainer run --bind $(pwd):/workspace\n\n\n-it (interactive)\nUse apptainer shell --nv",
    "crumbs": [
      "Monday",
      "13. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/13-rfdiffusion-aa.html#understanding-the-output",
    "href": "monday/13-rfdiffusion-aa.html#understanding-the-output",
    "title": "13. RFdiffusion All Atom (Optional)",
    "section": "Understanding the Output",
    "text": "Understanding the Output\n\n\n\n\n\n\n\nOutput File\nDescription\n\n\n\n\nsample_N.pdb\nFinal designed structure\n\n\nsample_N_Xt-1_traj.pdb\nDenoising trajectory (animation of design)\n\n\nsample_N_X0-1_traj.pdb\nModel predictions at each step\n\n\n\nThe trajectory files can be loaded into PyMOL to visualize the diffusion process.",
    "crumbs": [
      "Monday",
      "13. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/13-rfdiffusion-aa.html#comparison-with-rfdiffusion2",
    "href": "monday/13-rfdiffusion-aa.html#comparison-with-rfdiffusion2",
    "title": "13. RFdiffusion All Atom (Optional)",
    "section": "Comparison with RFdiffusion2",
    "text": "Comparison with RFdiffusion2\n\n\n\nFeature\nRFdiffusion AA\nRFdiffusion2\n\n\n\n\nAtomic precision\nYes\nYes (improved)\n\n\nLigand binding\nYes\nYes\n\n\nActive site scaffolding\nLimited\nAdvanced\n\n\nModel architecture\nEarlier\nUpdated\n\n\nRecommended\nLegacy workflows\nNew projects\n\n\n\nRecommendation: Use RFdiffusion2 for new projects unless you have specific reasons to use this version.",
    "crumbs": [
      "Monday",
      "13. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/13-rfdiffusion-aa.html#troubleshooting",
    "href": "monday/13-rfdiffusion-aa.html#troubleshooting",
    "title": "13. RFdiffusion All Atom (Optional)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nContainer not found:\n\nEnsure .sif file is in current directory\nOr provide full path to container\n\nGPU errors:\n\nEnsure --nv flag is included for GPU\nLoad CUDA module: module load cuda/12.1\nVerify GPU availability: nvidia-smi\n\nPermission denied on container:\nchmod +x rf_se3_diffusion.sif\nInput PDB errors:\n\nVerify ligand is present in input PDB\nCheck ligand residue name matches inference.ligand\nEnsure PDB has proper formatting\n\nSubmodule errors:\ngit submodule init\ngit submodule update",
    "crumbs": [
      "Monday",
      "13. RFdiffusion All Atom (Optional)"
    ]
  },
  {
    "objectID": "monday/7-chai1.html",
    "href": "monday/7-chai1.html",
    "title": "7. Chai-1",
    "section": "",
    "text": "Chai-1 (paper, code) is a multi-modal foundation model for molecular structure prediction that achieves state-of-the-art performance across diverse benchmarks. Chai-1 enables unified prediction of proteins, small molecules, DNA, RNA, glycosylations, and more.",
    "crumbs": [
      "Monday",
      "7. Chai-1"
    ]
  },
  {
    "objectID": "monday/7-chai1.html#why-use-chai-1",
    "href": "monday/7-chai1.html#why-use-chai-1",
    "title": "7. Chai-1",
    "section": "Why Use Chai-1?",
    "text": "Why Use Chai-1?\n\nMulti-modal: Predict proteins, nucleic acids, small molecules, and modifications in one model\nState-of-the-art: Top performance on structure prediction benchmarks\nFlexible inputs: Handles complex multi-component assemblies\nExperimental restraints: Can incorporate known distance constraints\n\nRelated Tools: For protein-only predictions, see ESMFold (faster) or LocalColabFold (MSA-based). For binding affinity predictions, see Boltz-2.",
    "crumbs": [
      "Monday",
      "7. Chai-1"
    ]
  },
  {
    "objectID": "monday/7-chai1.html#resource-requirements",
    "href": "monday/7-chai1.html#resource-requirements",
    "title": "7. Chai-1",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n24 GB\n80 GB\nA100 80GB or H100 ideal\n\n\nCPU RAM\n32 GB\n64 GB\nFor preprocessing\n\n\nDisk Space\n10 GB\n20 GB\nModel weights\n\n\nPython\n3.10+\n3.11\nRequired\n\n\n\nGPU Compatibility: Requires bfloat16 support. Compatible GPUs include:\n\nA100, H100, L40S (recommended)\nA10, A30, RTX 4090 (works)\nOlder GPUs may not support bfloat16",
    "crumbs": [
      "Monday",
      "7. Chai-1"
    ]
  },
  {
    "objectID": "monday/7-chai1.html#preparation",
    "href": "monday/7-chai1.html#preparation",
    "title": "7. Chai-1",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nGPU with bfloat16 support\nPython 3.10+\n\nVerify bfloat16 support:\nimport torch\nprint(torch.cuda.is_bf16_supported())  # Should print True",
    "crumbs": [
      "Monday",
      "7. Chai-1"
    ]
  },
  {
    "objectID": "monday/7-chai1.html#installation",
    "href": "monday/7-chai1.html#installation",
    "title": "7. Chai-1",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nCreate a conda environment:\n\nmamba create -n chailab python=3.11\nmamba activate chailab\n\nInstall Chai-1:\n\npip install chai_lab==0.6.1\nExpected download: ~5-10 GB of model weights (downloaded on first run).\nAlternative: Latest development version:\npip install git+https://github.com/chaidiscovery/chai-lab.git",
    "crumbs": [
      "Monday",
      "7. Chai-1"
    ]
  },
  {
    "objectID": "monday/7-chai1.html#testing-the-installation",
    "href": "monday/7-chai1.html#testing-the-installation",
    "title": "7. Chai-1",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test FASTA file test.fasta:\n&gt;protein|name=example\nMKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\nRun prediction:\nchai-lab fold test.fasta output_folder/\nSuccess indicators:\n\nCommand completes without errors\noutput_folder/ contains:\n\npred.model_idx_0.cif - Predicted structure\nscores.model_idx_0.npz - Confidence scores\n\n\nExpected runtime: 2-5 minutes for first run (includes model download), ~30 seconds for subsequent runs.\nNote: By default, this generates 5 sample predictions using embeddings without MSAs.",
    "crumbs": [
      "Monday",
      "7. Chai-1"
    ]
  },
  {
    "objectID": "monday/7-chai1.html#hpc-job-script",
    "href": "monday/7-chai1.html#hpc-job-script",
    "title": "7. Chai-1",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=chai\n#SBATCH --partition=gpu\n#SBATCH --gpus=a100:1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate chailab\n\n# Set custom download directory (avoid filling home)\nexport CHAI_DOWNLOADS_DIR=/scratch/$USER/chai_models\n\n# Run prediction with MSAs\nchai-lab fold --use-msa-server --use-templates-server \\\n    my_complex.fasta \\\n    predictions/",
    "crumbs": [
      "Monday",
      "7. Chai-1"
    ]
  },
  {
    "objectID": "monday/7-chai1.html#usage-examples",
    "href": "monday/7-chai1.html#usage-examples",
    "title": "7. Chai-1",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic prediction (no MSAs, fast):\nchai-lab fold input.fasta output/\nWith MSAs (higher accuracy, uses ColabFold server):\nchai-lab fold --use-msa-server --use-templates-server input.fasta output/\nUsing internal MSA server (if your HPC has one):\nchai-lab fold --use-msa-server \\\n    --msa-server-url \"https://internal.colabserver.edu\" \\\n    input.fasta output/\nGenerate more samples:\nchai-lab fold --num-trunk-recycles 5 --num-diffn-timesteps 200 \\\n    input.fasta output/",
    "crumbs": [
      "Monday",
      "7. Chai-1"
    ]
  },
  {
    "objectID": "monday/7-chai1.html#input-format",
    "href": "monday/7-chai1.html#input-format",
    "title": "7. Chai-1",
    "section": "Input Format",
    "text": "Input Format\nChai-1 uses a modified FASTA format with entity type headers:\nProtein:\n&gt;protein|name=my_protein\nMKTVRQERLKSIVRILERSKEPVSG...\nLigand (SMILES):\n&gt;ligand|name=my_drug\nCC(C)CC1=CC=C(C=C1)C(C)C(=O)O\nDNA:\n&gt;dna|name=promoter\nATGCATGCATGCATGC\nRNA:\n&gt;rna|name=aptamer\nAUGCAUGCAUGCAUGC\nProtein complex (multiple chains):\n&gt;protein|name=chain_A\nMKTVRQERLK...\n&gt;protein|name=chain_B\nMVKLTAEGSE...",
    "crumbs": [
      "Monday",
      "7. Chai-1"
    ]
  },
  {
    "objectID": "monday/7-chai1.html#python-api",
    "href": "monday/7-chai1.html#python-api",
    "title": "7. Chai-1",
    "section": "Python API",
    "text": "Python API\nfrom chai_lab.chai1 import run_inference\n\nresults = run_inference(\n    fasta_file=\"input.fasta\",\n    output_dir=\"output/\",\n    num_trunk_recycles=3,\n    num_diffn_timesteps=200,\n    seed=42\n)\nSee examples/predict_structure.py in the repository for more details.",
    "crumbs": [
      "Monday",
      "7. Chai-1"
    ]
  },
  {
    "objectID": "monday/7-chai1.html#advanced-features",
    "href": "monday/7-chai1.html#advanced-features",
    "title": "7. Chai-1",
    "section": "Advanced Features",
    "text": "Advanced Features\nCustom Templates:\nchai-lab fold --custom-template template.cif input.fasta output/\nExperimental Restraints: Specify inter-chain contacts:\n# See: github.com/chaidiscovery/chai-lab/tree/main/examples/restraints\nCovalent Bonds: Specify covalent modifications:\n# See: github.com/chaidiscovery/chai-lab/tree/main/examples/covalent_bonds",
    "crumbs": [
      "Monday",
      "7. Chai-1"
    ]
  },
  {
    "objectID": "monday/7-chai1.html#understanding-the-output",
    "href": "monday/7-chai1.html#understanding-the-output",
    "title": "7. Chai-1",
    "section": "Understanding the Output",
    "text": "Understanding the Output\n\n\n\nFile\nDescription\n\n\n\n\npred.model_idx_N.cif\nPredicted structure (mmCIF format)\n\n\nscores.model_idx_N.npz\nConfidence scores\n\n\nmsa_*.a3m\nGenerated MSAs (if using MSA server)\n\n\n\nConfidence metrics (in scores file):\n\npLDDT: Per-residue confidence\npTM: Predicted TM-score\npAE: Predicted aligned error\ninterface scores: For multi-chain predictions",
    "crumbs": [
      "Monday",
      "7. Chai-1"
    ]
  },
  {
    "objectID": "monday/7-chai1.html#web-server",
    "href": "monday/7-chai1.html#web-server",
    "title": "7. Chai-1",
    "section": "Web Server",
    "text": "Web Server\nFor quick tests without installation: lab.chaidiscovery.com",
    "crumbs": [
      "Monday",
      "7. Chai-1"
    ]
  },
  {
    "objectID": "monday/7-chai1.html#troubleshooting",
    "href": "monday/7-chai1.html#troubleshooting",
    "title": "7. Chai-1",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n“bfloat16 not supported”:\n\nYour GPU doesn’t support bfloat16\nTry a newer GPU (A100, H100, RTX 4090)\nOlder GPUs (V100, etc.) may not work\n\nOut of memory:\n\nRequest GPU with more memory\nReduce --num-diffn-timesteps\nFor very large complexes, split into smaller units\n\nModel download location:\n# Set before running\nexport CHAI_DOWNLOADS_DIR=/scratch/$USER/chai_models\nMSA server rate limits:\n\nThe public ColabFold MMseqs2 server is a shared resource\nFor batch jobs, space out requests\nConsider setting up a local MSA server for high-throughput\n\nSlow first run:\n\nFirst run downloads ~5-10 GB of model weights\nSubsequent runs are much faster\nSet CHAI_DOWNLOADS_DIR to avoid re-downloading",
    "crumbs": [
      "Monday",
      "7. Chai-1"
    ]
  },
  {
    "objectID": "monday/11-bindcraft.html",
    "href": "monday/11-bindcraft.html",
    "title": "11. BindCraft",
    "section": "",
    "text": "BindCraft (paper, code) is an end-to-end binder design pipeline that combines AlphaFold2 backpropagation, ProteinMPNN, and PyRosetta to design protein binders against target proteins.",
    "crumbs": [
      "Monday",
      "11. BindCraft"
    ]
  },
  {
    "objectID": "monday/11-bindcraft.html#why-use-bindcraft",
    "href": "monday/11-bindcraft.html#why-use-bindcraft",
    "title": "11. BindCraft",
    "section": "Why Use BindCraft?",
    "text": "Why Use BindCraft?\n\nComplete pipeline: Integrates structure prediction, sequence design, and scoring\nAutomated optimization: Multi-stage design with confidence-based filtering\nProduction ready: Validated binders in published work\nLearning resource: See how professional protein design pipelines work\n\nRelated Tools: For backbone design, see RFdiffusion2. For sequence design, see LigandMPNN. For structure prediction, see LocalColabFold.",
    "crumbs": [
      "Monday",
      "11. BindCraft"
    ]
  },
  {
    "objectID": "monday/11-bindcraft.html#resource-requirements",
    "href": "monday/11-bindcraft.html#resource-requirements",
    "title": "11. BindCraft",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n24 GB\n32+ GB\nLarge targets need more\n\n\nCPU RAM\n32 GB\n64 GB\nFor PyRosetta scoring\n\n\nDisk Space\n2 MB + 5.3 GB\n10 GB\nCode + AlphaFold2 weights\n\n\nTime\nHours\nDays\nPer design campaign\n\n\n\nImportant: BindCraft requires PyRosetta. Ensure you have a valid installation or license if required by your institution.",
    "crumbs": [
      "Monday",
      "11. BindCraft"
    ]
  },
  {
    "objectID": "monday/11-bindcraft.html#preparation",
    "href": "monday/11-bindcraft.html#preparation",
    "title": "11. BindCraft",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nPyRosetta installed (or accessible via license)\nCUDA-compatible GPU\n\nCheck your CUDA version:\nnvcc --version\n# Note the version number (e.g., 12.4)",
    "crumbs": [
      "Monday",
      "11. BindCraft"
    ]
  },
  {
    "objectID": "monday/11-bindcraft.html#installation",
    "href": "monday/11-bindcraft.html#installation",
    "title": "11. BindCraft",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the BindCraft repository:\n\ngit clone https://github.com/martinpacesa/BindCraft /path/to/bindcraft\ncd /path/to/bindcraft\n\nRun the installation script:\n\nbash install_bindcraft.sh --cuda '12.4' --pkg_manager 'conda'\nImportant options:\n\nReplace 12.4 with your actual CUDA version\nUse --pkg_manager 'mamba' for faster installation\nIf --cuda is left blank, auto-detection may fail\n\nExpected time: 20-40 minutes.\nThe script creates a conda environment called BindCraft with all dependencies.",
    "crumbs": [
      "Monday",
      "11. BindCraft"
    ]
  },
  {
    "objectID": "monday/11-bindcraft.html#testing-the-installation",
    "href": "monday/11-bindcraft.html#testing-the-installation",
    "title": "11. BindCraft",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\n\nActivate the environment:\n\nconda activate BindCraft\n\nRun a test design against the example target (PDL1):\n\ncd /path/to/bindcraft\npython -u ./bindcraft.py \\\n    --settings './settings_target/PDL1.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'\nSuccess indicators:\n\nStarts generating trajectories without errors\nLog shows design iterations progressing\nCreates output directory with design files\n\nNote: A complete run takes hours to days. For testing, stop after a few trajectories complete (Ctrl+C).",
    "crumbs": [
      "Monday",
      "11. BindCraft"
    ]
  },
  {
    "objectID": "monday/11-bindcraft.html#hpc-job-script",
    "href": "monday/11-bindcraft.html#hpc-job-script",
    "title": "11. BindCraft",
    "section": "HPC Job Script",
    "text": "HPC Job Script\nUsing the provided template:\nsbatch ./bindcraft.slurm \\\n    --settings './settings_target/PDL1.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'\nOr create your own:\n#!/bin/bash\n#SBATCH --job-name=bindcraft\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --time=48:00:00\n#SBATCH --output=%x_%j.out\n\n# source ~/.bashrc\nconda activate BindCraft\n\ncd /path/to/bindcraft\n\npython -u ./bindcraft.py \\\n    --settings './settings_target/my_target.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'",
    "crumbs": [
      "Monday",
      "11. BindCraft"
    ]
  },
  {
    "objectID": "monday/11-bindcraft.html#setting-up-your-own-target",
    "href": "monday/11-bindcraft.html#setting-up-your-own-target",
    "title": "11. BindCraft",
    "section": "Setting Up Your Own Target",
    "text": "Setting Up Your Own Target\nStep 1: Prepare your target PDB\n\nPlace your target protein PDB in the BindCraft folder\nTrim unnecessary chains/residues to reduce memory and speed up design\n\nStep 2: Create target settings (settings_target/my_target.json):\n{\n    \"design_path\": \"./my_binder_designs\",\n    \"binder_name\": \"my_binder\",\n    \"starting_pdb\": \"./my_target.pdb\",\n    \"chains\": \"A\",\n    \"target_hotspot_residues\": \"A10-20\",\n    \"lengths\": \"50-100\",\n    \"number_of_final_designs\": 100\n}\nStep 3: Run the pipeline:\npython -u ./bindcraft.py \\\n    --settings './settings_target/my_target.json' \\\n    --filters './settings_filters/default_filters.json' \\\n    --advanced './settings_advanced/default_4stage_multimer.json'",
    "crumbs": [
      "Monday",
      "11. BindCraft"
    ]
  },
  {
    "objectID": "monday/11-bindcraft.html#key-settings-explained",
    "href": "monday/11-bindcraft.html#key-settings-explained",
    "title": "11. BindCraft",
    "section": "Key Settings Explained",
    "text": "Key Settings Explained\n\nTarget Settings (settings_target/*.json)\n\n\n\n\n\n\n\n\nSetting\nDescription\nExample\n\n\n\n\nstarting_pdb\nPath to target structure\n\"./my_target.pdb\"\n\n\nchains\nWhich chain(s) to target\n\"A\" or \"A,B\"\n\n\ntarget_hotspot_residues\nResidues to target\n\"A10-20\" or null (auto)\n\n\nlengths\nBinder length range\n\"50-100\"\n\n\nnumber_of_final_designs\nDesigns to generate\n100\n\n\n\n\n\nFilter Settings (settings_filters/*.json)\nControls which designs pass quality thresholds:\n\nConfidence scores (pLDDT, pTM, i_pTM)\nInterface quality (shape complementarity, energy)\nDefault filters are good starting points\n\n\n\nAdvanced Settings (settings_advanced/*.json)\n\nDesign algorithm (default: 4-stage)\nNumber of iterations per stage\nAlphaFold2 and ProteinMPNN parameters",
    "crumbs": [
      "Monday",
      "11. BindCraft"
    ]
  },
  {
    "objectID": "monday/11-bindcraft.html#understanding-the-pipeline",
    "href": "monday/11-bindcraft.html#understanding-the-pipeline",
    "title": "11. BindCraft",
    "section": "Understanding the Pipeline",
    "text": "Understanding the Pipeline\nBindCraft demonstrates a complete protein design workflow:\n1. DESIGN        → AlphaFold2 backpropagation generates binder backbones\n       ↓\n2. OPTIMIZE      → ProteinMPNN designs sequences for backbones\n       ↓\n3. VALIDATE      → AlphaFold2 predicts designed complex structure\n       ↓\n4. SCORE         → PyRosetta evaluates interface quality\n       ↓\n5. FILTER        → Keep designs passing confidence thresholds",
    "crumbs": [
      "Monday",
      "11. BindCraft"
    ]
  },
  {
    "objectID": "monday/11-bindcraft.html#tips-for-success",
    "href": "monday/11-bindcraft.html#tips-for-success",
    "title": "11. BindCraft",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nTrim your target: Remove unnecessary chains/residues to reduce memory\nStart with defaults: Use default filter and advanced settings initially\nGenerate enough designs: Aim for 100+ final designs (top 5-20 for experiments)\nBe patient: Expect hundreds to thousands of trajectories for enough accepted binders\nMonitor acceptance rate: Low acceptance → adjust design weights or filters\nCheck the wiki: BindCraft Wiki",
    "crumbs": [
      "Monday",
      "11. BindCraft"
    ]
  },
  {
    "objectID": "monday/11-bindcraft.html#understanding-the-output",
    "href": "monday/11-bindcraft.html#understanding-the-output",
    "title": "11. BindCraft",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\nmy_binder_designs/\n├── accepted/\n│   ├── design_001.pdb        # Passing designs\n│   ├── design_002.pdb\n│   └── ...\n├── rejected/                  # Filtered out designs\n├── trajectories/              # All generated trajectories\n├── scores.csv                 # All metrics for each design\n└── summary.txt                # Run statistics",
    "crumbs": [
      "Monday",
      "11. BindCraft"
    ]
  },
  {
    "objectID": "monday/11-bindcraft.html#troubleshooting",
    "href": "monday/11-bindcraft.html#troubleshooting",
    "title": "11. BindCraft",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nGPU memory errors:\n\nReduce target PDB size (trim chains)\nRequest more GPU memory (32+ GB recommended)\nCheck with: nvidia-smi\n\nCUDA version mismatch:\n\nRe-run install with correct CUDA version\nCheck: nvcc --version\n\nLow acceptance rate (few designs pass filters):\n\nAdjust design weights in advanced settings\nRelax filter thresholds\nChange target hotspot selection\nIncrease target site area\n\nPyRosetta license errors:\n\nVerify PyRosetta license is valid\nCheck license file location\nContact PyRosetta for academic license\n\nSlow progress:\n\nThis is normal - protein design takes time\nMonitor trajectory count, not clock time\nLarge targets are slower",
    "crumbs": [
      "Monday",
      "11. BindCraft"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html",
    "href": "monday/prework-1-env-git.html",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "",
    "text": "This assignment ensures you have properly set up your development environment and can use GitHub for version control. By completing this assignment, you will:\n\nInstall Anaconda (if not already installed)\nCreate a conda environment with all required packages for the bootcamp\nVerify your installation with a Python script\nCommit and push your verification results to GitHub",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#overview",
    "href": "monday/prework-1-env-git.html#overview",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "",
    "text": "This assignment ensures you have properly set up your development environment and can use GitHub for version control. By completing this assignment, you will:\n\nInstall Anaconda (if not already installed)\nCreate a conda environment with all required packages for the bootcamp\nVerify your installation with a Python script\nCommit and push your verification results to GitHub",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#instructions",
    "href": "monday/prework-1-env-git.html#instructions",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "Instructions",
    "text": "Instructions\n\nStep 1: Install Anaconda (or Alternative)\nNote: If you already have Miniconda, Miniforge, Mambaforge, or Mamba installed, you can use those instead - they all work with this assignment!\nOption 1: Download Anaconda (Mac/Windows/Linux)\nFor Mac/Windows, download and install from: https://www.anaconda.com/download\nFor Windows users in WSL, use the Linux installer:\n# Download the Linux installer in WSL\nwget https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh\n\n# Run the installer\nbash Anaconda3-2024.10-1-Linux-x86_64.sh\n\n# Follow the prompts and accept the license\n# When asked to initialize conda, type 'yes'\n\n# Restart your terminal or run:\nsource ~/.bashrc\nOption 2: Install via Homebrew (Mac users only) If you have Homebrew installed:\nbrew install --cask anaconda\nThen add conda to your PATH (follow the instructions shown after installation).\nVerify installation by running:\nconda --version\n\n\nStep 2: Create Your Working Directory\nSince this is a self-guided bootcamp, you’ll need a place to organize your files.\n\nCreate a new folder for the bootcamp (e.g., RosettaBootcamp2025).\nInside that folder, create a subfolder for this module (e.g., HW1-setup).\nDownload the environment.yml and verify_setup.py files provided in the course materials (or create them manually if needed).\n\n\n\nStep 3: Create the Conda Environment\nNavigate to your working directory in your terminal and run:\nconda env create -f environment.yml\nThis will create an environment named bootcamp2025_HW1 with all required packages: - Python 3.11 - PyRosetta (for protein structure manipulation - channels configured in environment.yml) - NumPy (numerical computing) - Pandas (data analysis) - Matplotlib & Seaborn (visualization) - Jupyter (interactive notebooks) - SciPy (scientific computing) - scikit-learn (machine learning) - Biopython (bioinformatics tools)\nNote: The environment.yml file includes the RosettaCommons conda channel, so you don’t need to configure it manually.\n\n\nStep 4: Activate the Environment\nconda activate bootcamp2025_HW1\nYou should see (bootcamp2025_HW1) in your terminal prompt.\n\n\nStep 5: Run the Verification Script\npython verify_setup.py\nThis script will: - Check that all required packages are installed - Display version information for each package - Generate a verification_result.json file\nIf all checks pass, you should see:\n🎉 SUCCESS! All packages are installed correctly!\nIf some checks fail, the script will provide guidance on what went wrong.\n\n\nStep 6: Verify Your Setup\nOpen the generated verification_result.json file to confirm that verification_passed is set to true. This confirms your environment is ready for the rest of the bootcamp!",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#troubleshooting",
    "href": "monday/prework-1-env-git.html#troubleshooting",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nPyRosetta Installation Issues\nIf PyRosetta fails to install: 1. Make sure you’re using the environment.yml file which includes the RosettaCommons channel 2. Try creating the environment again with verbose output: conda env create -f environment.yml --verbose 3. If that fails, try installing PyRosetta separately: conda install -c https://conda.rosettacommons.org pyrosetta 4. Check that you’re using a compatible Python version (3.11 is specified in environment.yml)\n\n\nImport Errors\nIf packages are installed but imports fail: 1. Make sure you’ve activated the environment: conda activate bootcamp2025_HW1 2. Try reinstalling the problematic package: conda install --force-reinstall &lt;package-name&gt;\n\n\nGit Issues\nIf you have trouble with git: - Make sure git is installed: git --version - Configure your git identity if needed: bash   git config --global user.name \"Your Name\"   git config --global user.email \"your.email@example.com\"\n\n\nAuthentication with GitHub\nIf you have trouble pushing to GitHub, you may need to set up authentication: - Use GitHub’s personal access token: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token - Or set up SSH keys: https://docs.github.com/en/authentication/connecting-to-github-with-ssh",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-1-env-git.html#questions-and-getting-help",
    "href": "monday/prework-1-env-git.html#questions-and-getting-help",
    "title": "Pre-work 1: Environment Setup & GitHub Basics",
    "section": "Questions and Getting Help",
    "text": "Questions and Getting Help\nIf you have questions or encounter issues while working through this module, please open an issue on the GitHub repository (GitHub account required).",
    "crumbs": [
      "Monday",
      "Pre-work 1: Environment Setup & GitHub Basics"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html",
    "href": "monday/prework-3-python.html",
    "title": "Pre-work 3: Python Refresher",
    "section": "",
    "text": "This assignment is designed as a Python refresher for biologists and bioinformaticians who may be a bit rusty. You’ll complete a series of small exercises that build up to creating a simple sequence analysis tool. This assignment should take approximately 30-45 minutes to complete.\nIMPORTANT: Each Python file contains function templates with TODO comments. Your job is to fill in the missing code where you see # TODO: comments. The scripts won’t work until you complete the TODO sections!",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#overview",
    "href": "monday/prework-3-python.html#overview",
    "title": "Pre-work 3: Python Refresher",
    "section": "",
    "text": "This assignment is designed as a Python refresher for biologists and bioinformaticians who may be a bit rusty. You’ll complete a series of small exercises that build up to creating a simple sequence analysis tool. This assignment should take approximately 30-45 minutes to complete.\nIMPORTANT: Each Python file contains function templates with TODO comments. Your job is to fill in the missing code where you see # TODO: comments. The scripts won’t work until you complete the TODO sections!",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#prerequisites",
    "href": "monday/prework-3-python.html#prerequisites",
    "title": "Pre-work 3: Python Refresher",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nHW1 completed (Python environment set up)",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#download-assignment-files",
    "href": "monday/prework-3-python.html#download-assignment-files",
    "title": "Pre-work 3: Python Refresher",
    "section": "Download Assignment Files",
    "text": "Download Assignment Files\n  Download Assignment Files (ZIP) \n\nDownload the ZIP file above.\nUnzip the folder on your computer.\nOpen the folder in VS Code.",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#how-to-complete-this-assignment",
    "href": "monday/prework-3-python.html#how-to-complete-this-assignment",
    "title": "Pre-work 3: Python Refresher",
    "section": "How to Complete This Assignment",
    "text": "How to Complete This Assignment\nEach Python file is a template with incomplete functions. Look for # TODO: comments - these mark where you need to write code. Read the docstrings and hints carefully, then implement the missing functionality.\nThe scripts will not work until you complete the TODO sections!\n\nPart 1: Python Basics Warmup (basics.py)\nOpen basics.py and fill in the TODO sections for these exercises: - reverse_string() - Write a function to reverse a string - count_characters() - Use a for loop to count characters in a string - amino_acid_composition() - Create a dictionary to calculate amino acid percentages - filter_sequences_by_length() - Use a list comprehension to filter sequences\nTo test your work:\n# Navigate to the unzipped directory first!\ncd path/to/python-refresher\n\nconda activate bootcamp2025_HW1\npython basics.py\nIf you see None or errors, you haven’t completed all the TODOs yet!\n\n\nPart 2: Sequence Utilities (sequence_utils.py)\nOpen sequence_utils.py and implement the TODO sections for these bioinformatics functions: - molecular_weight() - Calculate the molecular weight of a protein - count_hydrophobic() - Count hydrophobic amino acids - find_motif() - Find all positions of a motif in a sequence - count_charged_residues() - Count positive and negative charges\nTo test your work:\npython sequence_utils.py\nThe test cases at the bottom will only work once you’ve filled in all the TODOs!\n\n\nPart 3: FASTA File Parsing (read_fasta.py)\nOpen read_fasta.py and complete the TODO sections to: - read_fasta() - Read and parse a FASTA file, storing sequences in a dictionary (header → sequence) - print_fasta_stats() - Print basic statistics about the sequences\nTo test your work:\npython read_fasta.py sample.fasta\nYou should see information about each protein sequence. If not, check your TODOs!\n\n\nPart 4: Sequence Analysis Tool (analyze_sequence.py)\nOpen analyze_sequence.py and fill in the TODO sections to create a complete analysis script: - analyze_sequences() - Use your sequence_utils functions to analyze each protein - write_results() - Write the analysis results to a file\nThis part brings together everything from Parts 1-3!\nTo run your completed analysis:\npython analyze_sequence.py sample.fasta\nThis should create an analysis_results.txt file with your results. If it doesn’t work, you’re missing some TODOs!",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#tips",
    "href": "monday/prework-3-python.html#tips",
    "title": "Pre-work 3: Python Refresher",
    "section": "Tips",
    "text": "Tips\n\nTest frequently: Run your code after each function to catch errors early\nUse print statements: Debug by printing intermediate values\nRead the TODO comments carefully: They contain hints and requirements\nDon’t worry about edge cases: Focus on getting the basic functionality working\nAsk for help: If you’re stuck, reach out!",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#expected-output",
    "href": "monday/prework-3-python.html#expected-output",
    "title": "Pre-work 3: Python Refresher",
    "section": "Expected Output",
    "text": "Expected Output\nWhen you run analyze_sequence.py, you should see output similar to:\nAnalyzing sequences from sample.fasta...\n\nSequence: gene_example_1\n  Length: 120 bp\n  GC Content: 45.83%\n  Reverse Complement: CGATCG...\n\nResults written to analysis_results.txt",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/prework-3-python.html#questions-and-getting-help",
    "href": "monday/prework-3-python.html#questions-and-getting-help",
    "title": "Pre-work 3: Python Refresher",
    "section": "Questions and Getting Help",
    "text": "Questions and Getting Help\nIf you have questions or encounter issues while working through this module, please open an issue on the GitHub repository (GitHub account required).",
    "crumbs": [
      "Monday",
      "Pre-work 3: Python Refresher"
    ]
  },
  {
    "objectID": "monday/1-hpc-setup.html",
    "href": "monday/1-hpc-setup.html",
    "title": "1. Common HPC Setup",
    "section": "",
    "text": "Before installing individual ML tools, ensure your HPC environment is properly configured. This guide covers the foundational setup that all subsequent modules depend on.",
    "crumbs": [
      "Monday",
      "1. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/1-hpc-setup.html#resource-requirements-overview",
    "href": "monday/1-hpc-setup.html#resource-requirements-overview",
    "title": "1. Common HPC Setup",
    "section": "Resource Requirements Overview",
    "text": "Resource Requirements Overview\nMost ML protein tools share similar computational requirements. Here’s a general guide:\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n40+ GB\nA100 80GB ideal for large proteins\n\n\nCPU RAM\n32 GB\n64 GB\nMore for MSA generation\n\n\nDisk Space\n50 GB\n200+ GB\nModel weights + databases\n\n\nCUDA\n11.6+\n12.1+\nCheck tool-specific requirements",
    "crumbs": [
      "Monday",
      "1. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/1-hpc-setup.html#checking-your-hpc-environment",
    "href": "monday/1-hpc-setup.html#checking-your-hpc-environment",
    "title": "1. Common HPC Setup",
    "section": "Checking Your HPC Environment",
    "text": "Checking Your HPC Environment\n Mark as complete\n\n\n\n\n\n\nTipInternet Access on HPC\n\n\n\nMany HPC clusters do not have internet access on compute nodes (the nodes where your heavy jobs run). They often only have internet on “login” or “head” nodes.\n\nDownloads: Always run installation and download commands on a login node.\nExecution: When running jobs, ensure your tools don’t try to download models on the fly. Pre-download all weights and databases.\n\n\n\n\n1. Check Available CUDA Modules\nmodule avail cuda\nThis shows all CUDA versions installed on your cluster. Note the versions - you’ll need to match them to tool requirements.\n\n\n2. Check GPU Availability\nRequest an interactive GPU session:\n# SLURM example\nsrun --gpus=1 --pty bash\nThen check GPU status:\nnvidia-smi\nThis shows: - GPU model (A100, V100, RTX 4090, etc.) - GPU memory (important for large models) - Current CUDA driver version\n\n\n3. Check CUDA Toolkit Version\nnvcc --version\nIf this fails, load a CUDA module first:\nmodule load cuda/12.1\nnvcc --version",
    "crumbs": [
      "Monday",
      "1. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/1-hpc-setup.html#condamamba-setup",
    "href": "monday/1-hpc-setup.html#condamamba-setup",
    "title": "1. Common HPC Setup",
    "section": "Conda/Mamba Setup",
    "text": "Conda/Mamba Setup\n Mark as complete\nMost tools use Conda environments. Mamba is recommended as it’s significantly faster than Conda for dependency resolution.\n\nInstalling Mamba (if not available)\nIf your HPC doesn’t have Mamba, install Miniforge:\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\nbash Miniforge3-Linux-x86_64.sh\nFollow prompts, then restart your shell or run:\nsource ~/.bashrc\n\n\nBest Practices for HPC Conda Usage\n\nUse dedicated environment directories: Set environment location to avoid filling home directory quota:\n\n# Add to ~/.condarc\nenvs_dirs:\n  - /scratch/$USER/conda_envs\npkgs_dirs:\n  - /scratch/$USER/conda_pkgs\n\nOne environment per tool: Don’t try to install all tools in one environment - dependency conflicts are common.\nExport environments for reproducibility:\n\nmamba env export &gt; environment.yml",
    "crumbs": [
      "Monday",
      "1. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/1-hpc-setup.html#docker-vs-singularityapptainer",
    "href": "monday/1-hpc-setup.html#docker-vs-singularityapptainer",
    "title": "1. Common HPC Setup",
    "section": "Docker vs Singularity/Apptainer",
    "text": "Docker vs Singularity/Apptainer\n Mark as complete\nIMPORTANT: Most academic HPCs do NOT support Docker for security reasons. Use Singularity or Apptainer instead.\n\nLoading Container Runtime\nmodule load apptainer\n# or on older systems:\nmodule load singularity\n\n\nConverting Docker Commands to Apptainer\nMany tool READMEs show Docker commands. Here’s how to translate them:\n\n\n\nDocker Command\nApptainer Equivalent\n\n\n\n\ndocker run\napptainer run\n\n\ndocker run --gpus all\napptainer run --nv\n\n\ndocker run -v /path:/path\napptainer run --bind /path:/path\n\n\ndocker pull image:tag\napptainer pull docker://image:tag\n\n\n\nExample conversion:\n# Docker (won't work on HPC):\ndocker run --gpus all -v $(pwd):/workspace myimage:latest python script.py\n\n# Apptainer (works on HPC):\napptainer run --nv --bind $(pwd):/workspace myimage.sif python script.py\n\n\nPulling Docker Images as Singularity Files\napptainer pull docker://nvcr.io/nvidia/pytorch:23.10-py3\n# Creates: pytorch_23.10-py3.sif",
    "crumbs": [
      "Monday",
      "1. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/1-hpc-setup.html#slurm-job-submission-basics",
    "href": "monday/1-hpc-setup.html#slurm-job-submission-basics",
    "title": "1. Common HPC Setup",
    "section": "SLURM Job Submission Basics",
    "text": "SLURM Job Submission Basics\n Mark as complete\nMost HPCs use SLURM for job scheduling. Here’s a template for ML jobs:\n#!/bin/bash\n#SBATCH --job-name=my_ml_job\n#SBATCH --partition=gpu          # GPU partition name (varies by cluster)\n#SBATCH --gpus=1                  # Number of GPUs\n#SBATCH --cpus-per-task=8         # CPUs for data loading\n#SBATCH --mem=64G                 # RAM\n#SBATCH --time=04:00:00           # Wall time (HH:MM:SS)\n#SBATCH --output=%x_%j.out        # Output file (%x=job name, %j=job ID)\n#SBATCH --error=%x_%j.err         # Error file\n\n# Load required modules\nmodule load cuda/12.1\nmodule load apptainer\n\n# Activate conda environment\n# source ~/.bashrc  # Source your shell profile if needed\nsource /path/to/your/miniconda3/etc/profile.d/conda.sh # Better: source conda.sh directly\nmamba activate my_env\n\n# Run your command\npython my_script.py\n\nCommon SLURM Commands\n\n\n\nCommand\nDescription\n\n\n\n\nsbatch script.sh\nSubmit job\n\n\nsqueue -u $USER\nCheck your jobs\n\n\nscancel JOB_ID\nCancel a job\n\n\nsinfo\nShow partition info\n\n\nsacct -j JOB_ID\nJob accounting info\n\n\n\n\n\nGPU Partition Names\nGPU partition names vary by cluster. Common names:\n\ngpu, gpus, gpu-shared\na100, v100, rtx\ngpu-debug (for testing)\n\nCheck your cluster’s documentation or run sinfo to see available partitions.",
    "crumbs": [
      "Monday",
      "1. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/1-hpc-setup.html#environment-variables",
    "href": "monday/1-hpc-setup.html#environment-variables",
    "title": "1. Common HPC Setup",
    "section": "Environment Variables",
    "text": "Environment Variables\n Mark as complete\nSeveral tools use environment variables. Add these to your ~/.bashrc:\n# Model weight storage (prevents filling home directory)\nexport TORCH_HOME=/scratch/$USER/torch_cache\nexport HF_HOME=/scratch/$USER/huggingface_cache\nexport TRANSFORMERS_CACHE=/scratch/$USER/transformers_cache\n\n# ColabFold databases\nexport COLABFOLD_DOWNLOAD_DIR=/scratch/$USER/colabfold_db\n\n# Chai-1 models\nexport CHAI_DOWNLOADS_DIR=/scratch/$USER/chai_models\n\n# General cache\nexport XDG_CACHE_HOME=/scratch/$USER/.cache\nReplace /scratch/$USER with your cluster’s scratch or work directory path.",
    "crumbs": [
      "Monday",
      "1. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/1-hpc-setup.html#verifying-gpu-works-with-pytorch",
    "href": "monday/1-hpc-setup.html#verifying-gpu-works-with-pytorch",
    "title": "1. Common HPC Setup",
    "section": "Verifying GPU Works with PyTorch",
    "text": "Verifying GPU Works with PyTorch\n Mark as complete\nAfter setting up an environment with PyTorch, verify GPU access:\nimport torch\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"GPU count: {torch.cuda.device_count()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n    # Quick computation test\n    x = torch.randn(1000, 1000, device='cuda')\n    y = torch.matmul(x, x)\n    print(\"GPU computation test: PASSED\")\nSave as test_gpu.py and run:\npython test_gpu.py\nExpected output (example):\nPyTorch version: 2.1.0\nCUDA available: True\nCUDA version: 12.1\nGPU count: 1\nGPU name: NVIDIA A100-SXM4-80GB\nGPU memory: 84.9 GB\nGPU computation test: PASSED",
    "crumbs": [
      "Monday",
      "1. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/1-hpc-setup.html#understanding-gpu-memory-requirements",
    "href": "monday/1-hpc-setup.html#understanding-gpu-memory-requirements",
    "title": "1. Common HPC Setup",
    "section": "Understanding GPU Memory Requirements",
    "text": "Understanding GPU Memory Requirements\nDifferent tasks require different GPU memory:\n\n\n\nTask\nTypical GPU Memory\n\n\n\n\nStructure prediction (small protein &lt;200 aa)\n8-16 GB\n\n\nStructure prediction (large protein &gt;500 aa)\n32-80 GB\n\n\nProtein design (RFdiffusion2)\n16-32 GB\n\n\nDocking (DiffDock-PP, PLACER)\n8-16 GB\n\n\nLanguage model inference (ESM3)\n16-40 GB\n\n\nBinder design (BindCraft)\n32-80 GB\n\n\n\nIf you get out-of-memory errors: 1. Request a GPU with more memory 2. Reduce batch size or sequence length 3. Use CPU offloading if available 4. Process sequences in chunks",
    "crumbs": [
      "Monday",
      "1. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/1-hpc-setup.html#troubleshooting-common-issues",
    "href": "monday/1-hpc-setup.html#troubleshooting-common-issues",
    "title": "1. Common HPC Setup",
    "section": "Troubleshooting Common Issues",
    "text": "Troubleshooting Common Issues\n\n“CUDA out of memory”\n\nRequest more GPU memory\nReduce batch size\nUse gradient checkpointing if training\n\n\n\n“No CUDA runtime found”\nmodule load cuda/12.1  # Load CUDA module\nnvcc --version         # Verify it loaded\n\n\n“Singularity: command not found”\nmodule load apptainer  # or: module load singularity\n\n\nConda environment activation fails in SLURM\nAdd to your job script:\n# Source conda.sh directly (adjust path to your installation)\nsource /path/to/miniforge3/etc/profile.d/conda.sh\nmamba activate my_env\n\n\nPermission denied on container\nchmod +x container.sif",
    "crumbs": [
      "Monday",
      "1. Common HPC Setup"
    ]
  },
  {
    "objectID": "monday/9-diffdock-pp.html",
    "href": "monday/9-diffdock-pp.html",
    "title": "9. DiffDock-PP",
    "section": "",
    "text": "DiffDock-PP (paper, code) is a graph neural network trained for de-noising of rigid transformations (rotation and translation) to predict protein-protein docking orientations between two rigid protein subunits.",
    "crumbs": [
      "Monday",
      "9. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/9-diffdock-pp.html#why-use-diffdock-pp",
    "href": "monday/9-diffdock-pp.html#why-use-diffdock-pp",
    "title": "9. DiffDock-PP",
    "section": "Why Use DiffDock-PP?",
    "text": "Why Use DiffDock-PP?\n\nFast protein-protein docking: Predicts binding orientations without expensive sampling\nValidation tool: Orthogonally validate structure predictions from other methods\nEnsemble predictions: Generate multiple docking poses for uncertainty estimation\nRigid-body docking: Efficient for cases where backbone flexibility is minimal\n\nRelated Tools: For protein-ligand docking, see PLACER. For flexible ligand binding with ensemble generation, see PLACER. For structure prediction of complexes, see Chai-1 or Boltz-2.",
    "crumbs": [
      "Monday",
      "9. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/9-diffdock-pp.html#resource-requirements",
    "href": "monday/9-diffdock-pp.html#resource-requirements",
    "title": "9. DiffDock-PP",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n8 GB\n16 GB\nScales with protein size\n\n\nCPU RAM\n8 GB\n16 GB\nFor preprocessing\n\n\nDisk Space\n2 GB\n5 GB\nModel weights\n\n\nCUDA\n11.6\n11.6-11.7\nSpecific version required",
    "crumbs": [
      "Monday",
      "9. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/9-diffdock-pp.html#preparation",
    "href": "monday/9-diffdock-pp.html#preparation",
    "title": "9. DiffDock-PP",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nCUDA 11.6 or 11.7 available\n\nCheck CUDA availability:\nmodule avail cuda\n# Look for cuda/11.6 or cuda/11.7",
    "crumbs": [
      "Monday",
      "9. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/9-diffdock-pp.html#installation",
    "href": "monday/9-diffdock-pp.html#installation",
    "title": "9. DiffDock-PP",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the DiffDock-PP repository:\n\ngit clone https://github.com/ketatam/DiffDock-PP.git\ncd DiffDock-PP\n\nCreate a new environment:\n\nmamba create -n diffdock_pp python=3.9\nmamba activate diffdock_pp\n\nInstall PyTorch with CUDA 11.6:\n\nmamba install pytorch=1.13.0 pytorch-cuda=11.6 -c pytorch -c nvidia\nWhy CUDA 11.6? DiffDock-PP was developed and tested with this version. Using different versions may cause compatibility issues with PyG.\n\nInstall PyTorch Geometric (PyG) packages:\n\nmamba install pytorch-scatter pytorch-sparse pytorch-cluster pytorch-spline-conv pyg -c pyg\n\nInstall remaining dependencies:\n\nmamba install mkl=2024.0 \"numpy&lt;2.0\" dill tqdm pyyaml pandas biopandas scikit-learn biopython e3nn wandb tensorboard tensorboardX matplotlib\nWhy numpy&lt;2.0? NumPy 2.0 introduced breaking changes that affect many scientific packages. Keeping NumPy below 2.0 ensures compatibility.",
    "crumbs": [
      "Monday",
      "9. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/9-diffdock-pp.html#testing-the-installation",
    "href": "monday/9-diffdock-pp.html#testing-the-installation",
    "title": "9. DiffDock-PP",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\n\nCreate required directories:\n\nmkdir storage\n\nRun the test script on the DB5 benchmark:\n\nbash src/db5_inference.sh\nSuccess indicators:\n\nCommand completes without errors\nOutput folder visualization/epoch-0/ is created\nDirectory contains PDB files of docked complexes (multiple .pdb files)\n\nExpected runtime: 5-15 minutes depending on GPU.\nVerify output:\nls visualization/epoch-0/*.pdb | wc -l\n# Should show multiple PDB files",
    "crumbs": [
      "Monday",
      "9. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/9-diffdock-pp.html#hpc-job-script",
    "href": "monday/9-diffdock-pp.html#hpc-job-script",
    "title": "9. DiffDock-PP",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=diffdock_pp\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/11.6\n\n# source ~/.bashrc\nmamba activate diffdock_pp\n\ncd /path/to/DiffDock-PP\n\n# Create output directory\nmkdir -p storage\n\n# Run inference\nbash src/db5_inference.sh",
    "crumbs": [
      "Monday",
      "9. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/9-diffdock-pp.html#usage-examples",
    "href": "monday/9-diffdock-pp.html#usage-examples",
    "title": "9. DiffDock-PP",
    "section": "Usage Examples",
    "text": "Usage Examples\nRun DB5 benchmark (default test):\nbash src/db5_inference.sh\nCustom docking (requires understanding the codebase):\nDiffDock-PP requires input data in a specific format. For custom proteins:\n\nPrepare receptor and ligand PDB files\nCreate data configuration files\nRun inference script\n\nSee the repository documentation for detailed input format requirements.",
    "crumbs": [
      "Monday",
      "9. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/9-diffdock-pp.html#understanding-the-output",
    "href": "monday/9-diffdock-pp.html#understanding-the-output",
    "title": "9. DiffDock-PP",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput structure:\nvisualization/\n└── epoch-0/\n    ├── complex_1_pose_0.pdb    # Docking pose 1\n    ├── complex_1_pose_1.pdb    # Docking pose 2\n    ├── complex_1_pose_2.pdb    # Docking pose 3\n    └── ...                      # More poses\nEach PDB file contains:\n\nBoth protein chains with predicted relative orientation\nMultiple poses represent different docking predictions\nCompare poses to assess uncertainty",
    "crumbs": [
      "Monday",
      "9. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/9-diffdock-pp.html#use-cases",
    "href": "monday/9-diffdock-pp.html#use-cases",
    "title": "9. DiffDock-PP",
    "section": "Use Cases",
    "text": "Use Cases\n\nProtein-Protein Docking: Predict binding orientations between protein chains\nComplex Validation: Validate predicted protein-protein interfaces from other methods\nEnsemble Generation: Generate multiple docking poses to capture uncertainty\nBenchmarking: Compare against other docking methods",
    "crumbs": [
      "Monday",
      "9. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/9-diffdock-pp.html#when-to-use-diffdock-pp-vs-other-tools",
    "href": "monday/9-diffdock-pp.html#when-to-use-diffdock-pp-vs-other-tools",
    "title": "9. DiffDock-PP",
    "section": "When to Use DiffDock-PP vs Other Tools",
    "text": "When to Use DiffDock-PP vs Other Tools\n\n\n\nTool\nBest For\n\n\n\n\nDiffDock-PP\nRigid protein-protein docking\n\n\nPLACER\nProtein-ligand docking with conformational sampling\n\n\nChai-1/Boltz-2\nAb initio complex structure prediction\n\n\nBindCraft\nDe novo binder design",
    "crumbs": [
      "Monday",
      "9. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/9-diffdock-pp.html#troubleshooting",
    "href": "monday/9-diffdock-pp.html#troubleshooting",
    "title": "9. DiffDock-PP",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nPyG installation fails:\n\nEnsure CUDA toolkit is loaded before installing\nInstall PyTorch first, then PyG packages\nVerify versions match:\npython -c \"import torch; print(torch.__version__, torch.cuda.is_available())\"\n\nCUDA version mismatch:\nCheck your system’s CUDA version:\nnvcc --version\nThis should match the pytorch-cuda version (11.6). If not:\nmodule load cuda/11.6\n“ModuleNotFoundError” for PyG components:\n\nInstall all PyG packages together:\nmamba install pytorch-scatter pytorch-sparse pytorch-cluster pytorch-spline-conv pyg -c pyg\n\nNumPy errors:\n\nEnsure numpy&lt;2.0 is installed\nDon’t upgrade NumPy even if prompted\n\nGPU not detected:\n# Verify CUDA is available to PyTorch\npython -c \"import torch; print(torch.cuda.is_available())\"\n# Should print: True\nEmpty output directory:\n\nCheck for error messages in terminal output\nVerify input files exist and are formatted correctly\nEnsure storage/ directory was created",
    "crumbs": [
      "Monday",
      "9. DiffDock-PP"
    ]
  },
  {
    "objectID": "monday/12-esm3.html",
    "href": "monday/12-esm3.html",
    "title": "12. ESM3 (Optional)",
    "section": "",
    "text": "ESM3 (paper, code) is a frontier generative model for biology that jointly reasons across three fundamental biological properties of proteins: sequence, structure, and function. It represents a multimodal generative masked language model.\nNote: This tool is marked as OPTIONAL. Install if you’re interested in protein generation and multimodal design beyond structure prediction.",
    "crumbs": [
      "Monday",
      "12. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-esm3.html#why-use-esm3",
    "href": "monday/12-esm3.html#why-use-esm3",
    "title": "12. ESM3 (Optional)",
    "section": "Why Use ESM3?",
    "text": "Why Use ESM3?\n\nMultimodal generation: Jointly reason about sequence, structure, and function\nProtein generation: Create novel proteins with desired properties\nSequence completion: Fill in masked or missing regions\nEmbeddings: Extract rich protein representations (ESM C)\n\nRelated Tools: For structure prediction only, see ESMFold. For sequence design given structure, see LigandMPNN.",
    "crumbs": [
      "Monday",
      "12. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-esm3.html#resource-requirements",
    "href": "monday/12-esm3.html#resource-requirements",
    "title": "12. ESM3 (Optional)",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n24+ GB\nFor esm3-small (1.4B params)\n\n\nCPU RAM\n16 GB\n32 GB\nFor preprocessing\n\n\nDisk Space\n10 GB\n20 GB\nModel weights\n\n\nPython\n3.10+\n3.10\nRequired\n\n\n\nModel sizes:\n\nesm3-small-2024-08 (1.4B params): Runs locally\nesm3-medium-2024-08 (7B params): Via Forge API\nesm3-large-2024-03 (98B params): Via Forge API",
    "crumbs": [
      "Monday",
      "12. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-esm3.html#preparation",
    "href": "monday/12-esm3.html#preparation",
    "title": "12. ESM3 (Optional)",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nHuggingFace account (for model access)",
    "crumbs": [
      "Monday",
      "12. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-esm3.html#installation",
    "href": "monday/12-esm3.html#installation",
    "title": "12. ESM3 (Optional)",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nCreate a conda environment:\n\nmamba create -n esm3 python=3.10\nmamba activate esm3\n\nInstall the ESM library:\n\npip install esm",
    "crumbs": [
      "Monday",
      "12. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-esm3.html#huggingface-authentication",
    "href": "monday/12-esm3.html#huggingface-authentication",
    "title": "12. ESM3 (Optional)",
    "section": "HuggingFace Authentication",
    "text": "HuggingFace Authentication\nESM3 weights are stored on HuggingFace Hub. You need to authenticate:\n\nCreate a HuggingFace account at huggingface.co\nGenerate an API token with “Read” permission at huggingface.co/settings/tokens\nAuthenticate in Python:\n\nfrom huggingface_hub import login\nlogin()  # Follow prompts to enter your token\nOr set environment variable:\nexport HF_TOKEN=\"your_token_here\"",
    "crumbs": [
      "Monday",
      "12. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-esm3.html#testing-the-installation",
    "href": "monday/12-esm3.html#testing-the-installation",
    "title": "12. ESM3 (Optional)",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test script test_esm3.py:\nimport os\nfrom huggingface_hub import login\nfrom esm.models.esm3 import ESM3\nfrom esm.sdk.api import ESMProtein, GenerationConfig\n\n# Authenticate\n# Method 1: Environment variable (Recommended for HPC jobs)\nif \"HF_TOKEN\" in os.environ:\n    login(token=os.environ[\"HF_TOKEN\"])\n# Method 2: Interactive login (Run once on login node)\nelse:\n    login()\n\n# Load the model (downloads weights on first run)\nmodel = ESM3.from_pretrained(\"esm3-small-2024-08\").to(\"cuda\")  # or \"cpu\"\n\n# Generate a protein sequence completion\nprompt = \"MKTVRQ_______________QLAEELSVSRQVIVQDIAYLRSLG\"\nprotein = ESMProtein(sequence=prompt)\n\n# Generate sequence\nprotein = model.generate(\n    protein,\n    GenerationConfig(track=\"sequence\", num_steps=8, temperature=0.7)\n)\n\nprint(\"Generated sequence:\")\nprint(protein.sequence)\n\n# Generate structure\nprotein = model.generate(\n    protein,\n    GenerationConfig(track=\"structure\", num_steps=8)\n)\n\n# Save structure\nprotein.to_pdb(\"./generated.pdb\")\nprint(\"Structure saved to generated.pdb\")\nRun the test:\npython test_esm3.py\nSuccess indicators:\n\nModel loads without errors\nSequence completion fills in the masked region\nStructure is generated and saved as PDB\n\nExpected runtime: 2-5 minutes (first run downloads ~3GB weights).",
    "crumbs": [
      "Monday",
      "12. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-esm3.html#hpc-job-script",
    "href": "monday/12-esm3.html#hpc-job-script",
    "title": "12. ESM3 (Optional)",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=esm3\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate esm3\n\n# Set HuggingFace token\nexport HF_TOKEN=\"your_token_here\"\n\npython generate_protein.py",
    "crumbs": [
      "Monday",
      "12. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-esm3.html#usage-examples",
    "href": "monday/12-esm3.html#usage-examples",
    "title": "12. ESM3 (Optional)",
    "section": "Usage Examples",
    "text": "Usage Examples\nSequence generation (fill masked regions):\nfrom esm.models.esm3 import ESM3\nfrom esm.sdk.api import ESMProtein, GenerationConfig\n\nmodel = ESM3.from_pretrained(\"esm3-small-2024-08\").to(\"cuda\")\n\n# Use underscores for masked positions\nprotein = ESMProtein(sequence=\"MKTVRQ_______________QLAEELSVSRQVIVQDIAYLRSLG\")\n\n# Generate\nprotein = model.generate(\n    protein,\n    GenerationConfig(track=\"sequence\", num_steps=8, temperature=0.7)\n)\nprint(protein.sequence)\nStructure prediction:\nprotein = ESMProtein(sequence=\"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLG\")\n\nprotein = model.generate(\n    protein,\n    GenerationConfig(track=\"structure\", num_steps=8)\n)\n\nprotein.to_pdb(\"predicted.pdb\")\nUsing ESM C for embeddings only (faster, smaller):\nfrom esm.models.esmc import ESMC\nfrom esm.sdk.api import ESMProtein, LogitsConfig\n\nprotein = ESMProtein(sequence=\"MKTVRQERLK\")\nclient = ESMC.from_pretrained(\"esmc_300m\").to(\"cuda\")\n\n# Get embeddings\nprotein_tensor = client.encode(protein)\nlogits_output = client.logits(\n    protein_tensor,\n    LogitsConfig(sequence=True, return_embeddings=True)\n)\n\nprint(f\"Embedding shape: {logits_output.embeddings.shape}\")",
    "crumbs": [
      "Monday",
      "12. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-esm3.html#available-models",
    "href": "monday/12-esm3.html#available-models",
    "title": "12. ESM3 (Optional)",
    "section": "Available Models",
    "text": "Available Models\n\n\n\nModel\nParameters\nAvailability\n\n\n\n\nesm3-small-2024-08\n1.4B\nLocal (free)\n\n\nesmc_300m\n300M\nLocal (fast embeddings)\n\n\nesmc_600m\n600M\nLocal\n\n\nesm3-medium-2024-08\n7B\nForge API\n\n\nesm3-large-2024-03\n98B\nForge API",
    "crumbs": [
      "Monday",
      "12. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-esm3.html#generation-tracks",
    "href": "monday/12-esm3.html#generation-tracks",
    "title": "12. ESM3 (Optional)",
    "section": "Generation Tracks",
    "text": "Generation Tracks\nESM3 can generate different “tracks”:\n\n\n\nTrack\nDescription\n\n\n\n\nsequence\nGenerate amino acid sequence\n\n\nstructure\nGenerate 3D coordinates\n\n\nfunction\nGenerate functional annotations",
    "crumbs": [
      "Monday",
      "12. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-esm3.html#key-parameters",
    "href": "monday/12-esm3.html#key-parameters",
    "title": "12. ESM3 (Optional)",
    "section": "Key Parameters",
    "text": "Key Parameters\n\n\n\nParameter\nDescription\n\n\n\n\ntrack\nWhat to generate: sequence, structure, function\n\n\nnum_steps\nNumber of generation steps (more = better quality)\n\n\ntemperature\nSampling diversity (higher = more diverse)",
    "crumbs": [
      "Monday",
      "12. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-esm3.html#use-cases",
    "href": "monday/12-esm3.html#use-cases",
    "title": "12. ESM3 (Optional)",
    "section": "Use Cases",
    "text": "Use Cases\n\nProtein generation: Create novel proteins\nSequence completion: Fill in missing regions\nStructure prediction: Generate 3D structures\nFunction prediction: Predict functional properties\nEmbeddings: Extract protein representations for ML",
    "crumbs": [
      "Monday",
      "12. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/12-esm3.html#troubleshooting",
    "href": "monday/12-esm3.html#troubleshooting",
    "title": "12. ESM3 (Optional)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nHuggingFace authentication errors:\n\nVerify token has “Read” permission\nRun login() in Python and follow prompts\nOr set HF_TOKEN environment variable\n\nModel download issues:\n\nCheck network connectivity\nWeights are large (~3GB for small model)\nSet HF_HOME to location with space:\nexport HF_HOME=/scratch/$USER/huggingface\n\nGPU memory issues:\n\nUse CPU if GPU is insufficient: .to(\"cpu\")\nReduce batch size if processing multiple proteins\nesmc_300m is smaller and faster for embeddings\n\nSlow generation:\n\nGPU strongly recommended\nReduce num_steps for faster (lower quality) results\nUse ESM C for embeddings (no structure generation)",
    "crumbs": [
      "Monday",
      "12. ESM3 (Optional)"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html",
    "href": "monday/prework-2-pymol-vscode.html",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "",
    "text": "This assignment ensures you have PyMOL and VS Code installed and configured for the bootcamp. You will:\n\nInstall PyMOL (a molecular visualization tool)\nInstall VS Code (a powerful code editor/IDE)\nVerify your installations by completing hands-on tasks",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#overview",
    "href": "monday/prework-2-pymol-vscode.html#overview",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "",
    "text": "This assignment ensures you have PyMOL and VS Code installed and configured for the bootcamp. You will:\n\nInstall PyMOL (a molecular visualization tool)\nInstall VS Code (a powerful code editor/IDE)\nVerify your installations by completing hands-on tasks",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#prerequisites",
    "href": "monday/prework-2-pymol-vscode.html#prerequisites",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nHW1 completed",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#part-1-pymol-installation",
    "href": "monday/prework-2-pymol-vscode.html#part-1-pymol-installation",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Part 1: PyMOL Installation",
    "text": "Part 1: PyMOL Installation\nPyMOL is a molecular visualization tool that is super useful for visualizing, manipulating, and analyzing molecular structures, especially proteins. We will use PyMOL to explore protein structures during bootcamp. We recognize that it’s likely you are already quite familiar with PyMOL but if you have been using an alternative visualization software, this is a great opportunity to explore PyMOL. Please be ready to use this in the Bootcamp.\n\nInstallation Options\nOption 1: Official PyMOL (Requires Educational License) 1. Visit the PyMOL website: https://pymol.org/ 2. Obtain a free educational license: https://pymol.org/edu/ 3. Download and install PyMOL\nOption 2: Open-Source PyMOL via Homebrew (Mac users) If you have Homebrew installed:\nbrew install pymol\nVerify installation by launching PyMOL (GUI) from your applications or by running:\npymol\n\n\nOptional PyMOL Learning Resources\nThough a comprehensive understanding of the various features in PyMOL is not within the scope of this course, it may be a useful tool for your future work and presentations.\n\nPyMOL Tutorial Video - Complete this tutorial using a protein of interest\nPyMOL Guide - Comprehensive written guide\nPyMOL Wiki - Explore more PyMOL features and tools\n\n\n\nCustomizing PyMOL (Optional)\nIf you would like to alter your visual defaults, you may do so by editing your .pymolrc, which is a script that gets read every time PyMOL opens. To do so, go to File &gt; Edit pymolrc. Here are the defaults that Rosetta member Joey Lubin uses, which can be pasted into the window that opens.",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#part-2-vs-code-installation",
    "href": "monday/prework-2-pymol-vscode.html#part-2-vs-code-installation",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Part 2: VS Code Installation",
    "text": "Part 2: VS Code Installation\nVS Code is a powerful IDE that is largely customizable and can incorporate many different extensions and features. If you do not have VS Code or an alternative IDE, please install it before the bootcamp.\n\nInstallation Options\nOption 1: Download from Website Visit the VS Code website: https://code.visualstudio.com/\nOption 2: Install via Homebrew (Mac users) If you have Homebrew installed:\nbrew install --cask visual-studio-code\nVerify installation by launching VS Code or running:\ncode --version\n\n\nRecommended VS Code Extensions\nOnce VS Code is installed, we recommend installing the following extensions:\n\nPython (Microsoft) - Python language support\nPylance (Microsoft) - Fast Python language server\nJupyter (Microsoft) - Jupyter notebook support\n\nTo install extensions: 1. Open VS Code 2. Click the Extensions icon in the sidebar (or press Cmd+Shift+X / Ctrl+Shift+X) 3. Search for each extension and click “Install”",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#part-3-verification-tasks",
    "href": "monday/prework-2-pymol-vscode.html#part-3-verification-tasks",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Part 3: Verification Tasks",
    "text": "Part 3: Verification Tasks\nNow let’s verify that everything is working correctly!\n\nTask 1: PyMOL Verification\n\nFetch a protein structure:\n\nOpen PyMOL\nIn the PyMOL command line, type: fetch 1ubq\nThis fetches ubiquitin (PDB ID: 1UBQ), a small regulatory protein\n\nStyle your visualization:\n\nTry different representations (cartoon, sticks, surface, etc.)\nColor the structure in a way you find visually appealing\nFeel free to experiment!\n\nSave your work:\n\nSave the PyMOL session: File &gt; Save Session As...\nName it 1ubq_session.pse and save it in this assignment directory\nExport an image: File &gt; Export Image As &gt; PNG...\nName it 1ubq_visualization.png and save it in this assignment directory\n\n\n\n\nTask 2: VS Code Verification\n\nOpen VS Code\nCreate a Python script:\n\nOpen this assignment folder in VS Code: File &gt; Open Folder...\nCreate a new file called verify_setup.py\nCopy and paste the verification script (provided in this repository)\nRun the script to verify your setup\n\nRun the verification script:\n\nOpen the integrated terminal in VS Code: Terminal &gt; New Terminal\nActivate your conda environment from HW1: conda activate bootcamp2025_HW1\nRun: python verify_setup.py\nThe script will check your installations and create a verification_result.txt file",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#part-4-verify-your-work",
    "href": "monday/prework-2-pymol-vscode.html#part-4-verify-your-work",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Part 4: Verify Your Work",
    "text": "Part 4: Verify Your Work\nSince this is a self-guided activity, verify that you have the following files in your working directory:\n\n1ubq_session.pse - Your saved PyMOL session. Open it to make sure it loads your view.\n1ubq_visualization.png - Your rendered image of Ubiquitin.\nverification_result.txt - Output from the verification script showing success.\n\nSelf-Check: - Can you navigate comfortably in PyMOL (rotate, zoom, clip)? - Can you run a Python script directly inside VS Code? - Did you successfully install the Python and Jupyter extensions?\nIf yes, you are ready for the next module!",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#troubleshooting",
    "href": "monday/prework-2-pymol-vscode.html#troubleshooting",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nPyMOL Issues\nPyMOL won’t launch: - Make sure you’ve completed the installation fully - Mac users: Try the Homebrew installation method - Make sure you’re launching the GUI application, not just the Python package\nCan’t fetch PDB structures: - Check your internet connection - Try: fetch 1ubq, type=pdb1 if the default doesn’t work\n\n\nVS Code Issues\ncode command not found: - On Mac: Open VS Code, press Cmd+Shift+P, type “shell command”, and select “Install ‘code’ command in PATH” - On Windows: VS Code should be in your PATH automatically after installation\nPython extension not working: - Make sure Python is installed (you should have this from HW1) - Restart VS Code after installing extensions",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "monday/prework-2-pymol-vscode.html#questions-and-getting-help",
    "href": "monday/prework-2-pymol-vscode.html#questions-and-getting-help",
    "title": "Pre-work 2: PyMOL and VS Code Setup",
    "section": "Questions and Getting Help",
    "text": "Questions and Getting Help\nIf you have questions or encounter issues while working through this module, please open an issue on the GitHub repository (GitHub account required).",
    "crumbs": [
      "Monday",
      "Pre-work 2: PyMOL and VS Code Setup"
    ]
  },
  {
    "objectID": "tuesday/2-structure-prediction.html",
    "href": "tuesday/2-structure-prediction.html",
    "title": "2. Introduction to Structure Prediction",
    "section": "",
    "text": "This module provides the foundational concepts behind protein structure prediction—the theoretical framework that underlies tools like AlphaFold2, ESMFold, and other modern prediction methods.",
    "crumbs": [
      "Tuesday",
      "2. Introduction to Structure Prediction"
    ]
  },
  {
    "objectID": "tuesday/2-structure-prediction.html#slides",
    "href": "tuesday/2-structure-prediction.html#slides",
    "title": "2. Introduction to Structure Prediction",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Tuesday",
      "2. Introduction to Structure Prediction"
    ]
  },
  {
    "objectID": "tuesday/2-structure-prediction.html#what-is-structure-prediction",
    "href": "tuesday/2-structure-prediction.html#what-is-structure-prediction",
    "title": "2. Introduction to Structure Prediction",
    "section": "What is Structure Prediction?",
    "text": "What is Structure Prediction?\nAt its core, protein structure prediction attempts to answer a deceptively simple question:\n\nGiven the linear amino acid sequence of a polypeptide, what is its 3D structure?\n\nThis is known as the protein folding problem, and it has been one of the grand challenges of molecular biology for over 50 years.\nWhy is this hard?\nA protein sequence like:\nMGDIQVQVNIDDNGKNFDYTYTVTTESELQKVLNELMDYIKKQGAKRVRISITARTKKEAEKFAAILIKVFAELGYNDINVTFDGDTVTVEGQLE\nMust fold into a precise three-dimensional arrangement where: - Every atom has specific coordinates - The structure is thermodynamically stable - The fold enables biological function",
    "crumbs": [
      "Tuesday",
      "2. Introduction to Structure Prediction"
    ]
  },
  {
    "objectID": "tuesday/2-structure-prediction.html#the-theoretical-foundation",
    "href": "tuesday/2-structure-prediction.html#the-theoretical-foundation",
    "title": "2. Introduction to Structure Prediction",
    "section": "The Theoretical Foundation",
    "text": "The Theoretical Foundation\n\nAnfinsen’s Thermodynamic Hypothesis (1960s)\nChristian Anfinsen’s Nobel Prize-winning work established a fundamental principle:\n\nThe native (functional) structure of a protein is at a free energy minimum.\n\nThis means: - The sequence contains all the information needed to specify the structure - Given the right conditions, a protein will spontaneously fold to its native state - Structure is determined by physics, not by cellular machinery\n\n\n\n\n\n\nNoteAnfinsen’s Experiment\n\n\n\nAnfinsen showed that ribonuclease A, when completely unfolded (denatured), would spontaneously refold to its active form when denaturing conditions were removed. This proved that the amino acid sequence alone determines the final structure.\n\n\n\n\nLevinthal’s Paradox\nShortly after Anfinsen’s work, Cyrus Levinthal raised a puzzling question:\n\nHow can proteins fold so quickly if they have so many possible conformations?\n\nConsider a modest 101-residue protein: - Each residue has ~3 possible backbone conformations (φ/ψ angles) - Total possible conformations: 3^200 ≈ 10^95 states - If the protein sampled one conformation per picosecond, it would take longer than the age of the universe to find the right one\nYet proteins fold in milliseconds to seconds. How?\n\n\nThe Folding Funnel\nThe resolution to Levinthal’s paradox came from understanding that the energy landscape is not flat—it’s shaped like a funnel:\n     ╱ ╲ ╱ ╲      ← High energy, many conformations (unfolded)\n    ╱   ╲   ╲\n   ╱     ╲   ╲\n  ╱       ╲   ╲\n ╱    ↓    ╲   ╲   ← Energy decreases as structure forms\n╱___________╲___╲\n      ●          ← Native state (energy minimum)\nKey insights:\n\nNot a random search: The protein doesn’t sample all conformations—it follows an energetic gradient\nLocal interactions guide folding: Secondary structure (helices, sheets) forms first, providing scaffolding\nFunnel shape: Many paths lead to the same native state\nRugged landscape: In reality, there are local minima (kinetic traps) that can slow folding\n\n\n\n\n\n\n\nImportantReality is More Complex\n\n\n\nIn practice, the folding landscape isn’t one clean funnel—it’s many interconnected funnels. This creates: - Folding intermediates: Partially folded states - Kinetic traps: Local minima that delay folding - Multiple conformations: Some proteins sample several states - Intrinsically disordered regions: Parts that never fully fold\nThis complexity is what makes structure prediction so challenging!",
    "crumbs": [
      "Tuesday",
      "2. Introduction to Structure Prediction"
    ]
  },
  {
    "objectID": "tuesday/2-structure-prediction.html#a-brief-history-casp-and-the-road-to-alphafold",
    "href": "tuesday/2-structure-prediction.html#a-brief-history-casp-and-the-road-to-alphafold",
    "title": "2. Introduction to Structure Prediction",
    "section": "A Brief History: CASP and the Road to AlphaFold",
    "text": "A Brief History: CASP and the Road to AlphaFold\n\nCritical Assessment of protein Structure Prediction (CASP)\nCASP is a biennial competition that has tracked progress in structure prediction since 1994. Organizers release sequences of proteins whose structures have been solved but not yet published, and teams submit blind predictions.\nKey milestones:\n\n\n\nYear\nCASP\nMajor Development\n\n\n\n\n1998\nCASP3\nFragment-based assembly (Rosetta)\n\n\n2014\nCASP11\nCo-evolutionary analysis + deep learning\n\n\n2018\nCASP13\nAlphaFold1: Deep learning-guided minimization\n\n\n2020\nCASP14\nAlphaFold2: Near-experimental accuracy\n\n\n2022\nCASP15\nAlphaFold-Multimer, ESMFold emerge\n\n\n\n\n\nThe Pre-AlphaFold Era\nFragment-based methods (Rosetta, ~1998): - Break known structures into fragments - Assemble fragments guided by energy functions - Sample many conformations, select lowest energy\nCo-evolutionary methods (~2014): - Key insight: Residues that contact each other in 3D co-evolve in sequence - Multiple sequence alignments (MSAs) reveal which residues co-vary - These “contact maps” constrain structure prediction\nEarly deep learning (~2018): - Neural networks predict contact maps from MSAs - Use contacts to guide physics-based minimization - AlphaFold1 used this approach\n\n\nThe AlphaFold2 Revolution (2020)\nAlphaFold2 achieved what many thought was decades away:\n\nGDT scores of ~90 on difficult targets (previously ~60 was state-of-the-art)\nApproaching experimental accuracy for many proteins\nEnd-to-end learning: Directly predicts coordinates, not intermediate features\n\nWe’ll explore AlphaFold2’s architecture in detail in the next module.",
    "crumbs": [
      "Tuesday",
      "2. Introduction to Structure Prediction"
    ]
  },
  {
    "objectID": "tuesday/2-structure-prediction.html#structure-prediction-metrics",
    "href": "tuesday/2-structure-prediction.html#structure-prediction-metrics",
    "title": "2. Introduction to Structure Prediction",
    "section": "Structure Prediction Metrics",
    "text": "Structure Prediction Metrics\nWhen evaluating predictions, several metrics are used:\n\nRoot Mean Squared Deviation (RMSD)\nWhat it measures: Average distance between corresponding atoms after optimal superposition.\n\\[RMSD = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}d_i^2}\\]\nwhere \\(d_i\\) is the distance between atom \\(i\\) in the prediction and reference.\nInterpretation: - &lt; 2 Å: Excellent (atomic-level accuracy) - 2-4 Å: Good (correct fold, minor deviations) - 4-8 Å: Moderate (correct topology, some errors) - &gt; 8 Å: Poor (likely wrong fold)\n\n\n\n\n\n\nWarningRMSD Limitations\n\n\n\n\nLength-dependent: Longer proteins tend to have higher RMSD\nSensitive to outliers: One badly placed domain can dominate\nRequires superposition: Choice of alignment region matters\n\n\n\n\n\nLocal Distance Difference Test (lDDT)\nWhat it measures: Local structural similarity without superposition.\nFor each residue, lDDT asks: “Are the distances to neighboring residues preserved?”\nKey properties: - Range: 0 to 1 (or 0-100 as percentage) - Length-independent: Can compare proteins of different sizes - Per-residue: Identifies which regions are well-predicted\nThis is the metric used for pLDDT (predicted lDDT) in AlphaFold2!\n\n\nGlobal Distance Test (GDT)\nWhat it measures: Fraction of residues within various distance thresholds.\n\\[GDT_{TS} = \\frac{1}{4}(P_1 + P_2 + P_4 + P_8)\\]\nwhere \\(P_x\\) is the percentage of residues within \\(x\\) Å of the reference.\nInterpretation: - &gt; 90: Excellent - 70-90: Good - 50-70: Moderate - &lt; 50: Poor\nGDT-TS was the primary metric in CASP until AlphaFold2 essentially “solved” it.\n\n\nTemplate Modeling Score (TM-score)\nWhat it measures: Global structural similarity, normalized by protein length.\nKey properties: - Range: 0 to 1 - Length-independent: Enables comparison across protein sizes - Threshold: TM-score &gt; 0.5 generally indicates same fold\n\n\n\nTM-score\nInterpretation\n\n\n\n\n&gt; 0.5\nSame fold\n\n\n0.3-0.5\nPossibly related\n\n\n&lt; 0.3\nLikely different folds",
    "crumbs": [
      "Tuesday",
      "2. Introduction to Structure Prediction"
    ]
  },
  {
    "objectID": "tuesday/2-structure-prediction.html#applications-of-structure-prediction",
    "href": "tuesday/2-structure-prediction.html#applications-of-structure-prediction",
    "title": "2. Introduction to Structure Prediction",
    "section": "Applications of Structure Prediction",
    "text": "Applications of Structure Prediction\n\n1. Molecular Replacement in Crystallography\nProblem: X-ray crystallography requires initial “phases” to solve structures.\nSolution: Predicted structures can provide starting models for: - Phase determination - Completing partial experimental structures - Filling in loops or disordered regions\n\n\n2. Interpreting Experimental Results\nExample scenarios: - A mutation causes loss of function—why? The structure shows it’s in the active site. - A protein doesn’t express well—the structure reveals aggregation-prone regions. - Two proteins interact—structures suggest the binding interface.\n\n\n3. Functional Prediction and Hypothesis Testing\nFrom structure, you can infer: - Active site location and chemistry - Binding pocket characteristics - Likely interaction partners (from shape complementarity) - Potential allosteric sites\n\n\n4. Starting Point for Protein Engineering\nCritical for: - Rational design: Knowing where to put mutations - Directed evolution: Understanding which regions to diversify - De novo design: Validating whether designed sequences will fold\n\n\n\n\n\n\nTipStructure Prediction in Your Research\n\n\n\nIn this bootcamp, you’ll use structure prediction to:\n\nPredict structures of proteins of interest\nValidate designs from tools like RFdiffusion\nAssess confidence to know which regions to trust\nCompare predictions from different methods",
    "crumbs": [
      "Tuesday",
      "2. Introduction to Structure Prediction"
    ]
  },
  {
    "objectID": "tuesday/2-structure-prediction.html#the-modern-prediction-landscape",
    "href": "tuesday/2-structure-prediction.html#the-modern-prediction-landscape",
    "title": "2. Introduction to Structure Prediction",
    "section": "The Modern Prediction Landscape",
    "text": "The Modern Prediction Landscape\nToday’s structure prediction tools fall into two main categories:\n\nMSA-Based Methods (e.g., AlphaFold2, OpenFold)\nStrengths: - Highest accuracy for proteins with many homologs - Leverage evolutionary information - Multiple model ensemble provides uncertainty estimates\nLimitations: - Slower (MSA generation takes time) - Less accurate for orphan proteins (few homologs) - Computationally expensive\n\n\nLanguage Model-Based Methods (e.g., ESMFold)\nStrengths: - Very fast (no MSA needed) - Work on designed/synthetic proteins - Simpler pipeline\nLimitations: - Generally lower accuracy than MSA-based methods - No ensemble diversity from single prediction - May miss evolutionary insights\n\n\nNext-Generation Methods (e.g., AlphaFold3, Chai-1, Boltz-2)\nThe field is rapidly advancing with: - Multi-modal predictions: Proteins + ligands + nucleic acids - Diffusion-based approaches: New generative paradigms - Improved confidence estimation: Better uncertainty quantification",
    "crumbs": [
      "Tuesday",
      "2. Introduction to Structure Prediction"
    ]
  },
  {
    "objectID": "tuesday/2-structure-prediction.html#key-takeaways",
    "href": "tuesday/2-structure-prediction.html#key-takeaways",
    "title": "2. Introduction to Structure Prediction",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nAnfinsen’s hypothesis established that sequence determines structure\nLevinthal’s paradox highlighted the computational challenge\nThe folding funnel explains how proteins fold efficiently\nCASP has tracked 25+ years of progress\nAlphaFold2 achieved a major breakthrough in 2020\nMultiple metrics (RMSD, lDDT, GDT, TM-score) capture different aspects of accuracy\nStructure enables function: Predictions have many practical applications",
    "crumbs": [
      "Tuesday",
      "2. Introduction to Structure Prediction"
    ]
  },
  {
    "objectID": "tuesday/2-structure-prediction.html#looking-ahead",
    "href": "tuesday/2-structure-prediction.html#looking-ahead",
    "title": "2. Introduction to Structure Prediction",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nIn the next modules, you’ll:\n\nLearn AlphaFold2’s architecture and how it achieves high accuracy\nRun your own predictions using ColabFold/LocalColabFold\nCompare methods (AlphaFold2 vs ESMFold) to understand trade-offs\nVisualize confidence and interpret prediction quality\n\nUnderstanding these foundational concepts will help you use structure prediction tools effectively and interpret their outputs critically.",
    "crumbs": [
      "Tuesday",
      "2. Introduction to Structure Prediction"
    ]
  },
  {
    "objectID": "tuesday/2-structure-prediction.html#hands-on-exercise",
    "href": "tuesday/2-structure-prediction.html#hands-on-exercise",
    "title": "2. Introduction to Structure Prediction",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\nThis module is primarily conceptual, but let’s reinforce these ideas with some exploration.\n\nPart 1: Explore CASP Results\nGoal: Understand how the field has progressed by looking at real CASP data.\n\nVisit the CASP website: predictioncenter.org\nCompare CASP rounds:\n\nLook at results from CASP11 (2014) vs CASP14 (2020)\nFind the GDT-TS scores for the top predictors\nNotice how dramatically scores improved with AlphaFold2\n\nQuestions to consider:\n\nWhat was the typical GDT-TS for “hard” targets before AlphaFold2?\nHow did AlphaFold2’s scores compare to the rest of the field in CASP14?\nWhy do you think some targets are labeled “hard” vs “easy”?\n\n\n\n\nPart 2: Metric Comparison Exercise\nGoal: Develop intuition for different structure comparison metrics.\nUsing PyMOL, let’s compare two related structures:\n# Fetch two related kinase structures\nfetch 1ATP   # PKA with ATP bound\nfetch 1J3H   # PKA in different conformation\n\n# Align them\nalign 1J3H, 1ATP\n\n# Note the RMSD printed in the console\nNow think about:\n\nRMSD question: The RMSD might be 2-4 Å. Does this mean the structures are different, or is this expected variation?\nPer-residue analysis:\n# Color by RMSD per residue (after alignment)\n# Red = high deviation, blue = low\nWhich regions show the most difference? (Usually loops and termini)\nWhy lDDT matters: If one domain moves relative to another (like in a kinase), RMSD after superposition will be high, but lDDT for each domain individually would still be good. This is why AlphaFold2 uses pLDDT as its confidence metric.\n\n\n\nPart 3: Discussion Questions\nWork through these questions with a partner or write brief notes:\n\nAnfinsen’s hypothesis:\n\nWhat evidence supports the idea that sequence determines structure?\nCan you think of exceptions? (Hint: chaperones, prions, intrinsically disordered proteins)\n\nLevinthal’s paradox:\n\nIf a protein has 100 residues and samples conformations at 10^12 per second, how long would it take to sample all 3^200 conformations?\nWhy doesn’t this happen in reality?\n\nPractical implications:\n\nYou predict a structure with AlphaFold2 and get average pLDDT of 75. What does this mean?\nOne region has pLDDT &lt; 50. Should you trust it? What might it indicate biologically?\n\nChoosing metrics:\n\nYou’re comparing two predictions of the same protein. When would you use RMSD vs TM-score?\nWhy is GDT-TS preferred over raw RMSD in CASP?\n\n\n\n\nPart 4: Prepare for AlphaFold2\nTo get ready for the next module:\n\nReview your HPC access: Make sure you can log into your cluster\nCheck for GPU availability: Run nvidia-smi on a GPU node\nPrepare a test sequence: Find the FASTA sequence for a protein you’re interested in\n\n\n\n\n\n\n\nTipSequence Resources\n\n\n\n\nUniProt: uniprot.org - Search for proteins and download sequences\nRCSB PDB: rcsb.org - Get sequences for proteins with known structures\n\n\n\n\n\nReflection\nAfter this module, you should be able to:\n\nExplain Anfinsen’s thermodynamic hypothesis in your own words\nDescribe Levinthal’s paradox and how the folding funnel resolves it\nList the major milestones in CASP history\nCompare and contrast RMSD, lDDT, GDT, and TM-score\nExplain when you would use structure prediction in your research",
    "crumbs": [
      "Tuesday",
      "2. Introduction to Structure Prediction"
    ]
  },
  {
    "objectID": "tuesday/4-esmfold.html",
    "href": "tuesday/4-esmfold.html",
    "title": "4. ESMFold vs. AlphaFold2",
    "section": "",
    "text": "In this module, you will use ESMFold to predict protein structures and systematically compare results with the AlphaFold2 predictions you generated in the previous module. ESMFold uses a fundamentally different approach—a protein language model trained on sequences alone, without requiring Multiple Sequence Alignments (MSAs) or templates. By comparing ESMFold to AF2, you will understand the trade-offs between different prediction methods and learn when to use each approach.",
    "crumbs": [
      "Tuesday",
      "4. ESMFold vs. AlphaFold2"
    ]
  },
  {
    "objectID": "tuesday/4-esmfold.html#overview",
    "href": "tuesday/4-esmfold.html#overview",
    "title": "4. ESMFold vs. AlphaFold2",
    "section": "Overview",
    "text": "Overview\nESMFold represents a different philosophy from AlphaFold2. While AF2 leverages evolutionary information through MSAs, ESMFold is built on a protein language model (ESM-2) that learns patterns directly from millions of protein sequences. This makes it:\n\nMuch faster: No MSA generation step\nBetter for orphan proteins: Works without homologs\nSimpler pipeline: Sequence in, structure out\n\nBut there are trade-offs—ESMFold may miss evolutionary insights that help AF2 achieve higher accuracy on well-studied proteins.",
    "crumbs": [
      "Tuesday",
      "4. ESMFold vs. AlphaFold2"
    ]
  },
  {
    "objectID": "tuesday/4-esmfold.html#learning-objectives",
    "href": "tuesday/4-esmfold.html#learning-objectives",
    "title": "4. ESMFold vs. AlphaFold2",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand ESMFold’s language model approach vs AF2’s MSA-based approach\nWrite Python code to use ESMFold following official documentation\nCompare prediction quality, speed, and confidence across different models\nMake informed decisions about model selection for different use cases\nCritically evaluate strengths and limitations of different prediction methods",
    "crumbs": [
      "Tuesday",
      "4. ESMFold vs. AlphaFold2"
    ]
  },
  {
    "objectID": "tuesday/4-esmfold.html#hands-on-exercise",
    "href": "tuesday/4-esmfold.html#hands-on-exercise",
    "title": "4. ESMFold vs. AlphaFold2",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\nThis module is structured as a hands-on comparison between ESMFold and AlphaFold2. You’ll run predictions, extract confidence scores, and develop intuition for when to use each tool.\n\nSetup\nPrerequisites: - Completed the AlphaFold2 module and have GFP predictions available - Access to a GPU (ESMFold requires ~8-16GB GPU RAM) - Python environment with PyTorch and the ESM library\nCreate your working directory:\nmkdir esmfold_activity\ncd esmfold_activity\n\n\n\nPart 1: Running ESMFold Structure Prediction\nGoal: Generate an ESMFold prediction and understand the workflow.\n\n\n1.1 Quick Introduction to ESMFold\nVisit the ESM GitHub repository: - https://github.com/facebookresearch/esm\nKey points about ESMFold: 1. Uses esm.pretrained.esmfold_v1() to load the model 2. Runs inference with model.infer_pdb(sequence) - returns a PDB-formatted string 3. Stores confidence scores (pLDDT) in the B-factor column of the PDB file 4. No MSA required - just sequence in, structure out!\n\n\n1.2 Write Your ESMFold Prediction Script\nCreate a file predict_esmfold.py:\nYou can follow the official example from the ESM GitHub, or use this template:\nimport torch\nimport esm\nimport time\n\n# Load model\nprint(\"Loading ESMFold model...\")\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval()\n\n# Move to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\nprint(f\"Using device: {device}\")\n\n# Your sequence (GFP - same as Activity 1)\nsequence = \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n\n# Run prediction with timing\nprint(f\"Predicting structure for sequence of length {len(sequence)}...\")\nstart_time = time.time()\n\nwith torch.no_grad():\n    output = model.infer_pdb(sequence)\n\nelapsed = time.time() - start_time\nprint(f\"Prediction completed in {elapsed:.1f} seconds ({elapsed/60:.2f} minutes)\")\n\n# Save to file\noutput_file = \"esmfold_gfp.pdb\"\nwith open(output_file, \"w\") as f:\n    f.write(output)\n\nprint(f\"Structure saved to {output_file}\")\nQuestions to consider: 1. What does torch.no_grad() do? (Hint: disables gradient calculation - saves memory during inference) 2. Why don’t we need to generate MSAs like in AlphaFold2? 3. How long did your prediction take?\n\n\n1.3 Run Your First Prediction\npython predict_esmfold.py\nExpected output: - A PDB file named esmfold_gfp.pdb should be created - Note the prediction time - this is much faster than ColabFold!",
    "crumbs": [
      "Tuesday",
      "4. ESMFold vs. AlphaFold2"
    ]
  },
  {
    "objectID": "tuesday/4-esmfold.html#part-2-extracting-and-comparing-confidence-scores",
    "href": "tuesday/4-esmfold.html#part-2-extracting-and-comparing-confidence-scores",
    "title": "4. ESMFold vs. AlphaFold2",
    "section": "Part 2: Extracting and Comparing Confidence Scores",
    "text": "Part 2: Extracting and Comparing Confidence Scores\nGoal: Extract pLDDT scores from both ESMFold and AlphaFold2 predictions for comparison.\n\n2.1 Understanding pLDDT Storage in PDB Files\nBoth ESMFold and AlphaFold2 store pLDDT confidence scores in the B-factor column of PDB files.\nPDB format (ATOM lines): - Columns 1-6: “ATOM” - Columns 7-11: Atom serial number - Columns 12-16: Atom name - Columns 17-20: Residue name - Columns 23-26: Residue number - Columns 31-54: X, Y, Z coordinates - Columns 61-66: B-factor (where pLDDT is stored!)\n\n\n2.2 Extract pLDDT Scores\nWe’ve provided a script extract_plddt.py that works for both ESMFold and AlphaFold2 PDB files.\nDownload extract_plddt.py\nRun it on your ESMFold prediction:\npython extract_plddt.py esmfold_gfp.pdb\nRun it on your AlphaFold2 prediction from the previous module:\npython extract_plddt.py path/to/your/af2_prediction.pdb\nQuestions: 1. What is the average pLDDT for ESMFold vs AlphaFold2? 2. Which model has more high-confidence residues (&gt;90)? 3. Do both models identify the same low-confidence regions?\n\n\n2.3 Visualize Confidence Scores in PyMOL\nLoad your ESMFold structure and color by confidence:\nload esmfold_gfp.pdb\n# Color by B-factor (pLDDT): blue = high confidence, red = low confidence\nspectrum b, blue_white_red, minimum=50, maximum=100\nIdentify and display low confidence regions:\n# Select residues with low confidence\nselect low_conf, b &lt; 70\nshow sticks, low_conf\ncolor yellow, low_conf\nCompare to AlphaFold2 visually: - Do the same for your AF2 prediction - Are low-confidence regions in similar locations?",
    "crumbs": [
      "Tuesday",
      "4. ESMFold vs. AlphaFold2"
    ]
  },
  {
    "objectID": "tuesday/4-esmfold.html#part-3-direct-comparison---esmfold-vs-alphafold2",
    "href": "tuesday/4-esmfold.html#part-3-direct-comparison---esmfold-vs-alphafold2",
    "title": "4. ESMFold vs. AlphaFold2",
    "section": "Part 3: Direct Comparison - ESMFold vs AlphaFold2",
    "text": "Part 3: Direct Comparison - ESMFold vs AlphaFold2\nGoal: Compare the two predictions structurally and assess accuracy.\n\n3.1 Prepare for Comparison\nYou need: 1. Your ESMFold prediction: esmfold_gfp.pdb (from Part 1) 2. Your AlphaFold2 prediction from the previous module (ColabFold output) 3. The experimental crystal structure of GFP from PDB\nDownload the crystal structure:\nwget https://files.rcsb.org/download/1GFL.pdb -O 1gfl_native.pdb\n\n\n3.2 Visual Comparison in PyMOL\nLoad and align all three structures:\n# Load all structures\nload 1gfl_native.pdb, native\nload path/to/af2_prediction.pdb, af2\nload esmfold_gfp.pdb, esmfold\n\n# Align both predictions to the native structure\nalign af2, native\nalign esmfold, native\n\n# Display with different colors\nhide everything\nshow cartoon\ncolor green, native\ncolor cyan, af2\ncolor magenta, esmfold\nAnalysis questions: 1. Do ESMFold and AF2 agree on the overall fold? 2. Where are the major differences (if any)? 3. Which one looks closer to the native structure? 4. Are differences in well-structured regions (beta-barrel) or loops?\n\n\n3.3 Calculate RMSD\nIn PyMOL, calculate RMSD for each comparison:\nUse the same approach you used in the AlphaFold2 module. If you’re feeling confident, try writing a PyMOL script to automate this!\n# In PyMOL:\nrms_print af2, native\nrms_print esmfold, native\nrms_print af2, esmfold\nRecord your results:\n\n\n\nComparison\nRMSD (Å)\nInterpretation\n\n\n\n\nAF2 → Native\n\nBetter accuracy = lower RMSD\n\n\nESMFold → Native\n\n\n\n\nAF2 ↔︎ ESMFold\n\nAgreement &lt; 2Å is good\n\n\n\nDiscussion points: - Which model is more accurate for GFP? - Do the two models agree with each other (RMSD &lt; 2Å)? - Given that both are trained on different data, what does their agreement (or disagreement) tell you?",
    "crumbs": [
      "Tuesday",
      "4. ESMFold vs. AlphaFold2"
    ]
  },
  {
    "objectID": "tuesday/4-esmfold.html#part-4-exploring-esmfold-features-optional---if-youre-ahead",
    "href": "tuesday/4-esmfold.html#part-4-exploring-esmfold-features-optional---if-youre-ahead",
    "title": "4. ESMFold vs. AlphaFold2",
    "section": "Part 4: Exploring ESMFold Features ⭐ OPTIONAL - If You’re Ahead",
    "text": "Part 4: Exploring ESMFold Features ⭐ OPTIONAL - If You’re Ahead\n⏰ Due to time constraints, this section is optional. Only do this if you finish the core activity early, or come back to it later if you want to explore ESMFold more deeply!\nGoal: Understand ESMFold-specific capabilities and parameters.\n\n4.1 Memory Optimization with Chunking\nESMFold supports chunking to reduce memory usage.\nLooking at the documentation, you’ll find model.set_chunk_size(size): - Reduces memory from O(L²) to O(L) - Useful for very long sequences - May be slightly slower\nModify your prediction script to test chunking:\n# Before inference:\nmodel.set_chunk_size(128)\nExperiment: 1. Run with no chunking 2. Run with chunk_size=128 3. Run with chunk_size=64\nQuestions: 1. Does chunking affect the output structure? 2. Does it affect runtime? 3. When would you use this?\n\n\n4.2 Understanding Model Determinism\nTest if ESMFold is deterministic:\nRun the same prediction twice:\npython predict_esmfold.py  # Run 1\nmv output.pdb output1.pdb\n\npython predict_esmfold.py  # Run 2\nmv output.pdb output2.pdb\nCompare in PyMOL:\nExpected result: RMSD = 0.0 Å (identical)\nThis tells us: - ESMFold is deterministic (same input → same output) - Unlike some methods with dropout, it’s reproducible - You can’t generate diversity by running multiple times\nCompare to AF2: - AF2 gives 5 different predictions (different model weights) - Each can be slightly different - Provides ensemble diversity\nDiscussion: - Advantages of determinism? - Disadvantages (no ensemble diversity)? - How do you assess uncertainty with a single prediction?",
    "crumbs": [
      "Tuesday",
      "4. ESMFold vs. AlphaFold2"
    ]
  },
  {
    "objectID": "tuesday/4-esmfold.html#part-5-speed-comparison---esmfold-vs-alphafold2",
    "href": "tuesday/4-esmfold.html#part-5-speed-comparison---esmfold-vs-alphafold2",
    "title": "4. ESMFold vs. AlphaFold2",
    "section": "Part 5: Speed Comparison - ESMFold vs AlphaFold2",
    "text": "Part 5: Speed Comparison - ESMFold vs AlphaFold2\nGoal: Understand the practical speed differences between the two approaches.\n\n5.1 Compare Prediction Times\nYou already timed your ESMFold prediction in Part 1. Now compare it to your ColabFold run.\nThink back to your AlphaFold2 predictions: - How long did the MSA search take? - How long did the actual structure prediction take (for 5 models)? - What was the total time?\nFill in your observations:\n\n\n\n\n\n\n\n\nModel\nApproximate Time\nNotes\n\n\n\n\nESMFold\n_____ seconds/minutes\nFrom Part 1 output\n\n\nColabFold MSA search\n_____ minutes\nFrom AlphaFold2 module\n\n\nColabFold prediction (5 models)\n_____ minutes\nFrom AlphaFold2 module\n\n\nTotal ColabFold\n_____ minutes\n\n\n\n\nKey observations: 1. ESMFold is typically 10-60x faster than full AlphaFold2 pipelines 2. Most of AF2’s time is spent on MSA generation (searching sequence databases) 3. ESMFold skips this entirely - it’s trained to predict from sequence alone\n\n\n5.2 When Does Speed Matter?\nConsider these scenarios and think about which model you’d choose:\nScenario 1: Screening 1,000 designed proteins for a protein engineering project - ESMFold could finish in hours - AF2 could take days or weeks - Which would you use first? Why?\nScenario 2: Final structure for an important publication - You need the highest accuracy possible - You have time for validation - Which would you use? Would you run both?\nScenario 3: Analyzing 50 point mutations of a therapeutic antibody - Need quick results to guide next experiments - Medium accuracy is acceptable for initial screening - Your strategy?\nRemember: In practice, many researchers use ESMFold for fast screening, then validate interesting hits with AlphaFold2 for higher accuracy.",
    "crumbs": [
      "Tuesday",
      "4. ESMFold vs. AlphaFold2"
    ]
  },
  {
    "objectID": "tuesday/4-esmfold.html#part-6-choosing-the-right-model-for-your-research",
    "href": "tuesday/4-esmfold.html#part-6-choosing-the-right-model-for-your-research",
    "title": "4. ESMFold vs. AlphaFold2",
    "section": "Part 6: Choosing the Right Model for Your Research",
    "text": "Part 6: Choosing the Right Model for Your Research\nGoal: Develop intuition for when to use ESMFold vs AlphaFold2.\n\n6.1 Key Differences Summary\n\n\n\n\n\n\n\n\nAspect\nESMFold\nAlphaFold2\n\n\n\n\nTraining approach\nLanguage model on sequences\nStructure prediction with MSA+templates\n\n\nInput required\nSequence only\nSequence (MSA search automatic)\n\n\nSpeed\nVery fast (seconds-minutes)\nSlower (minutes-hours with MSA)\n\n\nNumber of predictions\n1 (deterministic)\n5 (different model weights)\n\n\nBest for\nFast screening, novel proteins\nHigh-stakes accuracy, well-studied proteins\n\n\nNeeds homologs?\nNo\nPerforms better with many homologs\n\n\n\n\n\n6.2 Decision Guide\nUse this guide to help choose the right tool:\nSTART: I need to predict a protein structure\n    ↓\nQ: Is speed critical? (screening many sequences, quick answer needed)\n    YES → Use ESMFold first\n    NO ↓\n\nQ: Is this a designed/synthetic protein with no natural homologs?\n    YES → Use ESMFold (AF2 needs homologs to work well)\n    NO ↓\n\nQ: Is this high-stakes? (publication, drug design, experimental planning)\n    YES → Run BOTH models and compare\n          - Builds confidence if they agree\n          - Reveals uncertainty if they disagree\n    NO ↓\n\nQ: Do I need uncertainty estimates from multiple predictions?\n    YES → Use AlphaFold2 (5 models provide ensemble diversity)\n    NO → Use ESMFold (faster, single high-quality prediction)\n\n\n6.3 Practical Scenarios - Test Your Understanding\nFor each scenario below, think about which model(s) you would use and why:\nScenario 1: Screening 5,000 designed protein variants for stability - Consider: Time constraints, cost, accuracy needs - Your choice: _______ - Why? _______\nScenario 2: Structure for molecular replacement in X-ray crystallography - Consider: Accuracy is critical, you need the best possible model - Your choice: _______ - Why? _______\nScenario 3: Novel metagenomic protein (no known homologs) - Consider: No evolutionary information available - Your choice: _______ - Why? _______\nScenario 4: Well-studied enzyme with 100+ homologs in PDB - Consider: Rich evolutionary information available - Your choice: _______ - Why? _______\nScenario 5: 200 point mutations to assess effect on protein binding - Consider: Need balance between throughput and accuracy - Your choice: _______ - Why? _______\n\n\n6.4 What If the Models Disagree?\nIf you run both ESMFold and AlphaFold2 and they give different structures:\nStep 1: Check confidence scores - Which model has higher average pLDDT? - Where specifically do they disagree (structured regions or flexible loops)?\nStep 2: Understand the disagreement - Small differences (&lt;2Å RMSD): Likely both correct, minor variations - Large differences (&gt;4Å RMSD): Real uncertainty - investigate further\nStep 3: What to do about it - If ESMFold has higher confidence → May indicate AF2 lacks good templates - If AF2 has higher confidence → Evolutionary information may be providing key insights - If both have low confidence in same region → Likely disordered or genuinely uncertain - When in doubt: Use predictions to design experiments, not as final answers\nRemember: Predictions are computational hypotheses. They guide experiments but don’t replace them!",
    "crumbs": [
      "Tuesday",
      "4. ESMFold vs. AlphaFold2"
    ]
  },
  {
    "objectID": "tuesday/4-esmfold.html#part-7-key-takeaways-best-practices",
    "href": "tuesday/4-esmfold.html#part-7-key-takeaways-best-practices",
    "title": "4. ESMFold vs. AlphaFold2",
    "section": "Part 7: Key Takeaways & Best Practices",
    "text": "Part 7: Key Takeaways & Best Practices\n\n7.1 What You’ve Learned\nESMFold: ✅ Very fast (no MSA generation needed) ✅ Works on designed/orphan proteins with no homologs ✅ Simple, streamlined workflow ✅ Great for high-throughput screening\n⚠️ Single prediction (no ensemble diversity) ⚠️ May miss evolutionary insights ⚠️ Less extensively validated than AF2\nAlphaFold2: ✅ Leverages evolutionary information (MSAs) ✅ 5-model ensemble for uncertainty estimation ✅ Extensively validated, highly accurate ✅ Best performance on proteins with many homologs\n⚠️ Slower (MSA search takes time) ⚠️ Needs homologs for best performance ⚠️ Higher computational cost\n\n\n7.2 Best Practices for Structure Prediction\nAlways do this: 1. ✅ Check pLDDT confidence scores - don’t trust low-confidence regions blindly 2. ✅ Inspect low-confidence regions carefully (may be disordered or uncertain) 3. ✅ Consider biological context (is this region expected to be structured?) 4. ✅ Validate critical predictions experimentally when possible\nSmart strategies: 1. 💡 Use ESMFold for rapid screening → follow up interesting hits with AF2 2. 💡 For high-stakes work, run both models and compare 3. 💡 When models disagree, use that as a signal to investigate further 4. 💡 Low pLDDT doesn’t always mean “wrong” - might indicate genuine flexibility\n\n\n7.3 Recommended Workflow for Most Projects\nNew protein structure question\n    ↓\nStart with ESMFold (fast initial prediction)\n    ↓\nCheck pLDDT scores\n    ↓\nHigh confidence (&gt;70) overall? ──NO→ Run AlphaFold2\n    ↓                                 (may need evolutionary info)\n    YES                               Compare results\n    ↓\nIs this critical? ──YES→ Validate with AlphaFold2\n    ↓                    (get ensemble diversity)\n    NO\n    ↓\nUse ESMFold prediction\n(fast, good enough for most purposes)\nThe bottom line: There’s no single “best” model. Choose based on your specific needs for speed, accuracy, and the biological context of your protein!",
    "crumbs": [
      "Tuesday",
      "4. ESMFold vs. AlphaFold2"
    ]
  },
  {
    "objectID": "tuesday/4-esmfold.html#additional-resources",
    "href": "tuesday/4-esmfold.html#additional-resources",
    "title": "4. ESMFold vs. AlphaFold2",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nDocumentation\n\nESM GitHub: https://github.com/facebookresearch/esm\nESMFold paper: Lin et al. (2023) Science 379:1123-1130\n\n\n\nOnline Resources\n\nESMFold web server: https://esmatlas.com/resources?action=fold\nAlphaFold Database: https://alphafold.ebi.ac.uk/\nRCSB PDB: https://www.rcsb.org/",
    "crumbs": [
      "Tuesday",
      "4. ESMFold vs. AlphaFold2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "",
    "text": "Welcome back! You were last on:   Resume where you left off\nWelcome to the ML Protein Design Bootcamp 2025! This self-paced course covers machine learning tools for protein structure prediction and design."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "Course Overview",
    "text": "Course Overview\nThis bootcamp is organized into 4 days of content plus a capstone project. Each day contains sequential modules that build on each other.\n\n\nMonday\nTool Installation: Install essential ML protein design tools on your HPC cluster including LocalColabFold, ESMFold, LigandMPNN, RFdiffusion2, Chai-1, Boltz-2, and BindCraft.\n\n\nTuesday\nStructure Prediction: Learn protein visualization with PyMOL, predict structures with AlphaFold2, and compare prediction methods (ESMFold vs AF2) to understand their trade-offs.\n\n\nWednesday\nAdvanced Prediction & Design: Compare AlphaFold3 and Chai-1 on complex predictions, then design proteins with RFdiffusion including binders and symmetric oligomers.\n\n\nThursday\nComing soon\nModules for Thursday.\n\n\nCapstone\nCapstone project bringing together everything you’ve learned."
  },
  {
    "objectID": "index.html#your-progress",
    "href": "index.html#your-progress",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "Your Progress",
    "text": "Your Progress\n\n\nCompleted modules: 0\n\n\n&lt;div id=\"progress-bar\" class=\"progress-bar\" role=\"progressbar\" style=\"width: 0%;\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"&gt;0%&lt;/div&gt;\n\n\n\nReset Progress"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "ML Protein Design Bootcamp 2025",
    "section": "Getting Started",
    "text": "Getting Started\n\nChoose a day from the navigation above or the cards below\nWork through modules in order - each builds on the previous\nCheck off sections as you complete them - your progress is saved automatically\nReturn anytime - use the “Resume” button to pick up where you left off\n\n\nReport an Issue / Ask a Question (GitHub account required)"
  },
  {
    "objectID": "wednesday/1-rfdiffusion.html",
    "href": "wednesday/1-rfdiffusion.html",
    "title": "2. RFdiffusion: Advanced Design",
    "section": "",
    "text": "Setup for the activity.\n\nGo to the RFdiffusion installation and make a new directory for the activity: mkdir activity.\n\nGenerate unconditional monomers.\n\nMake new folder for this example: cd activity; mkdir 1_uncond; cd 1_uncond\nCopy design_unconditional.sh here: cp ../../examples/design_unconditional.sh .\nEdit the design script by using 1) 'contigmap.contigs=[75-150]', 2) inference.num_designs=5 , 3) ../../scripts/run_inference.py.\nRun the script: bash design_unconditional.sh\n\nWhat did the script create?\nLook at the structures. What are kinds of topologies did you get?\n\n\nGenerate monomers that scaffold a motif.\n\nMake new folder for this example: cd activity; mkdir 2_motifs; cd 2_motifs\nCopy design_motifscaffolding.sh here: cp ../../examples/design_motifscaffolding.sh .\nFor this example, we’ll be scaffolding a site from RSV-F protein. Let’s copy that .pdb here: cp ../../examples/input_pdbs/5TPN.pdb .\nEdit the design script by using 1) inference.input_pdb=5TPN.pdb, 2) 'contigmap.contigs=[10-40/A163-181/10-40]', 3) inference.num_designs=5 , 4) ../../scripts/run_inference.py.\n\nThe backbone we’re generating will have 10-40 residues (randomly sampled), the the motif residues 163-181 (inclusive) on chain A of the input, then 10-40 residues (randomly sampled).\n\nRun the script: bash design_motifscaffolding.sh\n\nLook at the structures. How well is the motif scaffolded?\n\n\nGenerate partially diffused structures.\n\nMake new folder for this example: cd activity; mkdir 3_partial; cd 3_partial\nCopy design_partialdiffusion.sh here: cp ../../examples/design_partialdiffusion.sh .\nFor this example, we’ll be partially noising and denoising a 79 residue protein 2KL8. Let’s copy that .pdb here: cp ../../examples/input_pdbs/2KL8.pdb .\nEdit the design script by using 1) inference.input_pdb=2KL8.pdb, 2) 'contigmap.contigs=[79-79]’, and 3) inference.num_designs=5.\n\nHere we’re generating diversity around a particular fold by noising and denoising 10 steps (20% of the full trajectory). We’re adding noise to the entire structure (all 79 residues), but part of the structure can also be held fixed.\n\nRun the script: bash design_partialdiffusion.sh\n\nLook at the structures. How similar are the outputs to the original structure?\n\n\nGenerate binders with hotspots.\n\nMake new folder for this example: cd activity; mkdir 4_hotspot; cd 4_hotspot\nCopy design_ppi.sh here: cp ../../examples/design_ppi.sh .\nFor this example, we’ll be designing binders to insulin receptor. Let’s copy that .pdb here: cp ../../examples/input_pdbs/insulin_target.pdb .\nEdit the design script by using 1) inference.input_pdb=insulin_target.pdb, 2) 'contigmap.contigs=[A1-150/0 70-100]’, 3) ’ppi.hotspot_res=[A59,A83,A91]’, 4) inference.num_designs=5, 5) denoiser.noise_scale_ca=0, and 6) denoiser.noise_scale_frame=0.\n\nHere, we’re designing binders to insulin receptor. The contig describes the protein we want: residues 1-150 of the A chain of the receptor, a chainbreak (we don’t want to fuse the binder and target!), a 70-100 residue binder to be diffused. We also tell diffusion to target three specific residues, specifically residues 59, 83 and 91 of chain A. Finally, we reduce the noise added during inference to 0 to improve the quality of the designs.\n\nRun the script: bash design_ppi.sh\n\nLook at the structures. What are the topologies of your binders?\n\n\nGenerate fold-conditioned structures.\n\nMake new folder for this example: cd activity; mkdir 5_fold_cond; cd 5_fold_cond\nCopy design_timbarrel.sh here: cp ../../examples/design_timbarrel.sh .\nFor this example, we’ll be diffusing a TIM barrel by providing course-grained specification of the fold. Let’s copy that fold information here: cp -r ../../examples/tim_barrel_scaffold/ .\n\nWhat do the files in this folder represent?\n\nEdit the design script by using inference.num_designs=5.\n\nHere, we’re making a TIM barrel by providing course-grained specification of the fold. We specify the output path. We tell RFdiffusion that we want to do scaffoldguided design, and that we are not making a binder to a target (just a monomer). We provide a path to a directory of TIM barrel scaffolds, generated with the helper script helper_scripts/make_secstruc_adj.py. We generate 5 designs, with a reduced noise scale during inference of 0.5. We also sample additional length to increase diversity of the outputs by masking the loops and inserting 0-5 residues into each loop. We also add 0-5 residues to the N and the C-terminus. These allow use to sample additional diversity in our designs.\nYou may need to edit rfdiffusion/inference/model_runners.py like 751 to self.blockadjacency = iu.BlockAdjacency(conf, conf.inference.num_designs)\n\nRun the script: bash design_timbarrel.sh\n\nLook at the structures. How do these generations compare to the TIM barrel structure used for conditioning (6WVS)?\n\n\nGenerate oligomers of various symmetries.\n\nMake new folder for this example: cd activity; mkdir 6_symmetry; cd 6_symmetry\nCopy design_cyclic_oligos.sh, design_dihedral_oligos.sh, and design_tetrahedral_oligos.sh here: cp ../../examples/design_cyclic_oligos.sh ., cp ../../examples/design_dihedral_oligos.sh ., and cp ../../examples/design_tetrahedral_oligos.sh .\nGenerate cyclic oligomers:\n\nEdit the design_cyclic_oligos.sh script by changing 1) inference.symmetry=\"C4”, 2) inference.num_designs=5, 3) inference.output_prefix=\"example_outputs/C4_oligo”, and 4) 'contigmap.contigs=[200-200]’.\n\nIn this example, we generate 5 designs of C4 symmetric oligomers. For symmetrical diffusion, we need the symmetry config. We also apply an external potential to promote contacts both within (with a relative weight of 1) and between chains (relative weight 0.1). We specify that we want to apply these potentials to all chains, with a guide scale of 2.0 (a sensible starting point). We decay this potential with quadratic form, so that it is applied more strongly initially. We specify a total length of 200 aa, so each chain is 50 residues long.\n\nRun the script: bash design_cyclic_oligos.sh\n\nDo the structures have the desired symmetry?\nWhat topologies are found in the individual subunits?\n\n\nGenerate dihedral oligomers:\n\nEdit the design_dihedral_oligos.sh script by changing inference.num_designs=2.\n\nIn this example, we generate 2 D2 symmetric oligomers using the symmetry config. We also apply an external potential to promote contacts both within (with a relative weight of 1) and between chains (relative weight 0.1). We specify that we want to apply these potentials to all chains, with a guide scale of 2.0 (a sensible starting point). We decay this potential with quadratic form, so that it is applied more strongly initially. We specify a total length of 320 aa, so each chain is 80 residues long.\n\nRun the script: bash design_dihedral_oligos.sh\n\nDo the structures have the desired symmetry?\nWhat topologies are found in the individual subunits?\n\n\nGenerate tetrahedral oligomers:\n\nEdit the design_tetrahedral_oligos.sh script by changing 1) inference.num_designs=2, 2) 'contigmap.contigs=[720-720]’.\n\nIn this example, we generate tetrahedral symmetric oligomers. We use the symmetry config, and specify we want a tetrahedral oligomer, with 2 designs generated. We specify the output prefix, and also the potential we want to apply. This external potential promotes contacts both within (with a relative weight of 1) and between chains (relative weight 0.1). We specify that we want to apply these potentials to all chains, with a guide scale of 2.0 (a sensible starting point). We decay this potential with quadratic form, so that it is applied more strongly initially. We specify a total length of 720 aa, so each chain is 60 residues long\n\nRun the script: bash design_tetrahedral_oligos.sh\n\nDo the structures have the desired symmetry?\nWhat topologies are found in the individual subunits?",
    "crumbs": [
      "Wednesday",
      "2. RFdiffusion: Advanced Design"
    ]
  },
  {
    "objectID": "wednesday/1-rfdiffusion.html#hands-on-exercise",
    "href": "wednesday/1-rfdiffusion.html#hands-on-exercise",
    "title": "2. RFdiffusion: Advanced Design",
    "section": "",
    "text": "Setup for the activity.\n\nGo to the RFdiffusion installation and make a new directory for the activity: mkdir activity.\n\nGenerate unconditional monomers.\n\nMake new folder for this example: cd activity; mkdir 1_uncond; cd 1_uncond\nCopy design_unconditional.sh here: cp ../../examples/design_unconditional.sh .\nEdit the design script by using 1) 'contigmap.contigs=[75-150]', 2) inference.num_designs=5 , 3) ../../scripts/run_inference.py.\nRun the script: bash design_unconditional.sh\n\nWhat did the script create?\nLook at the structures. What are kinds of topologies did you get?\n\n\nGenerate monomers that scaffold a motif.\n\nMake new folder for this example: cd activity; mkdir 2_motifs; cd 2_motifs\nCopy design_motifscaffolding.sh here: cp ../../examples/design_motifscaffolding.sh .\nFor this example, we’ll be scaffolding a site from RSV-F protein. Let’s copy that .pdb here: cp ../../examples/input_pdbs/5TPN.pdb .\nEdit the design script by using 1) inference.input_pdb=5TPN.pdb, 2) 'contigmap.contigs=[10-40/A163-181/10-40]', 3) inference.num_designs=5 , 4) ../../scripts/run_inference.py.\n\nThe backbone we’re generating will have 10-40 residues (randomly sampled), the the motif residues 163-181 (inclusive) on chain A of the input, then 10-40 residues (randomly sampled).\n\nRun the script: bash design_motifscaffolding.sh\n\nLook at the structures. How well is the motif scaffolded?\n\n\nGenerate partially diffused structures.\n\nMake new folder for this example: cd activity; mkdir 3_partial; cd 3_partial\nCopy design_partialdiffusion.sh here: cp ../../examples/design_partialdiffusion.sh .\nFor this example, we’ll be partially noising and denoising a 79 residue protein 2KL8. Let’s copy that .pdb here: cp ../../examples/input_pdbs/2KL8.pdb .\nEdit the design script by using 1) inference.input_pdb=2KL8.pdb, 2) 'contigmap.contigs=[79-79]’, and 3) inference.num_designs=5.\n\nHere we’re generating diversity around a particular fold by noising and denoising 10 steps (20% of the full trajectory). We’re adding noise to the entire structure (all 79 residues), but part of the structure can also be held fixed.\n\nRun the script: bash design_partialdiffusion.sh\n\nLook at the structures. How similar are the outputs to the original structure?\n\n\nGenerate binders with hotspots.\n\nMake new folder for this example: cd activity; mkdir 4_hotspot; cd 4_hotspot\nCopy design_ppi.sh here: cp ../../examples/design_ppi.sh .\nFor this example, we’ll be designing binders to insulin receptor. Let’s copy that .pdb here: cp ../../examples/input_pdbs/insulin_target.pdb .\nEdit the design script by using 1) inference.input_pdb=insulin_target.pdb, 2) 'contigmap.contigs=[A1-150/0 70-100]’, 3) ’ppi.hotspot_res=[A59,A83,A91]’, 4) inference.num_designs=5, 5) denoiser.noise_scale_ca=0, and 6) denoiser.noise_scale_frame=0.\n\nHere, we’re designing binders to insulin receptor. The contig describes the protein we want: residues 1-150 of the A chain of the receptor, a chainbreak (we don’t want to fuse the binder and target!), a 70-100 residue binder to be diffused. We also tell diffusion to target three specific residues, specifically residues 59, 83 and 91 of chain A. Finally, we reduce the noise added during inference to 0 to improve the quality of the designs.\n\nRun the script: bash design_ppi.sh\n\nLook at the structures. What are the topologies of your binders?\n\n\nGenerate fold-conditioned structures.\n\nMake new folder for this example: cd activity; mkdir 5_fold_cond; cd 5_fold_cond\nCopy design_timbarrel.sh here: cp ../../examples/design_timbarrel.sh .\nFor this example, we’ll be diffusing a TIM barrel by providing course-grained specification of the fold. Let’s copy that fold information here: cp -r ../../examples/tim_barrel_scaffold/ .\n\nWhat do the files in this folder represent?\n\nEdit the design script by using inference.num_designs=5.\n\nHere, we’re making a TIM barrel by providing course-grained specification of the fold. We specify the output path. We tell RFdiffusion that we want to do scaffoldguided design, and that we are not making a binder to a target (just a monomer). We provide a path to a directory of TIM barrel scaffolds, generated with the helper script helper_scripts/make_secstruc_adj.py. We generate 5 designs, with a reduced noise scale during inference of 0.5. We also sample additional length to increase diversity of the outputs by masking the loops and inserting 0-5 residues into each loop. We also add 0-5 residues to the N and the C-terminus. These allow use to sample additional diversity in our designs.\nYou may need to edit rfdiffusion/inference/model_runners.py like 751 to self.blockadjacency = iu.BlockAdjacency(conf, conf.inference.num_designs)\n\nRun the script: bash design_timbarrel.sh\n\nLook at the structures. How do these generations compare to the TIM barrel structure used for conditioning (6WVS)?\n\n\nGenerate oligomers of various symmetries.\n\nMake new folder for this example: cd activity; mkdir 6_symmetry; cd 6_symmetry\nCopy design_cyclic_oligos.sh, design_dihedral_oligos.sh, and design_tetrahedral_oligos.sh here: cp ../../examples/design_cyclic_oligos.sh ., cp ../../examples/design_dihedral_oligos.sh ., and cp ../../examples/design_tetrahedral_oligos.sh .\nGenerate cyclic oligomers:\n\nEdit the design_cyclic_oligos.sh script by changing 1) inference.symmetry=\"C4”, 2) inference.num_designs=5, 3) inference.output_prefix=\"example_outputs/C4_oligo”, and 4) 'contigmap.contigs=[200-200]’.\n\nIn this example, we generate 5 designs of C4 symmetric oligomers. For symmetrical diffusion, we need the symmetry config. We also apply an external potential to promote contacts both within (with a relative weight of 1) and between chains (relative weight 0.1). We specify that we want to apply these potentials to all chains, with a guide scale of 2.0 (a sensible starting point). We decay this potential with quadratic form, so that it is applied more strongly initially. We specify a total length of 200 aa, so each chain is 50 residues long.\n\nRun the script: bash design_cyclic_oligos.sh\n\nDo the structures have the desired symmetry?\nWhat topologies are found in the individual subunits?\n\n\nGenerate dihedral oligomers:\n\nEdit the design_dihedral_oligos.sh script by changing inference.num_designs=2.\n\nIn this example, we generate 2 D2 symmetric oligomers using the symmetry config. We also apply an external potential to promote contacts both within (with a relative weight of 1) and between chains (relative weight 0.1). We specify that we want to apply these potentials to all chains, with a guide scale of 2.0 (a sensible starting point). We decay this potential with quadratic form, so that it is applied more strongly initially. We specify a total length of 320 aa, so each chain is 80 residues long.\n\nRun the script: bash design_dihedral_oligos.sh\n\nDo the structures have the desired symmetry?\nWhat topologies are found in the individual subunits?\n\n\nGenerate tetrahedral oligomers:\n\nEdit the design_tetrahedral_oligos.sh script by changing 1) inference.num_designs=2, 2) 'contigmap.contigs=[720-720]’.\n\nIn this example, we generate tetrahedral symmetric oligomers. We use the symmetry config, and specify we want a tetrahedral oligomer, with 2 designs generated. We specify the output prefix, and also the potential we want to apply. This external potential promotes contacts both within (with a relative weight of 1) and between chains (relative weight 0.1). We specify that we want to apply these potentials to all chains, with a guide scale of 2.0 (a sensible starting point). We decay this potential with quadratic form, so that it is applied more strongly initially. We specify a total length of 720 aa, so each chain is 60 residues long\n\nRun the script: bash design_tetrahedral_oligos.sh\n\nDo the structures have the desired symmetry?\nWhat topologies are found in the individual subunits?",
    "crumbs": [
      "Wednesday",
      "2. RFdiffusion: Advanced Design"
    ]
  },
  {
    "objectID": "wednesday/1-rfdiffusion.html#independent-project",
    "href": "wednesday/1-rfdiffusion.html#independent-project",
    "title": "2. RFdiffusion: Advanced Design",
    "section": "Independent Project",
    "text": "Independent Project\n(Use your target protein)\n\nMilestone 1: Target Preparation & Initial Exploration\n\nSet up your project directory structure\nDownload and inspect your assigned target PDB\nIdentify the binding surface and hotspot residues from the table below\nRun 2-3 test designs to verify your setup is working correctly\n\n\n\nMilestone 2: Unconditional Binder Generation\n\nGenerate at least 10 unconditional binders of length 70-100 towards your target protein.\n\nIs there a particular epitope on the target that RFdiffusion prefers?\nIs there a particular binder topology that RFdiffusion prefers?\n\n\n\n\nMilestone 3: Hotspot-Guided Binder Design\n\nEach of the targets that were assigned were previously used in binder design experiments (1, 2). These experiments made use of hotspots located on the target proteins. Generate at least 10 binders towards your target protein using the hotspots below.\n\nIs there a particular binder topology that RFdiffusion prefers to generate?\n\n\n\n\n\nTarget\nUniProt ID\nPDB ID\nHotspot Residues (chain and residue index from PDB)\n\n\n\n\nPD-L1\nQ9NZQ7\n5O45\nA56, A115, A123\n\n\nIL-7Rɑ\nP16871\n3DI3\nB58, B80, B139\n\n\nTrkA receptor\nP04629\n1WWW\nX294, X296, X333\n\n\nIFNAR2\nP48551\n2LAG\nB52, B82, B98\n\n\nBet v 1-A\nP15494\n1BV1\nA24, A28, A43\n\n\n\n\n\nMilestone 4: Potential Optimization\n\nPotentials can be powerful ways to bias the generation process. Try at least 3 different combinations of potentials, generating 5 backbones with each. Be sure to use hotspots for these generations too!\n\nHow did the potentials change your outputs?\nDid you find a particularly useful configuration of potentials?\n\n\n\n\nMilestone 5: Analysis & Selection\n\nCompare your unconditional vs. hotspot-guided vs. potential-optimized designs\nIdentify your top 3-5 most promising binder designs\nDocument what makes these designs stand out (binding pose, topology, contact with hotspots, etc.)",
    "crumbs": [
      "Wednesday",
      "2. RFdiffusion: Advanced Design"
    ]
  },
  {
    "objectID": "wednesday/1-rfdiffusion.html#troubleshooting-guide",
    "href": "wednesday/1-rfdiffusion.html#troubleshooting-guide",
    "title": "2. RFdiffusion: Advanced Design",
    "section": "Troubleshooting Guide",
    "text": "Troubleshooting Guide\n\nCommon Issues & Solutions\n\n\n\n\n\n\n\n\nProblem\nLikely Cause\nSolution\n\n\n\n\n“CUDA out of memory”\nGPU memory exhausted\nReduce inference.num_designs to 1-2, or design smaller proteins\n\n\nScript hangs at “Initializing model”\nMissing model weights or incorrect paths\nVerify RFdiffusion installation and model checkpoint paths\n\n\n“FileNotFoundError” for input PDB\nIncorrect file path or missing file\nCheck that PDB file exists in current directory with ls *.pdb\n\n\nDesigns look extended/unfolded\nInsufficient denoising or inappropriate settings\nCheck inference.num_steps (should be ~50), verify contig syntax\n\n\nBinders don’t contact target\nIncorrect contig specification or chainbreak\nVerify /0 chainbreak in contigs, check residue numbering in target PDB\n\n\nBinders don’t contact hotspots\nHotspot residues too far apart or incorrect chain ID\nVerify chain IDs and residue numbers match your PDB file exactly\n\n\n“ImportError” or module not found\nConda environment not activated\nActivate RFdiffusion environment: conda activate RFdiffusion (or appropriate env name)\n\n\nScript runs but produces no outputs\nOutput directory doesn’t exist or permissions issue\nCheck that output directory exists, verify write permissions\n\n\nSymmetric oligomers don’t look symmetric\nIncorrect symmetry specification or potentials\nVerify symmetry string (e.g., “C4”, “D2”, “T”), check potential settings\n\n\n“BlockAdjacency” error in fold-conditioned\nKnown bug in model_runners.py\nEdit rfdiffusion/inference/model_runners.py line ~751 as noted in activity\n\n\nVery slow generation times\nNormal for large/complex designs\nTetrahedral oligomers can take 30-60 min. Consider using screen or tmux\n\n\nAll designs look very similar\nInsufficient diversity sampling\nIncrease contig length ranges (e.g., [60-100] instead of [80-80]), adjust noise scales\n\n\n\n\n\nTips for Success\n\nAlways check your PDB file first: Use PyMOL or ChimeraX to verify chain IDs and residue numbering before running\nStart small: Test with num_designs=1 first to verify your setup works\nSave your commands: Keep a log of successful parameter combinations\nUse descriptive output names: Include key parameters in output_prefix (e.g., hotspot_A59_A83_A91)\nCheck the logs: RFdiffusion creates log files - read them if something goes wrong",
    "crumbs": [
      "Wednesday",
      "2. RFdiffusion: Advanced Design"
    ]
  },
  {
    "objectID": "wednesday/index.html",
    "href": "wednesday/index.html",
    "title": "Wednesday",
    "section": "",
    "text": "Wednesday focuses on advanced structure prediction and protein design. You’ll compare AlphaFold3 and Chai-1 for complex predictions (multimers, ligands, ions), then dive into RFdiffusion for de novo protein design including binder generation and symmetric oligomers.",
    "crumbs": [
      "Wednesday"
    ]
  },
  {
    "objectID": "wednesday/index.html#overview",
    "href": "wednesday/index.html#overview",
    "title": "Wednesday",
    "section": "",
    "text": "Wednesday focuses on advanced structure prediction and protein design. You’ll compare AlphaFold3 and Chai-1 for complex predictions (multimers, ligands, ions), then dive into RFdiffusion for de novo protein design including binder generation and symmetric oligomers.",
    "crumbs": [
      "Wednesday"
    ]
  },
  {
    "objectID": "wednesday/index.html#modules",
    "href": "wednesday/index.html#modules",
    "title": "Wednesday",
    "section": "Modules",
    "text": "Modules\n\n\n\nModule\nTopic\nDescription\n\n\n\n\n1. AlphaFold3 vs. Chai-1\nComparing Next-Gen Predictors\nCompare AF3 and Chai-1 on monomers, multimers, and protein-ligand complexes\n\n\n2. RFdiffusion\nAdvanced Protein Design\nDesign unconditional monomers, motif scaffolds, binders, and symmetric oligomers using RFdiffusion\n\n\n\n\n\n\n← Tuesday\n\n\nBack to Home\n\n\nThursday →",
    "crumbs": [
      "Wednesday"
    ]
  },
  {
    "objectID": "wednesday/0-alphafold3.html",
    "href": "wednesday/0-alphafold3.html",
    "title": "1. AlphaFold3 vs. Chai-1",
    "section": "",
    "text": "Setup for the activity\n\nIn order to make a prediction with the AlphaFold server, you’ll need a Google account to sign in.\nIn order to make a prediction with Chai-Lab, you’ll need to create a free account.\n\nMake a monomer prediction for 1QYS with both AF3 and Chai-Lab.\n\nAlphaFold 3:\n\nPaste in the sequence for 1QYS: MGDIQVQVNIDDNGKNFDYTYTVTTESELQKVLNELMDYIKKQGAKRVRISITARTKKEAEKFAAILIKVFAELGYNDINVTFDGDTVTVEGQLEGGSLEHHHHHH\nPress the “continue and preview job” button.\nPress “confirm and submit job”.\n\nChai-Lab:\n\nPaste in the same sequence for 1QYS.\nClick “Run Job” to submit the prediction.\n\nAnalysis:\n\nHow do the predicted structures compare to the native?\nHow does the AF3 prediction compare to Chai-Lab?\nHow confident is each model about its prediction?\n\n\nMake a multimer prediction for 8AJY with both AF3 and Chai-Lab.\n\nAlphaFold 3:\n\nPaste in the sequence for 8AJY_1 (chain A): MLTDRGMTYDLDPKDGSSAATKPVLEVTKKVFDTAADAAGQTVTVEFKVSGAEGKYATTGYHIYWDERLEVVATKTGAYAKKGAALEDSSLAKAENNGNGVFVASGADDDFGADGVMWTVELKVPADAKAGDVYPIDVAYQWDPSKGDLFTDNKDSAQGKLMQAYFFTQGIKSSSNPSTDEYLVKANATYADGYIAIKAGEPE\nClick “add entity”, make sure the molecule type to “protein”, and paste in the sequence of 8AJY_2 (chain B): MGSSHHHHHHSSGLVPRGSHMASKPQYRLGDVDFNGIIDGRDATAVLTEYARISTGKPAEFVGNTALAADVNKDNMIDAADATHILTYYAISSTRDDITSDDYFALHQPLRG\nThe native structure was originally solved in the presence of 3 Ca2+ ions. Click “add entity”, change the molecule type to “ion”, and select Ca2+ to add each.\nPress the “continue and preview job” button.\nPress “confirm and submit job”.\n\nChai-Lab:\n\nPaste in the sequence for 8AJY_1 (chain A).\nClick “Add Chain” and paste in the sequence for 8AJY_2 (chain B).\nClick “Add Ligand” and select “Ca2+” from the ion options. Repeat to add all 3 Ca2+ ions.\nClick “Run Job” to submit the prediction.\n\nAnalysis:\n\nHow do the predicted structures compare to the native?\nHow does the AF3 prediction compare to Chai-Lab?\nHow confident is each model about its prediction?\nWhat about the placement of the ions and the coordinating side chains?\n\n\nMake a protein-ligand prediction with Chai-Lab for 2JIE (beta-glucosidase with 2-fluoro-glucose).\n\nChai-Lab:\n\nPaste in the sequence for 2JIE (beta-glucosidase B): MHHHHHHSENTFIFPATFMWGTSTSSYQIEGGTDEGGRTPSIWDTFCQIPGKVIGGDCGDVACDHFHHFKEDVQLMKQLGFLHYRFSVAWPRIMPAAGIINEEGLLFYEHLLDEIELAGLIPMLTLYHWDLPQWIEDEGGWTQRETIQHFKTYASVIMDRFGERINWWNTINEPYCASILGYGTGEHAPGHENWREAFTAAHHILMCHGIASNLHKEKGLTGKIGITLNMEHVDAASERPEDVAAAIRRDGFINRWFAEPLFNGKYPEDMVEWYGTYLNGLDFVQPGDMELIQQPGDFLGINYYTRSIIRSTNDASLLQVEQVHMEEPVTDMGWEIHPESFYKLLTRIEKDFSKGLPILITENGAAMRDELVNGQIEDTGRHGYIEEHLKACHRFIEEGGQLKGYFVWSFLDNFEWAWGYSKRFGIVHINYETQERTPKQSALWFKQMMAKNGF\nClick “Add Ligand” and select “Custom SMILES”.\nEnter the SMILES string for 2-fluoro-glucose (G2F): OCC1OC(O)C(F)C(O)C1O\nClick “Run Job” to submit the prediction.\n\nAnalysis:\n\nHow does the predicted structure compare to the crystal structure (2JIE)?\nIs the ligand positioned correctly in the active site?\nHow well does Chai-Lab predict the protein-ligand interactions?\nWhat is the confidence score for the ligand placement?",
    "crumbs": [
      "Wednesday",
      "1. AlphaFold3 vs. Chai-1"
    ]
  },
  {
    "objectID": "wednesday/0-alphafold3.html#hands-on-exercise",
    "href": "wednesday/0-alphafold3.html#hands-on-exercise",
    "title": "1. AlphaFold3 vs. Chai-1",
    "section": "",
    "text": "Setup for the activity\n\nIn order to make a prediction with the AlphaFold server, you’ll need a Google account to sign in.\nIn order to make a prediction with Chai-Lab, you’ll need to create a free account.\n\nMake a monomer prediction for 1QYS with both AF3 and Chai-Lab.\n\nAlphaFold 3:\n\nPaste in the sequence for 1QYS: MGDIQVQVNIDDNGKNFDYTYTVTTESELQKVLNELMDYIKKQGAKRVRISITARTKKEAEKFAAILIKVFAELGYNDINVTFDGDTVTVEGQLEGGSLEHHHHHH\nPress the “continue and preview job” button.\nPress “confirm and submit job”.\n\nChai-Lab:\n\nPaste in the same sequence for 1QYS.\nClick “Run Job” to submit the prediction.\n\nAnalysis:\n\nHow do the predicted structures compare to the native?\nHow does the AF3 prediction compare to Chai-Lab?\nHow confident is each model about its prediction?\n\n\nMake a multimer prediction for 8AJY with both AF3 and Chai-Lab.\n\nAlphaFold 3:\n\nPaste in the sequence for 8AJY_1 (chain A): MLTDRGMTYDLDPKDGSSAATKPVLEVTKKVFDTAADAAGQTVTVEFKVSGAEGKYATTGYHIYWDERLEVVATKTGAYAKKGAALEDSSLAKAENNGNGVFVASGADDDFGADGVMWTVELKVPADAKAGDVYPIDVAYQWDPSKGDLFTDNKDSAQGKLMQAYFFTQGIKSSSNPSTDEYLVKANATYADGYIAIKAGEPE\nClick “add entity”, make sure the molecule type to “protein”, and paste in the sequence of 8AJY_2 (chain B): MGSSHHHHHHSSGLVPRGSHMASKPQYRLGDVDFNGIIDGRDATAVLTEYARISTGKPAEFVGNTALAADVNKDNMIDAADATHILTYYAISSTRDDITSDDYFALHQPLRG\nThe native structure was originally solved in the presence of 3 Ca2+ ions. Click “add entity”, change the molecule type to “ion”, and select Ca2+ to add each.\nPress the “continue and preview job” button.\nPress “confirm and submit job”.\n\nChai-Lab:\n\nPaste in the sequence for 8AJY_1 (chain A).\nClick “Add Chain” and paste in the sequence for 8AJY_2 (chain B).\nClick “Add Ligand” and select “Ca2+” from the ion options. Repeat to add all 3 Ca2+ ions.\nClick “Run Job” to submit the prediction.\n\nAnalysis:\n\nHow do the predicted structures compare to the native?\nHow does the AF3 prediction compare to Chai-Lab?\nHow confident is each model about its prediction?\nWhat about the placement of the ions and the coordinating side chains?\n\n\nMake a protein-ligand prediction with Chai-Lab for 2JIE (beta-glucosidase with 2-fluoro-glucose).\n\nChai-Lab:\n\nPaste in the sequence for 2JIE (beta-glucosidase B): MHHHHHHSENTFIFPATFMWGTSTSSYQIEGGTDEGGRTPSIWDTFCQIPGKVIGGDCGDVACDHFHHFKEDVQLMKQLGFLHYRFSVAWPRIMPAAGIINEEGLLFYEHLLDEIELAGLIPMLTLYHWDLPQWIEDEGGWTQRETIQHFKTYASVIMDRFGERINWWNTINEPYCASILGYGTGEHAPGHENWREAFTAAHHILMCHGIASNLHKEKGLTGKIGITLNMEHVDAASERPEDVAAAIRRDGFINRWFAEPLFNGKYPEDMVEWYGTYLNGLDFVQPGDMELIQQPGDFLGINYYTRSIIRSTNDASLLQVEQVHMEEPVTDMGWEIHPESFYKLLTRIEKDFSKGLPILITENGAAMRDELVNGQIEDTGRHGYIEEHLKACHRFIEEGGQLKGYFVWSFLDNFEWAWGYSKRFGIVHINYETQERTPKQSALWFKQMMAKNGF\nClick “Add Ligand” and select “Custom SMILES”.\nEnter the SMILES string for 2-fluoro-glucose (G2F): OCC1OC(O)C(F)C(O)C1O\nClick “Run Job” to submit the prediction.\n\nAnalysis:\n\nHow does the predicted structure compare to the crystal structure (2JIE)?\nIs the ligand positioned correctly in the active site?\nHow well does Chai-Lab predict the protein-ligand interactions?\nWhat is the confidence score for the ligand placement?",
    "crumbs": [
      "Wednesday",
      "1. AlphaFold3 vs. Chai-1"
    ]
  },
  {
    "objectID": "tuesday/index.html",
    "href": "tuesday/index.html",
    "title": "Tuesday",
    "section": "",
    "text": "Tuesday focuses on protein structure prediction using state-of-the-art tools. You’ll learn the theoretical foundations of protein folding, visualize structures with PyMOL, predict structures using AlphaFold2, and compare different prediction methods to understand their strengths and trade-offs.",
    "crumbs": [
      "Tuesday"
    ]
  },
  {
    "objectID": "tuesday/index.html#overview",
    "href": "tuesday/index.html#overview",
    "title": "Tuesday",
    "section": "",
    "text": "Tuesday focuses on protein structure prediction using state-of-the-art tools. You’ll learn the theoretical foundations of protein folding, visualize structures with PyMOL, predict structures using AlphaFold2, and compare different prediction methods to understand their strengths and trade-offs.",
    "crumbs": [
      "Tuesday"
    ]
  },
  {
    "objectID": "tuesday/index.html#modules",
    "href": "tuesday/index.html#modules",
    "title": "Tuesday",
    "section": "Modules",
    "text": "Modules\n\n\n\nModule\nTopic\nDescription\n\n\n\n\n1. PyMOL and VSCode\nVisualization & Development Tools\nLearn protein structure visualization with PyMOL and remote development with VSCode\n\n\n2. Structure Prediction\nFoundational Concepts\nUnderstand the protein folding problem, Anfinsen’s hypothesis, Levinthal’s paradox, and structure prediction metrics\n\n\n3. AlphaFold2\nAlphaFold2 & OpenFold\nDeep dive into AlphaFold2’s architecture, run ColabFold predictions, and interpret confidence scores\n\n\n4. ESMFold vs. AlphaFold2\nComparing Prediction Methods\nCompare ESMFold’s language model approach vs AF2’s MSA-based approach. Learn when to use each tool\n\n\n\n\n\n\n← Monday\n\n\nBack to Home\n\n\nWednesday →",
    "crumbs": [
      "Tuesday"
    ]
  },
  {
    "objectID": "tuesday/1-pymol.html",
    "href": "tuesday/1-pymol.html",
    "title": "1. PyMOL and Visual Studio Code",
    "section": "",
    "text": "This module introduces two essential tools for computational structural biology: PyMOL for molecular visualization and Visual Studio Code (VSCode) for code development and remote computing.",
    "crumbs": [
      "Tuesday",
      "1. PyMOL and Visual Studio Code"
    ]
  },
  {
    "objectID": "tuesday/1-pymol.html#slides",
    "href": "tuesday/1-pymol.html#slides",
    "title": "1. PyMOL and Visual Studio Code",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Tuesday",
      "1. PyMOL and Visual Studio Code"
    ]
  },
  {
    "objectID": "tuesday/1-pymol.html#part-1-pymol---molecular-visualization",
    "href": "tuesday/1-pymol.html#part-1-pymol---molecular-visualization",
    "title": "1. PyMOL and Visual Studio Code",
    "section": "Part 1: PyMOL - Molecular Visualization",
    "text": "Part 1: PyMOL - Molecular Visualization\n\nWhat is PyMOL?\nPyMOL is the industry-standard molecular visualization software used by structural biologists, computational chemists, and protein engineers worldwide. It’s built on Python under the hood, which means:\n\nCommands are Python-based: You can script complex visualization workflows\nExtensible: The PyMOL library allows you to add custom functionality\nReproducible: Scripts ensure your figures can be regenerated exactly\n\n\n\n\n\n\n\nTipInstallation Options\n\n\n\n\nDownload from: pymol.org\nConda installation: conda install -c schrodinger -c conda-forge pymol\n\nThe Conda installation is recommended for integration with Python workflows.\n\n\n\n\nWhy PyMOL Matters for AI/ML Protein Work\nIn the context of this bootcamp, PyMOL serves several critical functions:\n\nDiagnosing AI predictions: When AlphaFold2, ESMFold, or other tools generate structures, you need to visually assess whether the predictions are reasonable\nEvaluating designs: Tools like RFdiffusion and LigandMPNN produce protein designs—PyMOL lets you inspect binding pockets, interfaces, and overall fold quality\nExperimental planning: Before doing mutagenesis or other experiments, you can examine hydrogen bond networks, identify key residues, and plan mutations in silico\nPublication figures: High-quality structure figures are essential for papers and presentations\n\n\n\nNavigation and Views\nPyMOL’s interface has several key components you’ll use constantly:\nMouse Controls:\n\nLeft-click + drag: Rotate the molecule\nMiddle-click + drag: Translate (move) the molecule\nRight-click + drag: Zoom in/out\nScroll wheel: Also zooms\n\nThe Object Panel (right side): Every loaded structure appears here with five buttons: - A (Action): Hide, show, delete, rename objects - S (Show): Display as cartoon, sticks, surface, etc. - H (Hide): Hide specific representations - L (Label): Add text labels to atoms/residues - C (Color): Change colors\n\n\nInteractive Selections\nOne of PyMOL’s most powerful features is its selection system. You can select atoms, residues, chains, or entire objects using:\nClick selections: - Click on any atom to select it - Shift+click to add to selection - Ctrl+click to remove from selection\nSelection algebra (in the command line):\n# Select chain A\nselect chainA, chain A\n\n# Select residues 50-100\nselect loop, resi 50-100\n\n# Select all alpha carbons\nselect cas, name CA\n\n# Select residues within 5 Angstroms of a ligand\nselect binding_site, byres all within 5 of organic\n\n\nRepresentations (“Show As”)\nDifferent representations reveal different aspects of structure:\n\n\n\nRepresentation\nBest For\n\n\n\n\nCartoon\nOverall fold, secondary structure\n\n\nSticks\nActive sites, ligand interactions\n\n\nSurface\nBinding pockets, electrostatics\n\n\nSpheres\nSpace-filling, VDW radii\n\n\nLines\nQuick overview, large structures\n\n\nRibbon\nSimplified backbone trace\n\n\n\nExample workflow:\n# Load a structure\nfetch 1GFL\n\n# Show cartoon for overall structure\nshow cartoon\n\n# Show sticks for the chromophore\nselect chromophore, resn CRO\nshow sticks, chromophore\n\n# Hide everything else as sticks\nhide sticks, not chromophore\n\n\nSequence and Structure Connection\nPyMOL shows the sequence along the bottom of the window. This is incredibly useful because:\n\nClick on sequence: Highlights that residue in the 3D view\nDrag across sequence: Selects a range of residues\nColored by secondary structure: Helices, sheets, and loops are color-coded\n\nThis sequence-structure connection is essential when you’re comparing a predicted structure to what you know about the protein’s function.\n\n\nThe Wizard Menu\nPyMOL’s “Wizard” feature provides interactive tools for measurements:\nMeasurement Wizard: - Distances: Click two atoms to measure bond lengths or non-bonded distances - Angles: Click three atoms to measure bond angles - Dihedrals: Click four atoms to measure torsion angles (critical for Ramachandran analysis)\nTo access: Wizard → Measurement from the menu bar.\nMutagenesis Wizard: - Interactively mutate residues - See rotamer options for the new residue - Useful for quick “what if” experiments\nTo access: Wizard → Mutagenesis\n\n\nCreating Publication-Quality Figures\nPyMOL excels at creating beautiful figures. Key settings to know:\nColoring options:\n# Color by chain\nutil.cbc  # Color by chain (automatic)\n\n# Color by secondary structure\ncolor red, ss h    # Helices red\ncolor yellow, ss s # Sheets yellow\ncolor green, ss l+ # Loops green\n\n# Rainbow coloring (N-terminus to C-terminus)\nspectrum count, rainbow\n\n# Color by B-factor (temperature or pLDDT)\nspectrum b, blue_white_red, minimum=50, maximum=100\nTransparency for depth:\n# Make cartoon semi-transparent\nset cartoon_transparency, 0.5, object_name\n\n# Pro tip: Set most of structure to 0.85 transparency,\n# then set region of interest to 0 for highlighting\nset cartoon_transparency, 0.85, all\nset cartoon_transparency, 0, active_site\nSaving figures:\n# Set background to white (for publications)\nbg_color white\n\n# Ray-trace for high quality\nray 2400, 2400\n\n# Save as PNG\npng my_figure.png, dpi=300\n\n\nUseful Commands Reference\n\n\n\nCommand\nDescription\n\n\n\n\nfetch XXXX\nDownload structure from PDB\n\n\nload file.pdb\nLoad local file\n\n\nalign obj1, obj2\nSuperimpose structures\n\n\nrms_cur obj1, obj2\nCalculate RMSD\n\n\ncenter selection\nCenter view on selection\n\n\nzoom selection\nZoom to fit selection\n\n\norient\nReset to default orientation\n\n\nset grid_mode, 1\nView multiple objects side-by-side\n\n\n\n\n\n\n\n\n\nNoteMore PyMOL Resources\n\n\n\n\nPyMOL Wiki: pymolwiki.org - Comprehensive command reference\nTutorial PDF: PyRosetta Workshop PyMOL Tutorial\n\n\n\n\n\nResources\n\nPyMOL Tutorial Slides (PDF)\nAesthetic Script (Python) - Script to recreate RFdiffusion-style figure aesthetics\n\nUsing the Aesthetic Script:\n\nDownload aesthetic.py\nIn PyMOL: File → Run Script… and select the file\nOr from command line: run /path/to/aesthetic.py\nNew commands available: get_colors and get_lighting",
    "crumbs": [
      "Tuesday",
      "1. PyMOL and Visual Studio Code"
    ]
  },
  {
    "objectID": "tuesday/1-pymol.html#part-2-visual-studio-code",
    "href": "tuesday/1-pymol.html#part-2-visual-studio-code",
    "title": "1. PyMOL and Visual Studio Code",
    "section": "Part 2: Visual Studio Code",
    "text": "Part 2: Visual Studio Code\n\nWhy VSCode?\nVisual Studio Code is one of the most popular IDEs (Integrated Development Environments) for coding, and it’s particularly powerful for computational biology work because:\n\nRemote development: SSH directly into HPC clusters and edit code as if it were local\nExtensions: Thousands of extensions for Python, Jupyter, Git, and more\nCustomizable: Tailor the interface to your workflow\nFree: Open source and cross-platform\n\n\n\nGetting Started\nDownload: code.visualstudio.com\nAfter installation, the interface has several key areas:\n\nExplorer (left sidebar): File browser\nEditor (center): Where you write code\nTerminal (bottom): Integrated command line\nExtensions (left sidebar icon): Install add-ons\nSource Control (left sidebar): Git integration\n\nLearn more: VSCode Introductory Videos\n\n\nSSH into HPC Clusters\nThis is perhaps the most valuable feature for our bootcamp. Instead of editing files on the cluster with vim/nano, you can:\n\nConnect VSCode to your HPC account\nBrowse files graphically\nEdit with full IDE features (syntax highlighting, autocomplete, etc.)\nRun terminals on the cluster\n\nSetup Steps:\n\nInstall the Remote - SSH extension in VSCode\nClick the green &gt;&lt; icon in the bottom-left corner\nSelect “Connect to Host…” → “Add New SSH Host…”\nEnter your SSH command: username@cluster.university.edu\nFollow prompts to authenticate\n\n\n\n\n\n\n\nTipSSH Configuration\n\n\n\nFor easier connections, add your cluster to ~/.ssh/config:\nHost mycluster\n    HostName cluster.university.edu\n    User yourusername\n    IdentityFile ~/.ssh/id_rsa\nThen you can just connect to “mycluster” in VSCode.\n\n\nGuides:\n\nVSCode SSH Tutorial\nVSCode Remote SSH Documentation\n\n\n\nViewing Proteins Remotely with Mol*\nWhen working on an HPC cluster, you often want to view protein structures without downloading them to your local machine. VSCode has a Mol* extension that provides an interactive 3D protein viewer.\nSetup:\n\nInstall the Mol* extension in VSCode\nRight-click any .pdb file → “Open with Mol*”\n\nFeatures:\n\nColor by chain, secondary structure, hydrophobicity, B-factor\nCalculate RMSD between structures\nMeasure distances and angles\nToggle between chains/models\nCreate snapshots and animations\n\nDocumentation: Mol* Viewer Docs\n\n\nJupyter Notebooks in VSCode\nJupyter notebooks (.ipynb files) are excellent for:\n\nExploratory analysis: Test code in small chunks\nData visualization: Plot results inline\nDocumentation: Mix code with markdown explanations\nSharing: Notebooks show both code and output\n\nVSCode + Jupyter:\n\nInstall the Jupyter extension\nCreate a new notebook: code -r filename.ipynb\nSelect a Python kernel (your conda environment)\nRun cells with Shift+Enter\n\nThis works even when SSH’d into a cluster—you can run Jupyter notebooks on HPC GPUs through VSCode!\nDocumentation: Jupyter.org Docs\n\n\nUseful Extensions\n\n\n\nExtension\nPurpose\n\n\n\n\nRemote - SSH\nConnect to HPC clusters\n\n\nPython\nPython language support\n\n\nJupyter\nNotebook support\n\n\nMol*\nProtein structure viewer\n\n\nautoDocstring\nGenerate docstrings automatically\n\n\nGitLens\nEnhanced Git integration\n\n\n\n\n\nTips for Remote Work\nTmux - Terminal Multiplexer:\nWhen running long jobs on a cluster, tmux lets you: - Detach from a session and reconnect later - Keep processes running even if your connection drops - Split your terminal into multiple panes\nLearn more: Introduction to Tmux\nBasic tmux workflow:\n# Start a new session\ntmux new -s mysession\n\n# Detach (Ctrl+b, then d)\n\n# List sessions\ntmux ls\n\n# Reattach\ntmux attach -t mysession",
    "crumbs": [
      "Tuesday",
      "1. PyMOL and Visual Studio Code"
    ]
  },
  {
    "objectID": "tuesday/1-pymol.html#hands-on-exercise",
    "href": "tuesday/1-pymol.html#hands-on-exercise",
    "title": "1. PyMOL and Visual Studio Code",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nPart 1: PyMOL Basics\nGoal: Get comfortable with PyMOL navigation and basic commands.\n\nDownload and install PyMOL (or use the version on your cluster/classroom computers)\nLoad your first structure:\nfetch 1GFL\nThis downloads Green Fluorescent Protein (GFP) directly from the PDB.\nPractice navigation:\n\nRotate the structure (left-click drag)\nZoom in and out (scroll wheel or right-click drag)\nTranslate/pan (middle-click drag)\nTry orient to reset the view\n\nExplore representations:\n# Try each of these:\nshow cartoon\nshow sticks\nshow surface\nshow spheres\n\n# Reset to just cartoon\nhide everything\nshow cartoon\nWork with selections:\n# Select the chromophore (the part that glows!)\nselect chromophore, resn CRO\n\n# Show it as sticks\nshow sticks, chromophore\n\n# Color it differently\ncolor yellow, chromophore\nColor by B-factor (confidence in predicted structures):\nspectrum b, blue_white_red, minimum=0, maximum=100\n\n\n\nPart 2: Measurements and Analysis\n\nUse the Measurement Wizard:\n\nGo to Wizard → Measurement\nClick on two atoms to measure their distance\nTry measuring a hydrogen bond (should be ~2.8-3.2 Å)\n\nCalculate RMSD between objects:\n# Fetch another GFP structure\nfetch 1EMA\n\n# Align them\nalign 1EMA, 1GFL\n\n# The RMSD will be printed in the console\n\n\n\nPart 3: Create a Publication Figure\n\nSet up for a nice figure:\n# White background\nbg_color white\n\n# Show cartoon representation\nhide everything\nshow cartoon\n\n# Color by chain\nutil.cbc\n\n# Or try rainbow coloring\nspectrum count, rainbow\nHighlight the chromophore:\nshow sticks, chromophore\ncolor yellow, chromophore\nRay-trace and save:\nray 1200, 1200\npng my_first_pymol_figure.png, dpi=300\n\n\n\nPart 4: Try the Aesthetic Script (Optional)\nIf you have time, use the provided aesthetic.py script:\n\nDownload aesthetic.py\nIn PyMOL: run /path/to/aesthetic.py\nTry the new commands: get_colors and get_lighting\nCreate a publication-quality render in the RFdiffusion style!\n\n\n\nQuestions to Consider\n\nWhat does the GFP chromophore look like? How is it positioned within the protein?\nWhen you colored by B-factor, which regions had high vs. low values? What might this mean?\nHow similar were the two GFP structures (1GFL and 1EMA)? What was the RMSD?\nWhy is it useful to show both cartoon (overall structure) and sticks (specific residues) simultaneously?\n\n\n\n\nVSCode Setup Exercise\nIf you have access to an HPC cluster:\n\nInstall the Remote-SSH extension in VSCode\nConnect to your cluster using the steps described above\nNavigate to your home directory and create a test file\nOpen a terminal within VSCode (Terminal → New Terminal)\nVerify you can run commands on the cluster through VSCode\n\nThis setup will be essential for running AlphaFold2 and other prediction tools in the upcoming exercises!",
    "crumbs": [
      "Tuesday",
      "1. PyMOL and Visual Studio Code"
    ]
  },
  {
    "objectID": "tuesday/3-alphafold2.html",
    "href": "tuesday/3-alphafold2.html",
    "title": "3. AlphaFold2 and OpenFold",
    "section": "",
    "text": "This module dives deep into AlphaFold2—the breakthrough model that essentially “solved” the protein structure prediction problem—and OpenFold, its open-source, trainable implementation.",
    "crumbs": [
      "Tuesday",
      "3. AlphaFold2 and OpenFold"
    ]
  },
  {
    "objectID": "tuesday/3-alphafold2.html#slides",
    "href": "tuesday/3-alphafold2.html#slides",
    "title": "3. AlphaFold2 and OpenFold",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Tuesday",
      "3. AlphaFold2 and OpenFold"
    ]
  },
  {
    "objectID": "tuesday/3-alphafold2.html#the-alphafold2-breakthrough",
    "href": "tuesday/3-alphafold2.html#the-alphafold2-breakthrough",
    "title": "3. AlphaFold2 and OpenFold",
    "section": "The AlphaFold2 Breakthrough",
    "text": "The AlphaFold2 Breakthrough\n\nCASP14: A Watershed Moment\nAt the 14th Critical Assessment of protein Structure Prediction (CASP14) in 2020, AlphaFold2 achieved what many thought was impossible:\n\nGDT-TS scores of ~90 on targets where the previous state-of-the-art was ~60\nNear-experimental accuracy for many proteins\nConsistent performance across diverse protein families\n\nThis wasn’t incremental improvement—it was a paradigm shift.\n\n\n\n\n\n\nNoteThe Scale of the Achievement\n\n\n\nTo put this in perspective: before AlphaFold2, structure prediction was considered one of biology’s grand challenges. Some estimated it would take decades more to solve. AlphaFold2 essentially closed this chapter.\n\n\n\n\nAlphaFold2 vs OpenFold\nAlphaFold2 (DeepMind): - Original implementation in JAX - Released weights and inference code - Not easily trainable by the community\nOpenFold (Columbia/Harvard): - Faithful PyTorch reproduction - Fully trainable on new data - Community-friendly and extensible - 3-5x faster for most proteins - Lower memory usage: Can predict longer proteins on single GPUs\nFor this bootcamp, we’ll use ColabFold, which combines AlphaFold2’s models with fast MSA generation from MMseqs2.\nReference: Ahdritz et al. (2024) - OpenFold paper",
    "crumbs": [
      "Tuesday",
      "3. AlphaFold2 and OpenFold"
    ]
  },
  {
    "objectID": "tuesday/3-alphafold2.html#how-alphafold2-works",
    "href": "tuesday/3-alphafold2.html#how-alphafold2-works",
    "title": "3. AlphaFold2 and OpenFold",
    "section": "How AlphaFold2 Works",
    "text": "How AlphaFold2 Works\n\nHigh-Level Architecture\nAlphaFold2’s architecture flows from the sequence to the final 3D structure through several specialized modules.\n\n\n\n\n\nflowchart LR\n    Seq[Input Sequence] --&gt; MSA[MSA Generation]\n    Seq --&gt; Templ[Template Search]\n    \n    MSA --&gt; Evo[Evoformer]\n    Templ --&gt; Evo\n    \n    Evo --&gt; Struct[Structure Module]\n    Struct --&gt; Coord[3D Coordinates]\n    \n    subgraph \"Iterative Refinement (Recycling)\"\n    Evo\n    Struct\n    end\n    \n    style Evo fill:#f9f,stroke:#333,stroke-width:2px\n    style Struct fill:#bbf,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\n\nInputs and Outputs\nInputs: 1. Query sequence: The protein you want to predict 2. Multiple Sequence Alignment (MSA): Related sequences found by database search 3. Templates (optional): Known structures of homologous proteins\nOutputs: 1. 3D coordinates: Atomic positions for all residues 2. pLDDT scores: Per-residue confidence (0-100) 3. pTM score: Overall structure confidence 4. PAE matrix: Predicted Aligned Error between residue pairs\n\n\nThe MSA: Why Evolutionary Information Matters\nThe Multiple Sequence Alignment is arguably the most important input to AlphaFold2. It aligns your query sequence with evolutionarily related sequences to find co-evolution patterns.\nQuery:     MKVLWAALLVTFLAGCQAKVEQAVETEPEPELRQQTEWQSGQRWELAL\nHomolog1:  MKVLWAALLVTFLAGCQAKVEQAVETEPEPELRQQTEWQSGQRWELAL\nHomolog2:  MKVLWGALLVTFLAGCQAKIEQAVETEPEPELRQQTEWQSGQRWDLAL\nHomolog3:  MKILWAALLVSFLAGCQAKVEQAVEAEPEPELRQQTEWQSGQRWELAL\n           ** **.****:******* :*****.**************.*:***\n\n\n\n\n\n\nImportantThe MSA is Critical\n\n\n\nAlphaFold2’s accuracy depends heavily on MSA quality. Proteins with few homologs (orphan proteins, designed proteins) are harder to predict because there’s less evolutionary information to leverage.\n\n\n\nHow MSAs are Generated\nAlphaFold2 uses multiple database search tools to build comprehensive MSAs:\nJackHMMER (iterative profile HMM search):\n\nSearches UniRef90, MGnify, and other databases\nIteratively builds a profile from hits and re-searches\nHighly sensitive but computationally expensive\nUsed for the “genetic” MSA in AlphaFold2\n\nHHBlits (HMM-HMM search):\n\nSearches clustered databases like BFD (Big Fantastic Database)\nFaster than JackHMMER with comparable sensitivity\nUsed for additional MSA depth\n\nMMseqs2 (ColabFold’s approach):\n\n100-1000x faster than JackHMMER\nSearches pre-computed ColabFold databases\nSlight accuracy trade-off for massive speed gains\nMakes AlphaFold2 practical for large-scale predictions\n\n\n\n\n\n\n\nNoteMSA Depth Matters\n\n\n\nThe number of effective sequences (Neff) in an MSA correlates strongly with prediction accuracy:\n\n\n\nNeff\nExpected Accuracy\n\n\n\n\n&gt;1000\nHigh confidence predictions likely\n\n\n100-1000\nGood predictions for most proteins\n\n\n30-100\nPredictions may be unreliable in some regions\n\n\n&lt;30\nSignificant uncertainty; consider single-sequence methods\n\n\n\nNeff accounts for sequence redundancy—100 nearly identical sequences contribute less information than 100 diverse sequences.\n\n\n\n\nCo-evolution: The Key to Structure Prediction\nThe fundamental insight behind using MSAs for structure prediction is co-evolution: residues that are in physical contact in the 3D structure tend to mutate together during evolution to maintain their interaction.\n\n\n\n\n\nflowchart LR\n    subgraph \"In 3D Structure\"\n        A[Residue i&lt;br/&gt;Asp⁻] &lt;--&gt; B[Residue j&lt;br/&gt;Lys⁺]\n    end\n\n    subgraph \"During Evolution\"\n        C[Asp⁻ → Lys⁺] --&gt; D[Lys⁺ → Asp⁻]\n    end\n\n    A --&gt; C\n    B --&gt; D\n\n    style A fill:#ffcccc\n    style B fill:#ccccff\n\n\n\n\n\n\nWhy does this happen?\nConsider two residues forming a salt bridge (Asp⁻ interacting with Lys⁺):\n\nIf position i mutates from Asp to Lys (negative to positive charge)\nThe interaction is disrupted—the protein may misfold\nUnless position j compensates by mutating from Lys to Asp\nThe complementary mutation restores the interaction\n\nThis creates correlated mutations that we can detect statistically across thousands of homologous sequences.\n\n\nFrom Co-evolution to Contacts: Direct Coupling Analysis\nBefore deep learning, methods like Direct Coupling Analysis (DCA) extracted contact predictions from MSAs:\n\n\n\n\n\nflowchart TD\n    MSA[Multiple Sequence Alignment] --&gt; MI[Mutual Information&lt;br/&gt;Raw correlations]\n    MI --&gt; Problem[Problem: Transitive correlations&lt;br/&gt;A↔B and B↔C implies spurious A↔C]\n    Problem --&gt; DCA[Direct Coupling Analysis&lt;br/&gt;Separates direct from indirect]\n    DCA --&gt; Contacts[Contact Predictions]\n\n    style Problem fill:#ffcccc\n    style DCA fill:#ccffcc\n\n\n\n\n\n\nThe challenge: If residue A co-evolves with B, and B co-evolves with C, simple correlation analysis will show spurious A↔︎C co-evolution even if they never contact.\nDCA’s solution: Use inverse covariance (precision) matrices or pseudolikelihood methods to separate direct couplings from indirect (transitive) correlations.\nAlphaFold2 doesn’t explicitly compute DCA—instead, its attention mechanisms learn to extract these signals directly from the MSA representation.\n\n\nHow AlphaFold2 Processes the MSA\nThe MSA enters AlphaFold2 as a tensor with dimensions:\n\nSequences: Number of aligned sequences (typically ~500-5000, subsampled from full MSA)\nResidues: Length of the query sequence\nFeatures: One-hot encoding + positional features (22 dimensions per residue)\n\nThe model processes this through two parallel representations:\n\n\n\n\n\nflowchart TB\n    MSA[MSA Input&lt;br/&gt;N_seq × L × 22] --&gt; MSArep[MSA Representation&lt;br/&gt;N_seq × L × 256]\n    MSA --&gt; Pair[Pair Representation&lt;br/&gt;L × L × 128]\n\n    MSArep &lt;--&gt; Evo[Evoformer&lt;br/&gt;48 blocks]\n    Pair &lt;--&gt; Evo\n\n    Evo --&gt; Final[To Structure Module]\n\n    style Evo fill:#f9f,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\nMSA Representation: Tracks information about each residue in each sequence\nPair Representation: Tracks relationships between all pairs of residues in the query\n\n\n   Check Your Understanding\n  Why is the MSA so critical for AlphaFold2's accuracy?\n  \n    It provides template structures for the model to copy.\n    It reveals co-evolutionary patterns that imply 3D contacts.\n    It just increases the size of the training data.\n  \n  Check Answer\n  \n\n\n\n\nThe Evoformer: Learning from Evolution\nThe Evoformer is AlphaFold2’s core innovation—a specialized neural network architecture that processes evolutionary information to understand protein structure. It consists of 48 blocks, each updating two interconnected representations.\n\nThe Two Representations\nThe Evoformer maintains and iteratively refines two tensors:\n1. MSA Representation (N_seq × L × 256):\n\nEach entry represents one residue in one sequence\nCaptures per-position information across evolutionary history\nUpdated by row and column attention\n\n2. Pair Representation (L × L × 128):\n\nEach entry represents the relationship between two residues\nEncodes distance, orientation, and contact information\nUpdated by triangle attention and outer product mean\n\n\n\n\n\n\nflowchart TB\n    subgraph \"Evoformer Block (×48)\"\n        subgraph \"MSA Stack\"\n            Row[Row-wise Attention&lt;br/&gt;with pair bias]\n            Col[Column-wise Attention]\n        end\n\n        subgraph \"Communication\"\n            OPM[Outer Product Mean&lt;br/&gt;MSA → Pair]\n        end\n\n        subgraph \"Pair Stack\"\n            TriStart[Triangle Attention&lt;br/&gt;Starting Node]\n            TriEnd[Triangle Attention&lt;br/&gt;Ending Node]\n            TriOut[Triangle Multiplicative&lt;br/&gt;Outgoing]\n            TriIn[Triangle Multiplicative&lt;br/&gt;Incoming]\n            Trans[Transition Layer]\n        end\n    end\n\n    Row --&gt; Col\n    Col --&gt; OPM\n    OPM --&gt; TriStart\n    TriStart --&gt; TriEnd\n    TriEnd --&gt; TriOut\n    TriOut --&gt; TriIn\n    TriIn --&gt; Trans\n\n    style OPM fill:#ffd700\n\n\n\n\n\n\n\n\nRow-wise Attention: Within-Sequence Reasoning\nRow attention operates along the residue dimension for each sequence in the MSA:\nFor each sequence s in the MSA:\n    Query, Key, Value = Linear projections of MSA representation\n    Attention weights = softmax(Q·K^T / √d + pair_bias)\n    Output = Attention weights · V\nKey insight: The attention weights are biased by the pair representation. This means the model’s understanding of residue relationships directly influences how it processes each sequence.\nWhat this achieves:\n\nResidues that are structurally related attend to each other\nLong-range dependencies are captured regardless of sequence distance\nThe pair bias acts like a “structural prior” during MSA processing\n\n\n\nColumn-wise Attention: Cross-Sequence Reasoning\nColumn attention operates across sequences at each position:\nFor each position i in the sequence:\n    Compare how position i appears across all sequences\n    Learn patterns of conservation and variation\nWhat this achieves:\n\nIdentifies conserved residues (important for function/structure)\nDetects co-varying positions (co-evolution signal)\nAggregates information from thousands of evolutionary samples\n\n\n\n\n\n\n\nNoteRow vs Column Attention Intuition\n\n\n\n\nRow attention: “What other positions in this sequence are relevant to position i?”\nColumn attention: “What can I learn about position i by looking at how it varies across evolution?”\n\n\n\n\n\nThe Outer Product Mean: Bridging MSA and Pairs\nThe Outer Product Mean is the critical operation that transfers co-evolution information from the MSA representation into the pair representation:\nFor positions i and j:\n    pair_update[i,j] = Mean over sequences s of (MSA[s,i] ⊗ MSA[s,j])\nWhere ⊗ denotes the outer product.\nIntuition: If positions i and j consistently appear together in certain amino acid combinations across many sequences, this outer product will capture that correlation and inject it into the pair representation.\nThis is how AlphaFold2 “learns” co-evolution: The outer product implicitly computes correlation statistics between positions, similar to what DCA does explicitly—but in a learnable, differentiable way.\n\n\nTriangle Updates: Enforcing Geometric Consistency\nThe triangle updates are what make the pair representation geometrically consistent. They’re based on a simple principle: distances must satisfy the triangle inequality.\n\n\n\n\n\nflowchart LR\n    subgraph \"Triangle Inequality\"\n        A((i)) --- B((j))\n        B --- C((k))\n        A --- C\n    end\n\n    subgraph \"Constraint\"\n        D[d_ij + d_jk ≥ d_ik]\n    end\n\n\n\n\n\n\nIf we know the relationships i↔︎k and k↔︎j, we can infer something about i↔︎j.\nTriangle Attention (Starting and Ending Node):\n\n“Starting node”: For edge i→j, attend over all edges i→k that share the starting node\n“Ending node”: For edge i→j, attend over all edges k→j that share the ending node\n\nTriangle attention (starting):\n    For edge (i,j): aggregate information from all (i,k) edges\n\nTriangle attention (ending):\n    For edge (i,j): aggregate information from all (k,j) edges\nTriangle Multiplicative Updates (Outgoing and Incoming):\nThese use multiplicative gating to combine information:\nTriangle multiplicative (outgoing):\n    pair[i,j] += Σ_k gate(pair[i,k]) × pair[k,j]\n\nTriangle multiplicative (incoming):\n    pair[i,j] += Σ_k pair[i,k] × gate(pair[k,j])\nWhy both attention AND multiplicative? They capture different types of relationships:\n\nAttention: Soft selection of which intermediate nodes k matter\nMultiplicative: Direct combination of path information\n\n\n\n\n\n\n\nTipTriangle Updates Are Like Message Passing on a Graph\n\n\n\nThink of the pair representation as a fully-connected graph where each edge (i,j) stores information. Triangle updates pass messages along triangles, enforcing that if A is close to B and B is close to C, this constrains the A-C relationship.\n\n\n\n\nEvoformer Summary\nAfter 48 blocks, the Evoformer has:\n\nExtracted co-evolutionary signals from the MSA via column attention and outer product mean\nPropagated structural information through row attention with pair bias\nEnforced geometric consistency through triangle updates\nBuilt a rich pair representation encoding likely 3D relationships\n\n\n   Check Your Understanding\n  Which Evoformer operation transfers co-evolution signals from the MSA to the pair representation?\n  \n    Row-wise attention\n    Triangle multiplicative update\n    Outer product mean\n    Column-wise attention\n  \n  Check Answer\n  \n\n\n\n\n\nThe Structure Module: From Representations to 3D Coordinates\nThe Structure Module takes the refined representations from the Evoformer and converts them into actual 3D atomic coordinates. This is where AlphaFold2 transitions from “reasoning about structure” to “predicting structure.”\n\nThe Key Challenge: SE(3) Equivariance\nProtein structure prediction has a fundamental requirement: the prediction should be independent of how we orient the input.\n\nIf we rotate the coordinate frame, the predicted structure should rotate identically\nIf we translate the origin, distances should be preserved\nThe prediction should depend only on intrinsic properties, not arbitrary reference frames\n\nThis is called SE(3) equivariance (Special Euclidean group in 3D = rotations + translations).\nThe problem: Standard neural networks don’t naturally respect this symmetry. A naive network would give different predictions for the same protein oriented differently.\n\n\nFrames: The Backbone Representation\nAlphaFold2 represents each residue using a local coordinate frame (also called a “rigid body” or “frame”):\n\n\n\n\n\nflowchart LR\n    subgraph \"Residue Frame\"\n        O[Origin at Cα] --&gt; X[x-axis: N→Cα direction]\n        O --&gt; Y[y-axis: perpendicular in plane]\n        O --&gt; Z[z-axis: completes right-hand system]\n    end\n\n\n\n\n\n\nEach frame consists of:\n\nTranslation (t): The 3D position of the Cα atom\nRotation (R): A 3×3 rotation matrix defining the local orientation\n\nThe backbone is thus represented as a sequence of frames: T₁, T₂, …, T_L\nWhy frames? They naturally encode:\n\nPosition (where is this residue?)\nOrientation (how is the peptide bond oriented?)\nRelative geometry (how do two residues relate in space?)\n\n\n\nInvariant Point Attention (IPA): The Core Innovation\nInvariant Point Attention is the structure module’s key mechanism. It’s a modified attention operation that:\n\nIs invariant to global rotations and translations\nCan reason about 3D geometric relationships\nUses both the pair representation and current 3D coordinates\n\n\n\n\n\n\nflowchart TB\n    subgraph \"IPA Inputs\"\n        Single[Single Representation&lt;br/&gt;per-residue features]\n        Pair[Pair Representation&lt;br/&gt;from Evoformer]\n        Frames[Current Frames&lt;br/&gt;T_i for each residue]\n    end\n\n    subgraph \"IPA Computation\"\n        QKV[Generate Q, K, V&lt;br/&gt;in local frames]\n        Points[Generate query/key points&lt;br/&gt;in local coordinates]\n        Transform[Transform points to&lt;br/&gt;global frame]\n        Dist[Compute point-to-point&lt;br/&gt;distances]\n        Attn[Attention with&lt;br/&gt;pair bias + distance bias]\n    end\n\n    Single --&gt; QKV\n    Frames --&gt; Points\n    Points --&gt; Transform\n    Transform --&gt; Dist\n    Pair --&gt; Attn\n    Dist --&gt; Attn\n    QKV --&gt; Attn\n\n    Attn --&gt; Output[Updated Single&lt;br/&gt;Representation]\n\n\n\n\n\n\nThe “invariant points” trick:\n\nEach residue generates “query points” and “key points” in its local coordinate frame\nThese points are transformed to the global frame using the current frame estimate\nDistances between points are computed (distances are invariant to rotation/translation!)\nThese distances bias the attention weights\n\nWhy this works: By working with distances between transformed points, IPA can reason about 3D geometry while remaining invariant to the choice of global reference frame.\n\n\nThe Frame Update: Predicting Backbone Geometry\nAfter each IPA layer, the structure module updates the residue frames:\nFor each residue i:\n    quaternion, translation = MLP(single_representation[i])\n    T_i = T_i ∘ (quaternion_to_rotation(quaternion), translation)\nKey insight: Updates are applied as compositions with the current frame, not absolute predictions. This makes learning easier—the network only needs to predict small refinements.\nThe frames start from an initial guess (often just the identity—all residues at the origin with no rotation) and are iteratively refined.\n\n\nSide Chain Prediction: Torsion Angles\nThe structure module also predicts side chain conformations using torsion (dihedral) angles:\nbackbone torsion angles: φ, ψ, ω\nside chain torsion angles: χ₁, χ₂, χ₃, χ₄ (depending on amino acid)\nEach angle is predicted as:\n(sin(angle), cos(angle)) = MLP(single_representation)\nangle = atan2(sin, cos)\nWhy sin/cos instead of the angle directly? Angles wrap around (0° = 360°), which creates discontinuities. Predicting (sin, cos) avoids this problem.\n\n\nFrom Frames to Atoms\nOnce we have frames and torsion angles, computing atom positions is deterministic:\nFor each residue:\n    1. Place backbone atoms (N, Cα, C, O) using the frame\n    2. Place Cβ using ideal bond geometry\n    3. Rotate each side chain bond by predicted χ angles\n    4. All atom positions fall out of the geometry\nThis uses idealized bond lengths and angles from chemistry—the network only predicts the “free” degrees of freedom (frame rotations/translations and torsion angles).\n\n\nStructure Module Architecture Summary\n\n\n\n\n\nflowchart TB\n    EvoOut[Evoformer Output&lt;br/&gt;Single + Pair repr.] --&gt; Init[Initialize Frames&lt;br/&gt;Identity transforms]\n\n    subgraph \"Structure Module (8 layers)\"\n        Init --&gt; IPA1[IPA Layer 1]\n        IPA1 --&gt; Update1[Frame Update]\n        Update1 --&gt; IPA2[IPA Layer 2]\n        IPA2 --&gt; Update2[Frame Update]\n        Update2 --&gt; Dots[...]\n        Dots --&gt; IPA8[IPA Layer 8]\n        IPA8 --&gt; Update8[Frame Update]\n    end\n\n    Update8 --&gt; Torsion[Predict Torsion Angles]\n    Update8 --&gt; Final[Final Frames]\n\n    Final --&gt; Atoms[Compute Atom&lt;br/&gt;Positions]\n    Torsion --&gt; Atoms\n\n    style IPA1 fill:#bbf\n    style IPA8 fill:#bbf\n\n\n\n\n\n\n\n\n\nRecycling: Iterative Refinement\nAlphaFold2 doesn’t just run once—it recycles its predictions back as input for multiple rounds of refinement.\n\n\n\n\n\nflowchart LR\n    subgraph \"Recycle 1\"\n        E1[Evoformer] --&gt; S1[Structure Module]\n    end\n\n    subgraph \"Recycle 2\"\n        E2[Evoformer] --&gt; S2[Structure Module]\n    end\n\n    subgraph \"Recycle 3\"\n        E3[Evoformer] --&gt; S3[Structure Module]\n    end\n\n    S1 --&gt;|\"Pair repr + structure\"| E2\n    S2 --&gt;|\"Pair repr + structure\"| E3\n    S3 --&gt; Final[Final Prediction]\n\n\n\n\n\n\nWhat gets recycled?\n\nPair representation from previous iteration\nPredicted structure (frames) from structure module\nFirst row of MSA representation (query sequence features)\n\nWhy recycle?\n\nThe Evoformer can use structural information to better interpret the MSA\nIterative refinement allows progressive improvement\nSimilar to how Rosetta uses “iterative assembly” in fragment-based prediction\n\nTypical recycling: 3 rounds (default), up to 20 for difficult targets\n\n\n\n\n\n\nNoteRecycling Creates an Implicit “Dynamics”\n\n\n\nEach recycle can be thought of as a step of optimization. The structure progressively “folds” from a random initial state toward the predicted final structure. This is somewhat analogous to molecular dynamics or energy minimization.\n\n\n\n   Check Your Understanding\n  Why does Invariant Point Attention (IPA) use distances between transformed points?\n  \n    To make computation faster\n    Distances are invariant to rotation and translation\n    Points are easier to visualize than frames\n    To reduce memory usage\n  \n  Check Answer",
    "crumbs": [
      "Tuesday",
      "3. AlphaFold2 and OpenFold"
    ]
  },
  {
    "objectID": "tuesday/3-alphafold2.html#key-parameters-and-settings",
    "href": "tuesday/3-alphafold2.html#key-parameters-and-settings",
    "title": "3. AlphaFold2 and OpenFold",
    "section": "Key Parameters and Settings",
    "text": "Key Parameters and Settings\n\nRecycling Parameters\n\n\n\n\n\n\n\n\nParameter\nWhat it does\nTypical values\n\n\n\n\nnum_recycles\nHow many refinement passes\n3-20 (more = better but slower)\n\n\nmax_recycles\nHard cap for recycling\n20\n\n\nrecycle_early_stop_tolerance\nStop if structure converges\n0.5 Å\n\n\n\nWhen to increase recycles: - Large proteins (&gt;500 residues) - Multi-domain proteins - When initial predictions look uncertain\n\n\nMSA Settings\n\n\n\n\n\n\n\n\nParameter\nWhat it does\nOptions\n\n\n\n\nmsa_mode\nHow to generate MSA\nmmseqs2 (fast), jackhmmer (thorough)\n\n\npair_mode\nFor multimers: how to pair sequences\npaired, unpaired, paired+unpaired\n\n\nuse_msa\nWhether to use MSA at all\ntrue/false\n\n\nuse_templates\nWhether to search for structural templates\ntrue/false\n\n\n\n\n\n\n\n\n\nWarningSingle-Sequence Mode\n\n\n\nYou can run AlphaFold2 without an MSA (use_msa=false), but accuracy drops significantly. Use this only for: - Designed proteins with no natural homologs - Quick preliminary scans - When comparing to ESMFold\n\n\n\n\nModel Selection\n\n\n\n\n\n\n\n\nParameter\nWhat it does\nOptions\n\n\n\n\nmodel_type\nMonomer vs multimer\nmonomer, monomer_ptm, multimer\n\n\nnum_models\nNumber of model versions to run\n1-5\n\n\nrank_by\nHow to select best prediction\nplddt, ptm, iptm+ptm\n\n\n\nThe 5 models: AlphaFold2 was trained with 5 different random initializations. Running all 5 provides: - Ensemble diversity (different predictions) - Uncertainty estimation (do they agree?) - Better chance of finding the best structure\n\n\nRelaxation\n\n\n\nParameter\nWhat it does\n\n\n\n\nuse_amber\nRun Amber energy minimization\n\n\nuse_gpu_relax\nUse GPU for relaxation (faster)\n\n\n\nWhy relax? - Fixes minor clashes and bad geometry - Makes structures more physically realistic - Important for downstream applications (docking, MD)",
    "crumbs": [
      "Tuesday",
      "3. AlphaFold2 and OpenFold"
    ]
  },
  {
    "objectID": "tuesday/3-alphafold2.html#alphafold2-extensions",
    "href": "tuesday/3-alphafold2.html#alphafold2-extensions",
    "title": "3. AlphaFold2 and OpenFold",
    "section": "AlphaFold2 Extensions",
    "text": "AlphaFold2 Extensions\n\nAlphaFold-Multimer\nPredicting protein complexes (multiple chains interacting):\nKey modifications: - Cross-chain MSA pairing for evolutionary signal - Losses account for chain permutation symmetry - Interface-aware training\n# In your FASTA file, separate chains with \":\"\n&gt;complex\nSEQUENCEOFCHAINA:SEQUENCEOFCHAINB\n\n\nAlphaFold Database\nDeepMind released predictions for 200+ million proteins—essentially all of UniProt.\n\nAccess: alphafold.ebi.ac.uk\nCoverage: Human proteome + 47 other key organisms\nUse case: Check if your protein has already been predicted!\n\n\n\nAF-Cluster\nIdea: Proteins can have multiple conformations. Can we bias AlphaFold2 toward different states?\nApproach: - Cluster the MSA into subgroups - Run predictions with different MSA subsets - Different clusters may yield different conformational states\nReference: Wayment-Steele et al. (2022)\n\n\nAFsample\nIdea: Generate diverse predictions through sampling.\nApproach: - Enable dropout during inference - Increase recycling - Generate multiple diverse predictions\nThis is useful for understanding conformational flexibility and uncertainty.\n\n\nAutomated Workflows\nTools like EvoPro and BindCraft use AlphaFold2 as part of larger design pipelines:\n\nGenerate candidate designs\nPredict structures with AF2\nScore and filter based on confidence\nIterate to improve designs",
    "crumbs": [
      "Tuesday",
      "3. AlphaFold2 and OpenFold"
    ]
  },
  {
    "objectID": "tuesday/3-alphafold2.html#understanding-the-output",
    "href": "tuesday/3-alphafold2.html#understanding-the-output",
    "title": "3. AlphaFold2 and OpenFold",
    "section": "Understanding the Output",
    "text": "Understanding the Output\n\npLDDT: Per-Residue Confidence\npLDDT (predicted Local Distance Difference Test) ranges from 0-100:\n\n\n\npLDDT\nInterpretation\nWhat to do\n\n\n\n\n&gt;90\nVery high confidence\nTrust the local structure\n\n\n70-90\nConfident\nGenerally reliable\n\n\n50-70\nLow confidence\nTreat with caution\n\n\n&lt;50\nVery low confidence\nLikely disordered or wrong\n\n\n\n\n\n\n\n\n\nTipLow pLDDT ≠ Wrong\n\n\n\nLow pLDDT regions often correspond to: - Intrinsically disordered regions (genuinely unstructured) - Flexible loops (multiple conformations possible) - Crystal contacts (structure depends on environment)\nLow confidence is information, not failure!\n\n\n\n\npTM and ipTM: Global Confidence\n\npTM (predicted TM-score): Overall structure confidence (0-1)\nipTM (interface pTM): Confidence in interface prediction (for multimers)\n\n\n\n\nScore\nInterpretation\n\n\n\n\n&gt;0.8\nHigh confidence\n\n\n0.5-0.8\nModerate confidence\n\n\n&lt;0.5\nLow confidence\n\n\n\n\n\nPAE: Predicted Aligned Error\nThe PAE matrix shows confidence in relative positions between residue pairs.\nReading PAE plots: - Blue/low values: Confident in relative position - Red/high values: Uncertain about relative position - Diagonal blocks: Domains (confident within, uncertain between)\nPAE is crucial for: - Identifying domain boundaries - Assessing multimer interface confidence - Understanding which parts of the structure are well-determined relative to each other",
    "crumbs": [
      "Tuesday",
      "3. AlphaFold2 and OpenFold"
    ]
  },
  {
    "objectID": "tuesday/3-alphafold2.html#practical-considerations",
    "href": "tuesday/3-alphafold2.html#practical-considerations",
    "title": "3. AlphaFold2 and OpenFold",
    "section": "Practical Considerations",
    "text": "Practical Considerations\n\nWhen to Use AlphaFold2\nBest for: - Proteins with many homologs in databases - Monomers and stable complexes - When accuracy is paramount\nLess suitable for: - Designed/synthetic proteins (few homologs) - Highly dynamic proteins - When speed is critical (use ESMFold instead)\n\n\nResource Requirements\n\n\n\nResource\nMinimum\nRecommended\n\n\n\n\nGPU RAM\n16 GB\n40+ GB (A100)\n\n\nCPU RAM\n32 GB\n64 GB\n\n\nDisk\n15 GB\n100+ GB (with databases)\n\n\n\n\n\nTips for Better Predictions\n\nCheck the AlphaFold Database first—your protein may already be predicted\nUse all 5 models for important predictions\nExamine pLDDT and PAE—don’t just look at the structure\nConsider multiple conformations for flexible proteins\nValidate experimentally for high-stakes applications",
    "crumbs": [
      "Tuesday",
      "3. AlphaFold2 and OpenFold"
    ]
  },
  {
    "objectID": "tuesday/3-alphafold2.html#hands-on-exercise",
    "href": "tuesday/3-alphafold2.html#hands-on-exercise",
    "title": "3. AlphaFold2 and OpenFold",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\n\nPart 1: Run ColabFold Prediction\nGoal: Generate your first AlphaFold2 prediction.\nWe’ll predict the structure of Green Fluorescent Protein (GFP) using ColabFold.\n1. Prepare your sequence:\nCreate a file called gfp.fasta:\n&gt;GFP\nMSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\nYou can also download this file: 1GFL.fasta\n2. Run prediction:\nIf using LocalColabFold on your HPC:\ncolabfold_batch gfp.fasta gfp_output/\nIf using the ColabFold notebook: - Go to ColabFold - Paste your sequence - Run all cells\n3. Expected output: - gfp_relaxed_rank_001_*.pdb - Best predicted structure - gfp_scores_rank_001_*.json - Confidence scores - gfp_coverage.png - MSA coverage - gfp_pae.png - PAE heatmap\nExpected runtime: 5-15 minutes depending on your hardware.\n\n\nPart 2: Analyze Your Prediction\n1. Load the structure in PyMOL:\nload gfp_output/gfp_relaxed_rank_001_alphafold2_ptm_model_1_seed_000.pdb, af2_gfp\n2. Color by confidence (pLDDT):\nspectrum b, blue_white_red, minimum=50, maximum=100\n3. Questions to answer: - What is the overall pLDDT? (Check the JSON file or PyMOL’s B-factor range) - Which regions have high confidence? Low confidence? - Does GFP have any disordered regions?\n4. Compare to experimental structure:\nfetch 1GFL\nalign af2_gfp, 1GFL\n\nWhat is the RMSD between prediction and experiment?\nDo the structures overlay well?\n\n\n\nPart 3: Explore the PAE\n1. Open gfp_pae.png\n2. Interpret the plot: - Is there one block (single domain) or multiple blocks? - Are there any high-PAE (red) regions? - What does this tell you about the structure?\n\n\nPart 4: Experiment with Parameters\nTry running predictions with different settings:\nExperiment 1: Fewer recycles\ncolabfold_batch --num-recycle 1 gfp.fasta gfp_1recycle/\nCompare to the default (3 recycles). Is there a difference in quality?\nExperiment 2: Single sequence (no MSA)\ncolabfold_batch --msa-mode single_sequence gfp.fasta gfp_single_seq/\nThis mimics what ESMFold does. How does quality compare?\nExperiment 3: All 5 models\ncolabfold_batch --num-models 5 gfp.fasta gfp_all_models/\nDo the different models agree? What’s the variance in pLDDT?\n\n\nPart 5: Multimer Prediction (Optional)\nIf you have time, try predicting a protein complex:\n1. Create a multimer FASTA:\n&gt;homodimer\nMSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK:MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\n(Note: The : separates chains)\n2. Run prediction:\ncolabfold_batch homodimer.fasta homodimer_output/\n3. Analyze: - What is the ipTM score? - Does the interface look reasonable? - Check PAE for interface confidence (off-diagonal blocks)\n\n\nQuestions to Consider\n\nHow does prediction time scale with sequence length?\nWhy might some regions have low pLDDT even in a well-folded protein?\nWhen would you trust a prediction enough to use it for experimental planning?\nWhat would you do if AlphaFold2 and ESMFold give different predictions?\n\n\n\nRecord Your Results\nFill in this table with your observations:\n\n\n\nMetric\nYour GFP Prediction\n\n\n\n\nAverage pLDDT\n\n\n\npTM score\n\n\n\nRMSD to 1GFL\n\n\n\nPrediction time\n\n\n\nMSA depth\n\n\n\nRegions with pLDDT &lt; 70",
    "crumbs": [
      "Tuesday",
      "3. AlphaFold2 and OpenFold"
    ]
  },
  {
    "objectID": "monday/6-openfold.html",
    "href": "monday/6-openfold.html",
    "title": "6. OpenFold (Optional)",
    "section": "",
    "text": "OpenFold (paper, code) is a faithful, trainable PyTorch reproduction of DeepMind’s AlphaFold2. It achieves performance comparable to AlphaFold2 and provides a fully open-source implementation for protein structure prediction.",
    "crumbs": [
      "Monday",
      "6. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/6-openfold.html#why-use-openfold",
    "href": "monday/6-openfold.html#why-use-openfold",
    "title": "6. OpenFold (Optional)",
    "section": "Why Use OpenFold?",
    "text": "Why Use OpenFold?\n\nFull transparency: Open-source model architecture and training code\nTrainable: Can be fine-tuned or retrained on custom data\nResearch-friendly: Ideal for understanding how structure prediction works\nMSA-based accuracy: Uses evolutionary information for high-accuracy predictions\n\nRelated Tools: For faster predictions without MSAs, see ESMFold. For a more user-friendly MSA-based option, see LocalColabFold.",
    "crumbs": [
      "Monday",
      "6. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/6-openfold.html#resource-requirements",
    "href": "monday/6-openfold.html#resource-requirements",
    "title": "6. OpenFold (Optional)",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n40+ GB\nA100 for large proteins\n\n\nCPU RAM\n32 GB\n64+ GB\nMSA generation is memory-intensive\n\n\nDisk Space\n500 GB\n2+ TB\nSequence databases are large\n\n\nCUDA\n11.3+\n12.1+\nRequired for compilation\n\n\n\nNote: OpenFold requires significant disk space for sequence databases if generating MSAs locally. Check if your HPC already has AlphaFold/OpenFold databases installed.",
    "crumbs": [
      "Monday",
      "6. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/6-openfold.html#preparation",
    "href": "monday/6-openfold.html#preparation",
    "title": "6. OpenFold (Optional)",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nnvcc available for CUDA compilation\nSignificant disk space (or access to shared databases)\n\nCheck for existing databases:\n# Ask your HPC admins or check common locations\nls /shared/databases/alphafold/\nls /shared/databases/openfold/\nMany HPCs have pre-installed AlphaFold databases that OpenFold can use.",
    "crumbs": [
      "Monday",
      "6. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/6-openfold.html#installation",
    "href": "monday/6-openfold.html#installation",
    "title": "6. OpenFold (Optional)",
    "section": "Installation",
    "text": "Installation\n Mark as complete\nImportant: OpenFold installation can be complex. The official documentation at openfold.readthedocs.io has the most current instructions.\n\nClone the repository:\n\ngit clone https://github.com/aqlaboratory/openfold.git\ncd openfold\n\nCreate the conda environment:\n\nmamba env create -f environment.yml\nmamba activate openfold_venv\nExpected time: 10-20 minutes for environment creation.\n\nInstall OpenFold:\n\npip install -e .\n\nDownload model weights:\n\nbash scripts/download_openfold_params.sh openfold/resources\nExpected download: ~1-2 GB of model weights.\n\n(Optional) Download sequence databases for MSA generation:\n\n# This downloads ~2TB of data - skip if using HPC shared databases\nbash scripts/download_alphafold_dbs.sh /path/to/database/directory",
    "crumbs": [
      "Monday",
      "6. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/6-openfold.html#testing-the-installation",
    "href": "monday/6-openfold.html#testing-the-installation",
    "title": "6. OpenFold (Optional)",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test FASTA file test.fasta:\n&gt;test_protein\nMKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\nRun a prediction (using pre-computed MSAs or without MSAs for testing):\npython run_pretrained_openfold.py \\\n    test.fasta \\\n    /path/to/database/directory \\\n    --output_dir predictions/ \\\n    --config_preset model_1_ptm \\\n    --model_device cuda:0\nNote: For testing without databases, you can use --use_precomputed_alignments with a directory containing pre-computed MSA files.\nSuccess indicators:\n\nCommand completes without errors\npredictions/ directory contains PDB files\nOutput includes confidence metrics (pLDDT, pTM)\n\nExpected runtime: 5-30 minutes depending on MSA availability and protein size.",
    "crumbs": [
      "Monday",
      "6. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/6-openfold.html#hpc-job-script",
    "href": "monday/6-openfold.html#hpc-job-script",
    "title": "6. OpenFold (Optional)",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=openfold\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --time=08:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc # Source shell profile if needed\nmamba activate openfold_venv\n\ncd /path/to/openfold\n\n# Using HPC shared databases\nDATABASE_DIR=/shared/databases/alphafold\n\npython run_pretrained_openfold.py \\\n    my_protein.fasta \\\n    $DATABASE_DIR \\\n    --output_dir predictions/ \\\n    --config_preset model_1_ptm \\\n    --model_device cuda:0",
    "crumbs": [
      "Monday",
      "6. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/6-openfold.html#usage-examples",
    "href": "monday/6-openfold.html#usage-examples",
    "title": "6. OpenFold (Optional)",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic prediction with local databases:\npython run_pretrained_openfold.py \\\n    input.fasta \\\n    /path/to/databases \\\n    --output_dir output/ \\\n    --config_preset model_1_ptm\nUsing pre-computed MSAs:\npython run_pretrained_openfold.py \\\n    input.fasta \\\n    /path/to/databases \\\n    --use_precomputed_alignments /path/to/msas/ \\\n    --output_dir output/\nMultiple model presets (ensemble):\nfor preset in model_1_ptm model_2_ptm model_3_ptm; do\n    python run_pretrained_openfold.py \\\n        input.fasta \\\n        /path/to/databases \\\n        --config_preset $preset \\\n        --output_dir output_${preset}/\ndone",
    "crumbs": [
      "Monday",
      "6. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/6-openfold.html#model-presets",
    "href": "monday/6-openfold.html#model-presets",
    "title": "6. OpenFold (Optional)",
    "section": "Model Presets",
    "text": "Model Presets\n\n\n\nPreset\nDescription\n\n\n\n\nmodel_1_ptm\nStandard model with pTM head\n\n\nmodel_2_ptm\nAlternative model with pTM\n\n\nmodel_3_ptm\nThird model variant\n\n\nmodel_1_multimer_v3\nFor protein complexes",
    "crumbs": [
      "Monday",
      "6. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/6-openfold.html#understanding-the-output",
    "href": "monday/6-openfold.html#understanding-the-output",
    "title": "6. OpenFold (Optional)",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\npredictions/\n├── test_protein_model_1_ptm_unrelaxed.pdb    # Predicted structure\n├── test_protein_model_1_ptm_confidences.json # Confidence scores\n└── test_protein_model_1_ptm_timings.json     # Runtime statistics\nConfidence metrics:\n\npLDDT: Per-residue confidence (0-100, higher is better)\npTM: Predicted TM-score (0-1, &gt;0.8 is confident)\nPAE: Predicted Aligned Error matrix",
    "crumbs": [
      "Monday",
      "6. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/6-openfold.html#database-requirements",
    "href": "monday/6-openfold.html#database-requirements",
    "title": "6. OpenFold (Optional)",
    "section": "Database Requirements",
    "text": "Database Requirements\nIf generating MSAs locally, OpenFold needs these databases:\n\n\n\nDatabase\nSize\nPurpose\n\n\n\n\nBFD\n~1.7 TB\nSequence alignments\n\n\nMGnify\n~120 GB\nMetagenomic sequences\n\n\nUniRef90\n~100 GB\nSequence clustering\n\n\nUniRef30\n~200 GB\nHHblits templates\n\n\nPDB70\n~60 GB\nStructure templates\n\n\n\nTotal: ~2+ TB\nCheck HPC shared databases first - most research HPCs have these pre-installed.",
    "crumbs": [
      "Monday",
      "6. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/6-openfold.html#troubleshooting",
    "href": "monday/6-openfold.html#troubleshooting",
    "title": "6. OpenFold (Optional)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nCompilation errors during install:\n# Ensure CUDA toolkit is loaded\nmodule load cuda/12.1\nnvcc --version\n\n# Clean and retry\npip uninstall openfold\npip install -e .\n“Database not found” errors:\n\nVerify database paths exist\nCheck HPC documentation for shared database locations\nContact HPC admins about AlphaFold database availability\n\nOut of memory:\n\nRequest more GPU memory\nReduce --max_recycling_iters\nUse gradient checkpointing for training\n\nSlow MSA generation:\n\nMSA generation is CPU-bound and can take hours\nUse pre-computed MSAs when possible\nConsider using ColabFold’s MMseqs2 server instead\n\nModel weights not found:\n# Re-download weights\nbash scripts/download_openfold_params.sh openfold/resources\n\n# Verify files exist\nls openfold/resources/*.pt",
    "crumbs": [
      "Monday",
      "6. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/6-openfold.html#for-researchers-training",
    "href": "monday/6-openfold.html#for-researchers-training",
    "title": "6. OpenFold (Optional)",
    "section": "For Researchers: Training",
    "text": "For Researchers: Training\nOpenFold can be retrained or fine-tuned:\npython train_openfold.py \\\n    /path/to/training/data \\\n    /path/to/template_mmcif \\\n    /path/to/output \\\n    --config_preset initial_training\nSee the training documentation for details.",
    "crumbs": [
      "Monday",
      "6. OpenFold (Optional)"
    ]
  },
  {
    "objectID": "monday/index.html",
    "href": "monday/index.html",
    "title": "Monday: Tool Installation",
    "section": "",
    "text": "Monday is dedicated to installing the essential ML-based protein design and structure prediction tools on your HPC cluster. Each module guides you through installing a specific tool, from structure predictors like LocalColabFold and ESMFold to design tools like LigandMPNN and BindCraft.\nGoal: By the end of Monday, you should have all major tools installed and tested on your HPC cluster.",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#overview",
    "href": "monday/index.html#overview",
    "title": "Monday: Tool Installation",
    "section": "",
    "text": "Monday is dedicated to installing the essential ML-based protein design and structure prediction tools on your HPC cluster. Each module guides you through installing a specific tool, from structure predictors like LocalColabFold and ESMFold to design tools like LigandMPNN and BindCraft.\nGoal: By the end of Monday, you should have all major tools installed and tested on your HPC cluster.",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#pre-work",
    "href": "monday/index.html#pre-work",
    "title": "Monday: Tool Installation",
    "section": "Pre-work",
    "text": "Pre-work\nBefore starting the main modules, complete these pre-work assignments to ensure your local environment is ready.\n\n\n\n#\nModule\nDescription\nStatus\n\n\n\n\nP1\nEnvironment & GitHub\nSetup conda, git, and GitHub\nRequired\n\n\nP2\nPyMOL & VS Code\nInstall visualization and coding tools\nRequired\n\n\nP3\nPython Refresher\nRefresh Python skills for bioinformatics\nRecommended",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#getting-started",
    "href": "monday/index.html#getting-started",
    "title": "Monday: Tool Installation",
    "section": "Getting Started",
    "text": "Getting Started\nBefore installing individual tools, complete the HPC Setup module to ensure your environment is properly configured:\n\n\n\n#\nModule\nDescription\nStatus\n\n\n\n\n1\nCommon HPC Setup\nCUDA, Conda, containers, and environment setup\nStart Here",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#tool-installation-modules",
    "href": "monday/index.html#tool-installation-modules",
    "title": "Monday: Tool Installation",
    "section": "Tool Installation Modules",
    "text": "Tool Installation Modules\n\n\n\n#\nTool\nDescription\nStatus\n\n\n\n\n2\nLocalColabFold\nFast AlphaFold2 structure prediction\nRequired\n\n\n3\nLigandMPNN\nContext-aware protein sequence design\nRequired\n\n\n4\nRFdiffusion2\nAtom-level active site scaffolding\nRequired\n\n\n5\nESMFold\nSingle-sequence structure prediction\nRequired\n\n\n6\nOpenFold\nOpen-source AlphaFold2 reproduction\nOptional\n\n\n7\nChai-1\nMulti-modal biomolecular structure prediction\nRequired\n\n\n8\nBoltz-2\nStructure + binding affinity prediction\nRequired\n\n\n9\nDiffDock-PP\nProtein-protein docking\nRequired\n\n\n10\nPLACER\nProtein-ligand conformational ensemble prediction\nRequired\n\n\n11\nBindCraft\nEnd-to-end binder design pipeline\nRequired\n\n\n12\nESM3\nMultimodal protein generation\nOptional\n\n\n13\nRFdiffusion All Atom\nAll-atom protein design (predecessor to RFd2)\nOptional",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#tool-categories",
    "href": "monday/index.html#tool-categories",
    "title": "Monday: Tool Installation",
    "section": "Tool Categories",
    "text": "Tool Categories\n\nStructure Prediction\n\n\n\n\n\n\n\n\n\n\nTool\nInput\nOutput\nSpeed\nBest For\n\n\n\n\nLocalColabFold\nSequence + MSA\nStructure\nMedium\nHigh-accuracy single proteins\n\n\nESMFold\nSequence only\nStructure\nFast\nQuick predictions, no MSA needed\n\n\nOpenFold\nSequence + MSA\nStructure\nMedium\nResearch, custom training\n\n\nChai-1\nMulti-modal\nComplex structures\nMedium\nProteins + ligands + nucleic acids\n\n\nBoltz-2\nMulti-modal\nStructure + affinity\nMedium\nDrug discovery\n\n\n\n\n\nProtein Design\n\n\n\n\n\n\n\n\n\nTool\nInput\nOutput\nBest For\n\n\n\n\nLigandMPNN\nBackbone\nSequence\nSequence design with ligand context\n\n\nRFdiffusion2\nConstraints\nBackbone\nActive site scaffolding\n\n\nBindCraft\nTarget structure\nBinder designs\nEnd-to-end binder design\n\n\n\n\n\nDocking\n\n\n\n\n\n\n\n\n\nTool\nInput\nOutput\nBest For\n\n\n\n\nDiffDock-PP\nTwo proteins\nDocked complex\nProtein-protein docking\n\n\nPLACER\nProtein + ligand\nEnsemble poses\nProtein-ligand docking",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#tips-for-success",
    "href": "monday/index.html#tips-for-success",
    "title": "Monday: Tool Installation",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nStart with HPC Setup - Complete Module 1 before installing any tools\nUse separate conda environments - Each tool should have its own environment to avoid dependency conflicts\nCheck GPU availability - Most tools require GPU access; make sure you can request GPU nodes on your cluster\nNote your paths - Keep track of where you install each tool; you’ll need these paths later\nTest each installation - Don’t move on until you’ve verified each tool works\nCheck shared resources - Your HPC may already have databases (AlphaFold, ColabFold) installed",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#resource-overview",
    "href": "monday/index.html#resource-overview",
    "title": "Monday: Tool Installation",
    "section": "Resource Overview",
    "text": "Resource Overview\nApproximate requirements across all tools:\n\n\n\nResource\nTotal Needed\n\n\n\n\nDisk Space\n50-100 GB (tools only), 2+ TB (with databases)\n\n\nGPU RAM\n16-80 GB depending on task\n\n\nCPU RAM\n32-64 GB",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/index.html#getting-help",
    "href": "monday/index.html#getting-help",
    "title": "Monday: Tool Installation",
    "section": "Getting Help",
    "text": "Getting Help\nIf you encounter issues:\n\nCheck the tool’s official documentation (linked in each module)\nSearch for existing GitHub issues on the tool’s repository\nReport an issue on the bootcamp site (GitHub account required)\n\n\n\n\n← Back to Home\n\n\nTuesday →",
    "crumbs": [
      "Monday",
      "Monday: Tool Installation"
    ]
  },
  {
    "objectID": "monday/4-rfdiffusion2.html",
    "href": "monday/4-rfdiffusion2.html",
    "title": "4. RFdiffusion2",
    "section": "",
    "text": "RFdiffusion2 (paper, code) is a protein design model capable of atom-level active site scaffolding. It extends the original RFdiffusion to enable precise control over protein-ligand interactions at the atomic level.",
    "crumbs": [
      "Monday",
      "4. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/4-rfdiffusion2.html#why-use-rfdiffusion2",
    "href": "monday/4-rfdiffusion2.html#why-use-rfdiffusion2",
    "title": "4. RFdiffusion2",
    "section": "Why Use RFdiffusion2?",
    "text": "Why Use RFdiffusion2?\n\nAtomic-level control: Design proteins with precise active site geometries\nLigand scaffolding: Build proteins around small molecules with atomic accuracy\nMotif grafting: Incorporate functional motifs into new scaffolds\nFlexible backbone design: Generate novel folds with specific functional constraints\n\nRelated Tools: Use with LigandMPNN for sequence design after backbone generation. For the earlier version without atomic control, see RFdiffusion All Atom (Optional).",
    "crumbs": [
      "Monday",
      "4. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/4-rfdiffusion2.html#resource-requirements",
    "href": "monday/4-rfdiffusion2.html#resource-requirements",
    "title": "4. RFdiffusion2",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\n\n\n\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n32+ GB\nA100 for larger designs\n\n\nCPU RAM\n16 GB\n32 GB\nContainer-based execution\n\n\nDisk Space\n10 GB\n20 GB\nContainer + weights\n\n\nContainer\nApptainer/Singularity\nRequired\nNo native Docker on HPC",
    "crumbs": [
      "Monday",
      "4. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/4-rfdiffusion2.html#preparation",
    "href": "monday/4-rfdiffusion2.html#preparation",
    "title": "4. RFdiffusion2",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nApptainer/Singularity available on your cluster\nCUDA-capable GPU\n\nVerify your environment:\nmodule load apptainer    # or: module load singularity\napptainer --version\nnvidia-smi\nImportant: RFdiffusion2 uses containers. Most academic HPCs do NOT support Docker for security reasons - use Apptainer/Singularity instead.",
    "crumbs": [
      "Monday",
      "4. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/4-rfdiffusion2.html#installation",
    "href": "monday/4-rfdiffusion2.html#installation",
    "title": "4. RFdiffusion2",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the repository:\n\ngit clone https://github.com/RosettaCommons/RFdiffusion2.git\ncd RFdiffusion2\n\nAdd the repo to your PYTHONPATH (add to ~/.bashrc):\n\nexport PYTHONPATH=\"/path/to/your/RFdiffusion2:$PYTHONPATH\"\n\nDownload the model weights and container:\n\npython setup.py\nExpected download: ~5-10 GB (container + weights). This can take 30+ minutes.\nIf download is interrupted:\npython setup.py overwrite\n\nVerify Apptainer/Singularity is available:\n\nmodule load apptainer\n# or: module load singularity\nThe downloaded .sif file in rf_diffusion/exec/ is the Singularity container.",
    "crumbs": [
      "Monday",
      "4. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/4-rfdiffusion2.html#testing-the-installation",
    "href": "monday/4-rfdiffusion2.html#testing-the-installation",
    "title": "4. RFdiffusion2",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a demo case:\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=open_source_demo \\\n    sweep.benchmarks=active_site_unindexed_atomic_partial_ligand\nNote: Omit --nv flag if running without GPU (will be very slow).\nSuccess indicators:\n\nCommand completes without errors\nOutput directory created at pipeline_outputs/&lt;timestamp&gt;_open_source_demo/\nContains PDB files with designed structures\n\nExpected runtime: 5-15 minutes on GPU, 30+ minutes on CPU.",
    "crumbs": [
      "Monday",
      "4. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/4-rfdiffusion2.html#hpc-job-script",
    "href": "monday/4-rfdiffusion2.html#hpc-job-script",
    "title": "4. RFdiffusion2",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=rfdiff2\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load apptainer\nmodule load cuda/12.1\n\ncd /path/to/RFdiffusion2\n\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=open_source_demo",
    "crumbs": [
      "Monday",
      "4. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/4-rfdiffusion2.html#usage-examples",
    "href": "monday/4-rfdiffusion2.html#usage-examples",
    "title": "4. RFdiffusion2",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic backbone design:\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=my_config\nWith custom output directory:\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=open_source_demo \\\n    sweep.output_dir=/path/to/output\nMultiple design benchmarks:\napptainer exec --nv rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif \\\n    rf_diffusion/benchmark/pipeline.py \\\n    --config-name=open_source_demo \\\n    sweep.benchmarks=\"[benchmark1,benchmark2]\"",
    "crumbs": [
      "Monday",
      "4. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/4-rfdiffusion2.html#docker-to-apptainer-translation",
    "href": "monday/4-rfdiffusion2.html#docker-to-apptainer-translation",
    "title": "4. RFdiffusion2",
    "section": "Docker to Apptainer Translation",
    "text": "Docker to Apptainer Translation\nThe official documentation may show Docker commands. Here’s how to translate:\n\n\n\nDocker Command\nApptainer Equivalent\n\n\n\n\ndocker run --gpus all image\napptainer exec --nv image.sif\n\n\ndocker run -v /path:/path\napptainer exec --bind /path:/path\n\n\n-it (interactive)\napptainer shell --nv\n\n\n\nExample conversion:\n# Docker (won't work on HPC):\ndocker run --gpus all -v $(pwd):/workspace rfdiffusion:latest python script.py\n\n# Apptainer (works on HPC):\napptainer exec --nv --bind $(pwd):/workspace rfdiffusion.sif python script.py",
    "crumbs": [
      "Monday",
      "4. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/4-rfdiffusion2.html#understanding-the-output",
    "href": "monday/4-rfdiffusion2.html#understanding-the-output",
    "title": "4. RFdiffusion2",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput structure:\npipeline_outputs/\n└── &lt;timestamp&gt;_&lt;config_name&gt;/\n    ├── designs/\n    │   ├── design_0.pdb    # Designed backbone\n    │   ├── design_1.pdb\n    │   └── ...\n    ├── logs/\n    │   └── run.log         # Execution log\n    └── config.yaml         # Configuration used",
    "crumbs": [
      "Monday",
      "4. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/4-rfdiffusion2.html#configuration-system",
    "href": "monday/4-rfdiffusion2.html#configuration-system",
    "title": "4. RFdiffusion2",
    "section": "Configuration System",
    "text": "Configuration System\nRFdiffusion2 uses Hydra for configuration. Key config options:\n\n\n\nParameter\nDescription\n\n\n\n\nsweep.benchmarks\nWhich design task(s) to run\n\n\nsweep.output_dir\nOutput directory\n\n\ndiffuser.T\nNumber of diffusion timesteps\n\n\ninference.num_designs\nNumber of designs to generate",
    "crumbs": [
      "Monday",
      "4. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/4-rfdiffusion2.html#troubleshooting",
    "href": "monday/4-rfdiffusion2.html#troubleshooting",
    "title": "4. RFdiffusion2",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n“No GPU available” / extremely slow:\n\nEnsure --nv flag is included\nVerify GPU allocation: nvidia-smi\nLoad CUDA module: module load cuda/12.1\n\nContainer permission errors:\nchmod +x rf_diffusion/exec/bakerlab_rf_diffusion_aa.sif\n“FileNotFoundError” for weights:\n\nRe-run python setup.py to ensure all files downloaded\nCheck rf_diffusion/weights/ directory exists\n\nContainer not found:\n\nProvide full path to .sif file\nOr run from the RFdiffusion2 directory\n\nSetup script hangs during download:\n\nLarge files may take 30+ minutes\nCheck network connectivity\nIf interrupted, run python setup.py overwrite\n\nModule not found errors inside container:\n\nEnsure PYTHONPATH is set correctly\nContainer may need --bind for additional paths",
    "crumbs": [
      "Monday",
      "4. RFdiffusion2"
    ]
  },
  {
    "objectID": "monday/8-boltz2.html",
    "href": "monday/8-boltz2.html",
    "title": "8. Boltz-2",
    "section": "",
    "text": "Boltz-2 (paper, code) is a biomolecular foundation model that jointly models complex structures and binding affinities. It’s the first deep learning model to approach the accuracy of physics-based free-energy perturbation (FEP) methods while running 1000x faster.",
    "crumbs": [
      "Monday",
      "8. Boltz-2"
    ]
  },
  {
    "objectID": "monday/8-boltz2.html#why-use-boltz-2",
    "href": "monday/8-boltz2.html#why-use-boltz-2",
    "title": "8. Boltz-2",
    "section": "Why Use Boltz-2?",
    "text": "Why Use Boltz-2?\n\nStructure + Affinity: Predict both binding pose and binding strength\nDrug discovery ready: Affinity predictions useful for hit-to-lead optimization\nMulti-modal: Handles proteins, nucleic acids, small molecules, covalent modifications\nSpeed: 1000x faster than FEP methods for affinity prediction\n\nRelated Tools: For structure prediction only, see Chai-1. For protein-ligand docking, see PLACER or DiffDock-PP.",
    "crumbs": [
      "Monday",
      "8. Boltz-2"
    ]
  },
  {
    "objectID": "monday/8-boltz2.html#resource-requirements",
    "href": "monday/8-boltz2.html#resource-requirements",
    "title": "8. Boltz-2",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n32+ GB\nScales with complex size\n\n\nCPU RAM\n16 GB\n32 GB\nFor preprocessing\n\n\nDisk Space\n5 GB\n10 GB\nModel weights\n\n\nPython\n3.9+\n3.11\nRequired",
    "crumbs": [
      "Monday",
      "8. Boltz-2"
    ]
  },
  {
    "objectID": "monday/8-boltz2.html#preparation",
    "href": "monday/8-boltz2.html#preparation",
    "title": "8. Boltz-2",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nCUDA-capable GPU (recommended) or CPU\n\nImportant: Install Boltz in a fresh Python environment to avoid dependency conflicts.",
    "crumbs": [
      "Monday",
      "8. Boltz-2"
    ]
  },
  {
    "objectID": "monday/8-boltz2.html#installation",
    "href": "monday/8-boltz2.html#installation",
    "title": "8. Boltz-2",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nCreate a fresh environment:\n\nmamba create -n boltz python=3.11\nmamba activate boltz\n\nInstall Boltz with CUDA support:\n\npip install boltz[cuda] -U\nFor CPU-only or non-CUDA GPUs:\npip install boltz -U\nAlternative: Install from GitHub (for latest updates):\ngit clone https://github.com/jwohlwend/boltz.git\ncd boltz\npip install -e .[cuda]",
    "crumbs": [
      "Monday",
      "8. Boltz-2"
    ]
  },
  {
    "objectID": "monday/8-boltz2.html#testing-the-installation",
    "href": "monday/8-boltz2.html#testing-the-installation",
    "title": "8. Boltz-2",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test YAML file test_input.yaml:\nversion: 1\nsequences:\n  - protein:\n      id: [A, B]\n      sequence: MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\nRun prediction:\nboltz predict test_input.yaml --use_msa_server\nSuccess indicators:\n\nCommand completes without errors\nOutput directory contains:\n\nPredicted structure files (CIF format)\nConfidence scores\n\n\nExpected runtime: 1-3 minutes for this small test.",
    "crumbs": [
      "Monday",
      "8. Boltz-2"
    ]
  },
  {
    "objectID": "monday/8-boltz2.html#hpc-job-script",
    "href": "monday/8-boltz2.html#hpc-job-script",
    "title": "8. Boltz-2",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=boltz\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate boltz\n\n# Run prediction\nboltz predict my_complex.yaml --use_msa_server --out_dir results/",
    "crumbs": [
      "Monday",
      "8. Boltz-2"
    ]
  },
  {
    "objectID": "monday/8-boltz2.html#usage-examples",
    "href": "monday/8-boltz2.html#usage-examples",
    "title": "8. Boltz-2",
    "section": "Usage Examples",
    "text": "Usage Examples\nStructure prediction only:\nboltz predict structure.yaml\nWith MSA server (higher accuracy):\nboltz predict input.yaml --use_msa_server\nWith affinity prediction:\n# input.yaml\nversion: 1\nsequences:\n  - protein:\n      id: A\n      sequence: MKTVRQERLK...\n  - ligand:\n      id: L\n      smiles: \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\"\nproperties:\n  - affinity\nboltz predict input.yaml",
    "crumbs": [
      "Monday",
      "8. Boltz-2"
    ]
  },
  {
    "objectID": "monday/8-boltz2.html#input-format-yaml",
    "href": "monday/8-boltz2.html#input-format-yaml",
    "title": "8. Boltz-2",
    "section": "Input Format (YAML)",
    "text": "Input Format (YAML)\nBoltz uses YAML files to describe biomolecules:\nSimple protein:\nversion: 1\nsequences:\n  - protein:\n      id: A\n      sequence: MKTVRQERLKSIVRILERSKEPVSG...\nProtein-ligand complex:\nversion: 1\nsequences:\n  - protein:\n      id: A\n      sequence: MKTVRQERLK...\n  - ligand:\n      id: L\n      smiles: \"CCO\"\nProtein complex (homodimer):\nversion: 1\nsequences:\n  - protein:\n      id: [A, B]  # Same sequence for both chains\n      sequence: MKTVRQERLK...\nWith affinity prediction:\nversion: 1\nsequences:\n  - protein:\n      id: A\n      sequence: MKTVRQERLK...\n  - ligand:\n      id: L\n      smiles: \"CC(=O)NC1=CC=C(O)C=C1\"\nproperties:\n  - affinity\nSee prediction documentation for full format details.",
    "crumbs": [
      "Monday",
      "8. Boltz-2"
    ]
  },
  {
    "objectID": "monday/8-boltz2.html#binding-affinity-predictions",
    "href": "monday/8-boltz2.html#binding-affinity-predictions",
    "title": "8. Boltz-2",
    "section": "Binding Affinity Predictions",
    "text": "Binding Affinity Predictions\nBoltz-2 provides two affinity metrics:\n\n\n\n\n\n\n\n\nMetric\nRange\nUse Case\n\n\n\n\naffinity_probability_binary\n0-1\nHit discovery - probability that ligand is a binder\n\n\naffinity_pred_value\nlog10(IC50) in μM\nLead optimization - compare binding strengths\n\n\n\nInterpretation:\n\naffinity_probability_binary: Higher = more likely to bind\naffinity_pred_value: Lower = stronger binding (lower IC50)",
    "crumbs": [
      "Monday",
      "8. Boltz-2"
    ]
  },
  {
    "objectID": "monday/8-boltz2.html#msa-server-authentication",
    "href": "monday/8-boltz2.html#msa-server-authentication",
    "title": "8. Boltz-2",
    "section": "MSA Server Authentication",
    "text": "MSA Server Authentication\nFor servers requiring authentication:\nexport BOLTZ_MSA_TOKEN=\"your_token_here\"\nboltz predict input.yaml --use_msa_server",
    "crumbs": [
      "Monday",
      "8. Boltz-2"
    ]
  },
  {
    "objectID": "monday/8-boltz2.html#understanding-the-output",
    "href": "monday/8-boltz2.html#understanding-the-output",
    "title": "8. Boltz-2",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\nboltz_results_&lt;input&gt;/\n├── predictions/\n│   ├── model_0.cif      # Predicted structure\n│   └── confidence.json  # Confidence scores\n├── msa/                 # Generated MSAs (if using server)\n└── affinity/            # Affinity predictions (if requested)\nConfidence metrics:\n\npLDDT: Per-residue confidence\npTM: Predicted TM-score\ninterface pTM: For complexes",
    "crumbs": [
      "Monday",
      "8. Boltz-2"
    ]
  },
  {
    "objectID": "monday/8-boltz2.html#performance-comparison",
    "href": "monday/8-boltz2.html#performance-comparison",
    "title": "8. Boltz-2",
    "section": "Performance Comparison",
    "text": "Performance Comparison\n\n\n\nMethod\nSpeed\nAffinity Accuracy\n\n\n\n\nFEP (physics-based)\nHours-days\nGold standard\n\n\nBoltz-2\nSeconds-minutes\nComparable to FEP\n\n\nTraditional docking\nSeconds\nLower accuracy",
    "crumbs": [
      "Monday",
      "8. Boltz-2"
    ]
  },
  {
    "objectID": "monday/8-boltz2.html#troubleshooting",
    "href": "monday/8-boltz2.html#troubleshooting",
    "title": "8. Boltz-2",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nInstallation issues:\n\nUse a fresh environment\nTry removing [cuda] if CUDA issues arise\nVerify CUDA version compatibility\n\n“MSA server error”:\n\nCheck network connectivity\nVerify authentication token if required\nTry without --use_msa_server for testing\n\nOut of memory:\n\nRequest more GPU memory\nReduce complex size\nTry CPU-only mode for testing\n\nSlow without GPU:\n\nCPU mode is functional but significantly slower\nAlways use GPU for production runs\n\nYAML parsing errors:\n\nCheck YAML syntax (indentation matters)\nEnsure SMILES strings are quoted\nVerify sequence format",
    "crumbs": [
      "Monday",
      "8. Boltz-2"
    ]
  },
  {
    "objectID": "monday/5-esmfold.html",
    "href": "monday/5-esmfold.html",
    "title": "5. ESMFold",
    "section": "",
    "text": "ESMFold (paper, code) is an end-to-end single-sequence structure predictor that uses the ESM-2 language model to generate accurate 3D protein structures directly from sequence, without requiring multiple sequence alignments (MSAs).",
    "crumbs": [
      "Monday",
      "5. ESMFold"
    ]
  },
  {
    "objectID": "monday/5-esmfold.html#why-use-esmfold",
    "href": "monday/5-esmfold.html#why-use-esmfold",
    "title": "5. ESMFold",
    "section": "Why Use ESMFold?",
    "text": "Why Use ESMFold?\n\nSpeed: Significantly faster than AlphaFold2 (seconds vs minutes)\nNo MSA required: Works directly from sequence alone\nCompetitive accuracy: Often comparable to AlphaFold2 for well-folded domains\nLower resource usage: Can run on smaller GPUs\n\nRelated Tools: For MSA-based prediction with potentially higher accuracy, see LocalColabFold or OpenFold. For protein language model embeddings only, see ESM3.",
    "crumbs": [
      "Monday",
      "5. ESMFold"
    ]
  },
  {
    "objectID": "monday/5-esmfold.html#resource-requirements",
    "href": "monday/5-esmfold.html#resource-requirements",
    "title": "5. ESMFold",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n40+ GB\nLarger proteins need more memory\n\n\nCPU RAM\n16 GB\n32 GB\nCPU-only is possible but slow\n\n\nDisk Space\n5 GB\n10 GB\nModel weights\n\n\nPython\n≤3.9\n3.9\nImportant: Python 3.10+ may have issues\n\n\n\nWhy Python ≤3.9? ESMFold depends on OpenFold, which has compatibility issues with newer Python versions.",
    "crumbs": [
      "Monday",
      "5. ESMFold"
    ]
  },
  {
    "objectID": "monday/5-esmfold.html#preparation",
    "href": "monday/5-esmfold.html#preparation",
    "title": "5. ESMFold",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nnvcc available (for compiling OpenFold dependencies)\n\nVerify your environment:\nnvcc --version      # Required for OpenFold compilation\nmodule load cuda    # If nvcc not found",
    "crumbs": [
      "Monday",
      "5. ESMFold"
    ]
  },
  {
    "objectID": "monday/5-esmfold.html#installation",
    "href": "monday/5-esmfold.html#installation",
    "title": "5. ESMFold",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nCreate a conda environment with Python 3.9:\n\nmamba create -n esmfold python=3.9\nmamba activate esmfold\n\nInstall PyTorch (adjust CUDA version to match your cluster):\n\nmamba install pytorch pytorch-cuda=12.1 -c pytorch -c nvidia\n\nInstall ESM with ESMFold dependencies:\n\npip install \"fair-esm[esmfold]\"\n\nInstall OpenFold dependencies:\n\npip install 'dllogger @ git+https://github.com/NVIDIA/dllogger.git'\npip install 'openfold @ git+https://github.com/aqlaboratory/openfold.git@4b41059694619831a7db195b7e0988fc4ff3a307'\nNote: OpenFold compilation requires nvcc. If it fails, verify CUDA toolkit is loaded.\nAlternative method (using environment file):\nwget https://raw.githubusercontent.com/facebookresearch/esm/main/environment.yml\nmamba env create -f environment.yml\nmamba activate esmfold",
    "crumbs": [
      "Monday",
      "5. ESMFold"
    ]
  },
  {
    "objectID": "monday/5-esmfold.html#testing-the-installation",
    "href": "monday/5-esmfold.html#testing-the-installation",
    "title": "5. ESMFold",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nCreate a test script test_esmfold.py:\nimport torch\nimport esm\n\n# Load ESMFold model\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()  # Remove .cuda() if using CPU\n\n# Test sequence (65 residues)\nsequence = \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"\n\n# Run prediction\nwith torch.no_grad():\n    output = model.infer_pdb(sequence)\n\n# Save output\nwith open(\"test_result.pdb\", \"w\") as f:\n    f.write(output)\n\nprint(\"Structure prediction successful!\")\nprint(f\"Output saved to test_result.pdb\")\nprint(f\"Sequence length: {len(sequence)} residues\")\nRun the test:\npython test_esmfold.py\nSuccess indicators:\n\nCommand completes without errors\ntest_result.pdb file is created\nFile contains valid PDB coordinates\n\nExpected runtime: ~10-30 seconds on GPU for this small protein.",
    "crumbs": [
      "Monday",
      "5. ESMFold"
    ]
  },
  {
    "objectID": "monday/5-esmfold.html#hpc-job-script",
    "href": "monday/5-esmfold.html#hpc-job-script",
    "title": "5. ESMFold",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=esmfold\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate esmfold\n\n# Predict structures for all sequences in FASTA file\nesm-fold -i my_proteins.fasta -o predictions/ \\\n    --num-recycles 4 \\\n    --max-tokens-per-batch 1024",
    "crumbs": [
      "Monday",
      "5. ESMFold"
    ]
  },
  {
    "objectID": "monday/5-esmfold.html#usage-examples",
    "href": "monday/5-esmfold.html#usage-examples",
    "title": "5. ESMFold",
    "section": "Usage Examples",
    "text": "Usage Examples\nCommand line interface:\nesm-fold -i sequences.fasta -o output_pdbs/\nKey CLI options:\n\n\n\nOption\nDescription\n\n\n\n\n-i\nInput FASTA file\n\n\n-o\nOutput directory for PDB files\n\n\n--num-recycles\nNumber of recycles (default: 4)\n\n\n--max-tokens-per-batch\nBatch shorter sequences together\n\n\n--chunk-size\nReduce memory (values: 128, 64, 32)\n\n\n--cpu-only\nRun on CPU only\n\n\n--cpu-offload\nOffload to CPU RAM for long sequences\n\n\n\nReduce memory for large proteins:\nesm-fold -i large_proteins.fasta -o output/ --chunk-size 64\nProcess very long sequences:\nesm-fold -i long_sequences.fasta -o output/ --cpu-offload\nPython API:\nimport torch\nimport esm\n\n# Load model\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\n# Predict structure\nsequence = \"MVKLTAEGSEVSRQVIVQDIAYLRSLG\"\nwith torch.no_grad():\n    pdb_string = model.infer_pdb(sequence)\n\n# Save\nwith open(\"prediction.pdb\", \"w\") as f:\n    f.write(pdb_string)\nGet confidence scores:\nimport torch\nimport esm\n\nmodel = esm.pretrained.esmfold_v1()\nmodel = model.eval().cuda()\n\nsequence = \"MVKLTAEGSEVSRQVIVQDIAYLRSLG\"\nwith torch.no_grad():\n    output = model.infer(sequence)\n\n# Per-residue confidence (pLDDT)\nplddt = output[\"plddt\"]  # Shape: (1, L)\nprint(f\"Mean pLDDT: {plddt.mean().item():.2f}\")\n\n# Predicted TM-score\nptm = output[\"ptm\"]\nprint(f\"pTM: {ptm.item():.3f}\")",
    "crumbs": [
      "Monday",
      "5. ESMFold"
    ]
  },
  {
    "objectID": "monday/5-esmfold.html#understanding-the-output",
    "href": "monday/5-esmfold.html#understanding-the-output",
    "title": "5. ESMFold",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nPDB output:\n\nStandard PDB format with predicted coordinates\nB-factor column contains pLDDT confidence scores (0-100)\nHigher pLDDT = higher confidence\n\nConfidence score interpretation:\n\n\n\npLDDT Range\nInterpretation\n\n\n\n\n90-100\nVery high confidence\n\n\n70-90\nConfident\n\n\n50-70\nLow confidence (may be disordered)\n\n\n&lt;50\nVery low confidence (likely disordered)",
    "crumbs": [
      "Monday",
      "5. ESMFold"
    ]
  },
  {
    "objectID": "monday/5-esmfold.html#memory-usage-guide",
    "href": "monday/5-esmfold.html#memory-usage-guide",
    "title": "5. ESMFold",
    "section": "Memory Usage Guide",
    "text": "Memory Usage Guide\nApproximate GPU memory by sequence length:\n\n\n\nSequence Length\nGPU Memory Needed\n\n\n\n\n&lt;200 aa\n8-16 GB\n\n\n200-400 aa\n16-24 GB\n\n\n400-600 aa\n24-40 GB\n\n\n600-1000 aa\n40-80 GB\n\n\n&gt;1000 aa\nUse --cpu-offload",
    "crumbs": [
      "Monday",
      "5. ESMFold"
    ]
  },
  {
    "objectID": "monday/5-esmfold.html#troubleshooting",
    "href": "monday/5-esmfold.html#troubleshooting",
    "title": "5. ESMFold",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nOpenFold installation fails:\n\nVerify nvcc is available:\nnvcc --version\n# If not found:\nmodule load cuda\nEnsure PyTorch CUDA version matches system CUDA\n\n“CUDA out of memory”:\n# Use chunking to reduce memory\nesm-fold -i input.fasta -o output/ --chunk-size 64\n\n# Or use CPU offloading for very long sequences\nesm-fold -i input.fasta -o output/ --cpu-offload\nSlow on GPU (should be fast):\n# Verify CUDA is detected\npython -c \"import torch; print(torch.cuda.is_available())\"\n# Should print: True\nPython version errors:\n\nESMFold requires Python ≤3.9 due to OpenFold dependencies\nCreate a new environment with Python 3.9 if needed\n\nModel download hangs:\n\nFirst run downloads ~2GB of weights\nSet custom cache location:\nexport TORCH_HOME=/scratch/$USER/torch_cache",
    "crumbs": [
      "Monday",
      "5. ESMFold"
    ]
  },
  {
    "objectID": "monday/10-placer.html",
    "href": "monday/10-placer.html",
    "title": "10. PLACER",
    "section": "",
    "text": "PLACER (paper, code) stands for Protein-Ligand Atomistic Conformational Ensemble Resolver. It’s a graph neural network that operates entirely at the atomic level to generate conformational ensembles of protein-ligand complexes.",
    "crumbs": [
      "Monday",
      "10. PLACER"
    ]
  },
  {
    "objectID": "monday/10-placer.html#why-use-placer",
    "href": "monday/10-placer.html#why-use-placer",
    "title": "10. PLACER",
    "section": "Why Use PLACER?",
    "text": "Why Use PLACER?\n\nEnsemble predictions: Generates multiple conformations to capture binding uncertainty\nAll-atom accuracy: Operates at atomic level for precise interactions\nSide chain flexibility: Predicts side chain conformations alongside ligand poses\nConfidence scores: Multiple metrics for ranking and validating predictions\n\nRelated Tools: For protein-protein docking, see DiffDock-PP. For structure prediction of complexes, see Chai-1 or Boltz-2.",
    "crumbs": [
      "Monday",
      "10. PLACER"
    ]
  },
  {
    "objectID": "monday/10-placer.html#resource-requirements",
    "href": "monday/10-placer.html#resource-requirements",
    "title": "10. PLACER",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n8 GB\n16 GB\n1-3 sec/model on GPU\n\n\nCPU RAM\n16 GB\n32 GB\n~1 min/model on 8 cores\n\n\nDisk Space\n2 GB\n5 GB\nModel weights included\n\n\nPython\n3.9+\n3.10\nRequired\n\n\n\nPerformance:\n\nGPU: 1-3 seconds per model\nCPU (8 cores): ~1 minute per model\nLigands with many symmetric groups take longer",
    "crumbs": [
      "Monday",
      "10. PLACER"
    ]
  },
  {
    "objectID": "monday/10-placer.html#preparation",
    "href": "monday/10-placer.html#preparation",
    "title": "10. PLACER",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nGPU recommended for reasonable throughput",
    "crumbs": [
      "Monday",
      "10. PLACER"
    ]
  },
  {
    "objectID": "monday/10-placer.html#installation",
    "href": "monday/10-placer.html#installation",
    "title": "10. PLACER",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the repository:\n\ngit clone https://github.com/baker-laboratory/PLACER.git\ncd PLACER\nThe repository includes model weights - no separate download needed.\n\nCreate the conda environment from the provided file:\n\nmamba env create -f envs/placer_env.yml\n\nActivate the environment:\n\nmamba activate placer_env",
    "crumbs": [
      "Monday",
      "10. PLACER"
    ]
  },
  {
    "objectID": "monday/10-placer.html#testing-the-installation",
    "href": "monday/10-placer.html#testing-the-installation",
    "title": "10. PLACER",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a simple heme docking prediction:\npython run_PLACER.py \\\n    --ifile examples/inputs/dnHEM1.pdb \\\n    --odir test_output \\\n    --rerank prmsd \\\n    -n 10 \\\n    --ligand_file HEM:examples/ligands/HEM.mol2\nSuccess indicators:\n\nCommand completes without errors\ntest_output/ directory is created\nContains ranked PDB files of docked complexes\n\nExpected runtime: 30-60 seconds on GPU, 10-15 minutes on CPU.",
    "crumbs": [
      "Monday",
      "10. PLACER"
    ]
  },
  {
    "objectID": "monday/10-placer.html#hpc-job-script",
    "href": "monday/10-placer.html#hpc-job-script",
    "title": "10. PLACER",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=placer\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc\nmamba activate placer_env\n\ncd /path/to/PLACER\n\n# Predict ligand binding with 100 samples\npython run_PLACER.py \\\n    --ifile my_complex.pdb \\\n    --odir results/ \\\n    --predict_ligand LIG-501 \\\n    --rerank prmsd \\\n    -n 100 \\\n    --ligand_file LIG:ligand.sdf",
    "crumbs": [
      "Monday",
      "10. PLACER"
    ]
  },
  {
    "objectID": "monday/10-placer.html#usage-examples",
    "href": "monday/10-placer.html#usage-examples",
    "title": "10. PLACER",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic ligand docking:\npython run_PLACER.py \\\n    -f INPUT.pdb \\\n    -o OUTPUT_DIR \\\n    -n 50\nLigand docking with cofactor fixed:\npython run_PLACER.py \\\n    --ifile 4dtz.cif \\\n    --odir output/ \\\n    --predict_ligand D-LDP-501 \\\n    --fixed_ligand C-HEM-500 \\\n    -n 100 \\\n    --rerank prmsd\nSide chain prediction (apo mode, no ligand):\npython run_PLACER.py \\\n    --ifile protein.pdb \\\n    --odir output/ \\\n    --target_res A-149 \\\n    -n 50 \\\n    --no-use_sm\nMultiple ligands simultaneously:\npython run_PLACER.py \\\n    --ifile complex.pdb \\\n    --odir output/ \\\n    --predict_multi \\\n    --predict_ligand LIG1 LIG2 \\\n    -n 100",
    "crumbs": [
      "Monday",
      "10. PLACER"
    ]
  },
  {
    "objectID": "monday/10-placer.html#key-parameters",
    "href": "monday/10-placer.html#key-parameters",
    "title": "10. PLACER",
    "section": "Key Parameters",
    "text": "Key Parameters\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\n-f or --ifile\nInput PDB/mmCIF file\n\n\n-o or --odir\nOutput directory\n\n\n-n or --nsamples\nNumber of ensemble samples (50-100 recommended)\n\n\n--rerank\nRank by confidence: prmsd, plddt, or plddt_pde\n\n\n--predict_ligand\nSpecify which ligand(s) to predict\n\n\n--fixed_ligand\nKeep certain ligands fixed in place\n\n\n--ligand_file\nProvide SDF/MOL2 for correct atom typing\n\n\n--target_res\nSpecific residue(s) for side chain prediction\n\n\n--no-use_sm\nApo mode - predict without small molecules",
    "crumbs": [
      "Monday",
      "10. PLACER"
    ]
  },
  {
    "objectID": "monday/10-placer.html#understanding-confidence-scores",
    "href": "monday/10-placer.html#understanding-confidence-scores",
    "title": "10. PLACER",
    "section": "Understanding Confidence Scores",
    "text": "Understanding Confidence Scores\nPLACER provides multiple confidence metrics:\n\n\n\n\n\n\n\n\nMetric\nDescription\nGood Values\n\n\n\n\nprmsd\nPredicted RMSD to true pose\n&lt;2.0 Å (excellent), &lt;4.0 Å (acceptable)\n\n\nplddt\nPer-residue confidence (1D track)\n&gt;0.8\n\n\nplddt_pde\nPer-residue confidence (2D track)\n&gt;0.8\n\n\nfape\nAll-atom FAPE loss\nLower is better\n\n\nrmsd\nActual RMSD to reference (if available)\n&lt;2.0 Å\n\n\nkabsch\nSuperimposed RMSD\nMeasures conformation accuracy\n\n\n\nRecommendation: Use --rerank prmsd for docking tasks.",
    "crumbs": [
      "Monday",
      "10. PLACER"
    ]
  },
  {
    "objectID": "monday/10-placer.html#python-api",
    "href": "monday/10-placer.html#python-api",
    "title": "10. PLACER",
    "section": "Python API",
    "text": "Python API\nPLACER can be imported as a Python module:\nimport sys\nsys.path.append(\"/path/to/PLACER\")\nimport PLACER\n\n# Load model\nplacer = PLACER.PLACER()\n\n# Set up input\npl_input = PLACER.PLACERinput()\npl_input.pdb(\"complex.pdb\")\npl_input.name(\"my_prediction\")\npl_input.ligand_reference({\"HEM\": \"heme.mol2\"})\n\n# Run 50 predictions\noutputs = placer.run(pl_input, 50)\n\n# Access results\nfor out in outputs:\n    print(f\"pRMSD: {out.prmsd:.2f}, pLDDT: {out.plddt:.2f}\")",
    "crumbs": [
      "Monday",
      "10. PLACER"
    ]
  },
  {
    "objectID": "monday/10-placer.html#understanding-the-output",
    "href": "monday/10-placer.html#understanding-the-output",
    "title": "10. PLACER",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\noutput/\n├── ranked_0.pdb          # Best pose by confidence\n├── ranked_1.pdb          # Second best\n├── ranked_2.pdb          # ...\n├── scores.csv            # All confidence metrics\n└── ensemble/             # All generated samples\nInterpreting ensemble results:\n\nMultiple similar poses = high confidence\nDiverse poses = uncertain binding mode\nCompare top-ranked poses to assess convergence",
    "crumbs": [
      "Monday",
      "10. PLACER"
    ]
  },
  {
    "objectID": "monday/10-placer.html#input-format-requirements",
    "href": "monday/10-placer.html#input-format-requirements",
    "title": "10. PLACER",
    "section": "Input Format Requirements",
    "text": "Input Format Requirements\nLigand must be in input structure:\n\nPLACER requires the ligand to be present in the PDB\nSMILES-only input is not supported\nUse --ligand_file to provide correct bonding information\n\nLigand file formats:\n\nSDF files: Best for drug-like molecules\nMOL2 files: Good for cofactors\nHelps with: aromatic rings, stereochemistry, bond orders",
    "crumbs": [
      "Monday",
      "10. PLACER"
    ]
  },
  {
    "objectID": "monday/10-placer.html#troubleshooting",
    "href": "monday/10-placer.html#troubleshooting",
    "title": "10. PLACER",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nNon-planar aromatic rings:\n\nProvide SDF/MOL2 file with --ligand_file\nThis ensures correct bonding information\n\nMissing ligands in PDB:\n\nLigands must be in the input structure\nSMILES-only input is not currently supported\n\nCustom/non-canonical residues:\npython run_PLACER.py \\\n    --ifile input.pdb \\\n    --residue_json custom_residues.json \\\n    --odir output/\nSlow predictions:\n\nSymmetric ligands (many equivalent atoms) are slower\nUse GPU for production runs\nReduce -n for initial testing\n\nOut of memory:\n\nPLACER is generally memory-efficient\nIf issues persist, try reducing ensemble size\nOr use CPU with multiple cores",
    "crumbs": [
      "Monday",
      "10. PLACER"
    ]
  },
  {
    "objectID": "monday/3-ligandmpnn.html",
    "href": "monday/3-ligandmpnn.html",
    "title": "3. LigandMPNN",
    "section": "",
    "text": "LigandMPNN (paper, code) is a deep learning model for context-aware protein sequence design. It extends ProteinMPNN to handle small molecules, metal ions, and other non-protein components in protein design tasks.",
    "crumbs": [
      "Monday",
      "3. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/3-ligandmpnn.html#why-use-ligandmpnn",
    "href": "monday/3-ligandmpnn.html#why-use-ligandmpnn",
    "title": "3. LigandMPNN",
    "section": "Why Use LigandMPNN?",
    "text": "Why Use LigandMPNN?\n\nLigand-aware design: Design sequences that account for bound cofactors, substrates, or drug molecules\nContext preservation: Maintain interactions with metals, DNA, RNA, or other molecules\nSide chain packing: Evaluate and optimize side chain conformations\nFlexible residue control: Fix, bias, or vary specific positions\n\nRelated Tools: Use with RFdiffusion2 for backbone design, or BindCraft for complete binder design pipelines.",
    "crumbs": [
      "Monday",
      "3. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/3-ligandmpnn.html#resource-requirements",
    "href": "monday/3-ligandmpnn.html#resource-requirements",
    "title": "3. LigandMPNN",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n4 GB\n16 GB\nScales with protein size\n\n\nCPU RAM\n8 GB\n16 GB\nCPU-only is viable but slower\n\n\nDisk Space\n2 GB\n5 GB\nModel weights\n\n\nPython\n3.9+\n3.11\nRequired",
    "crumbs": [
      "Monday",
      "3. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/3-ligandmpnn.html#preparation",
    "href": "monday/3-ligandmpnn.html#preparation",
    "title": "3. LigandMPNN",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\nPrerequisites:\n\nCompleted HPC Setup guide\nConda/Mamba installed\nGit installed\n\nVerify your environment:\npython --version    # Should be 3.9+\nnvcc --version      # For GPU support (optional)",
    "crumbs": [
      "Monday",
      "3. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/3-ligandmpnn.html#installation",
    "href": "monday/3-ligandmpnn.html#installation",
    "title": "3. LigandMPNN",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nClone the LigandMPNN repository:\n\ngit clone https://github.com/dauparas/LigandMPNN.git\ncd LigandMPNN\n\nDownload the model parameters: Note: This step requires internet access. If your compute node doesn’t have internet, run this on a login node.\n\nbash get_model_params.sh \"./model_params\"\nExpected download: ~500 MB of model weights.\n\nCreate a new conda environment:\n\nmamba create -n ligandmpnn_env python=3.11\nmamba activate ligandmpnn_env\n\nInstall dependencies:\n\npip install -r requirements.txt\nThis installs PyTorch, NumPy, and ProDy for PDB file handling.",
    "crumbs": [
      "Monday",
      "3. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/3-ligandmpnn.html#testing-the-installation",
    "href": "monday/3-ligandmpnn.html#testing-the-installation",
    "title": "3. LigandMPNN",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\nRun a test design on the provided example structure:\npython run.py \\\n    --seed 111 \\\n    --pdb_path \"./inputs/1BC8.pdb\" \\\n    --out_folder \"./outputs/test_output\"\nSuccess indicators:\n\nCommand completes without errors\nOutput folder contains:\n\nseqs/1BC8.fa - Designed sequences in FASTA format\nbackbones/1BC8.pdb - Input backbone (for reference)\npacked/1BC8_1.pdb - Structure with designed side chains\n\n\nExpected runtime: &lt;1 minute on GPU, ~5 minutes on CPU.",
    "crumbs": [
      "Monday",
      "3. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/3-ligandmpnn.html#hpc-job-script",
    "href": "monday/3-ligandmpnn.html#hpc-job-script",
    "title": "3. LigandMPNN",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=ligandmpnn\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.out\n\nmodule load cuda/12.1\n\n# source ~/.bashrc  # Optional: Source shell profile if needed\nmamba activate ligandmpnn_env\n\ncd /path/to/LigandMPNN\n\npython run.py \\\n    --model_type \"ligand_mpnn\" \\\n    --seed 111 \\\n    --pdb_path \"./inputs/my_protein.pdb\" \\\n    --out_folder \"./outputs/my_design\" \\\n    --number_of_batches 10",
    "crumbs": [
      "Monday",
      "3. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/3-ligandmpnn.html#usage-examples",
    "href": "monday/3-ligandmpnn.html#usage-examples",
    "title": "3. LigandMPNN",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic protein design (no ligand):\npython run.py \\\n    --pdb_path \"protein.pdb\" \\\n    --out_folder \"output/\"\nDesign with ligand context:\npython run.py \\\n    --model_type \"ligand_mpnn\" \\\n    --pdb_path \"protein_ligand.pdb\" \\\n    --out_folder \"output/\"\nFix specific residues (keep them unchanged):\npython run.py \\\n    --pdb_path \"protein.pdb\" \\\n    --fixed_residues \"A10 A20 A30\" \\\n    --out_folder \"output/\"\nDesign only specific positions:\npython run.py \\\n    --pdb_path \"protein.pdb\" \\\n    --redesigned_residues \"A50 A51 A52 A53\" \\\n    --out_folder \"output/\"\nBatch processing multiple structures:\n# Create a JSON file listing inputs\necho '{\"1\": \"input1.pdb\", \"2\": \"input2.pdb\"}' &gt; input_list.json\n\npython run.py \\\n    --pdb_path_multi \"input_list.json\" \\\n    --out_folder \"batch_output/\"\nWith temperature control (higher = more diverse):\npython run.py \\\n    --pdb_path \"protein.pdb\" \\\n    --temperature 0.2 \\\n    --out_folder \"output/\"",
    "crumbs": [
      "Monday",
      "3. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/3-ligandmpnn.html#key-parameters",
    "href": "monday/3-ligandmpnn.html#key-parameters",
    "title": "3. LigandMPNN",
    "section": "Key Parameters",
    "text": "Key Parameters\n\n\n\n\n\n\n\n\nParameter\nDescription\nDefault\n\n\n\n\n--model_type\nModel variant: protein_mpnn, ligand_mpnn, soluble_mpnn, etc.\nprotein_mpnn\n\n\n--temperature\nSampling temperature (0.1-1.0). Lower = more conservative\n0.1\n\n\n--number_of_batches\nNumber of sequences to generate\n1\n\n\n--batch_size\nSequences per batch\n1\n\n\n--fixed_residues\nSpace-separated residues to keep unchanged\nNone\n\n\n--redesigned_residues\nOnly design these residues\nAll\n\n\n--bias_AA\nBias toward specific amino acids\nNone",
    "crumbs": [
      "Monday",
      "3. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/3-ligandmpnn.html#model-types",
    "href": "monday/3-ligandmpnn.html#model-types",
    "title": "3. LigandMPNN",
    "section": "Model Types",
    "text": "Model Types\n\n\n\nModel\nUse Case\n\n\n\n\nprotein_mpnn\nStandard protein sequence design\n\n\nligand_mpnn\nDesign with small molecule context\n\n\nsoluble_mpnn\nBias toward soluble sequences\n\n\nglobal_label_membrane_mpnn\nMembrane protein design\n\n\nper_residue_label_membrane_mpnn\nFine-grained membrane design",
    "crumbs": [
      "Monday",
      "3. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/3-ligandmpnn.html#understanding-the-output",
    "href": "monday/3-ligandmpnn.html#understanding-the-output",
    "title": "3. LigandMPNN",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nOutput directory structure:\noutput/\n├── seqs/\n│   └── protein.fa          # Designed sequences\n├── backbones/\n│   └── protein.pdb         # Input structure\n└── packed/\n    ├── protein_1.pdb       # Design 1 with side chains\n    └── protein_2.pdb       # Design 2 with side chains\nFASTA output format:\n&gt;protein, score=1.234, seq_recovery=0.456\nMVKLTAEGSE...\n\nscore: Negative log-likelihood (lower = better fit to backbone)\nseq_recovery: Fraction matching native sequence (if provided)",
    "crumbs": [
      "Monday",
      "3. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/3-ligandmpnn.html#troubleshooting",
    "href": "monday/3-ligandmpnn.html#troubleshooting",
    "title": "3. LigandMPNN",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n“RuntimeError: CUDA out of memory”:\n\nUse CPU instead: remove CUDA module and run without GPU\nReduce --batch_size\nLigandMPNN is efficient; usually not memory-limited\n\nPDB parsing errors:\n\nEnsure PDB has proper formatting\nRemove alternate conformations: keep only “A” conformers\nCheck that ligand has proper atom naming\n\nLigand not recognized:\n\nEnsure ligand is in the PDB file with HETATM records\nUse --ligand flag to specify ligand residue name\nCheck that ligand coordinates are reasonable\n\nLow sequence diversity:\n\nIncrease --temperature (e.g., 0.2 or 0.3)\nIncrease --number_of_batches\nUse different random seeds\n\nSide chain clashes in output:\n\nThis is expected - downstream relaxation is recommended\nUse PyRosetta or Rosetta FastRelax\nOr validate with your structure prediction tool of choice",
    "crumbs": [
      "Monday",
      "3. LigandMPNN"
    ]
  },
  {
    "objectID": "monday/2-localcolabfold.html",
    "href": "monday/2-localcolabfold.html",
    "title": "2. LocalColabFold",
    "section": "",
    "text": "LocalColabFold (code) is a local installation of ColabFold, which provides an efficient implementation of AlphaFold2 protein structure prediction. ColabFold combines fast MSA generation from MMseqs2 with AlphaFold2’s structure prediction capabilities, making it significantly faster than the original AlphaFold2 implementation.",
    "crumbs": [
      "Monday",
      "2. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/2-localcolabfold.html#why-use-localcolabfold",
    "href": "monday/2-localcolabfold.html#why-use-localcolabfold",
    "title": "2. LocalColabFold",
    "section": "Why Use LocalColabFold?",
    "text": "Why Use LocalColabFold?\n\nHigh-throughput predictions: Run batch jobs without Colab time limits\nNo internet dependency: All computations run locally after setup\nHPC integration: Leverage your cluster’s GPUs for faster predictions\nMSA flexibility: Use pre-computed MSAs or generate them on-the-fly\n\nRelated Tools: For structure prediction without MSAs, see ESMFold. For multi-modal complexes, see Chai-1 or Boltz-2.",
    "crumbs": [
      "Monday",
      "2. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/2-localcolabfold.html#resource-requirements",
    "href": "monday/2-localcolabfold.html#resource-requirements",
    "title": "2. LocalColabFold",
    "section": "Resource Requirements",
    "text": "Resource Requirements\n\n\n\nResource\nMinimum\nRecommended\nNotes\n\n\n\n\nGPU RAM\n16 GB\n40+ GB\nA100 recommended for proteins &gt;500 aa\n\n\nCPU RAM\n32 GB\n64 GB\nMSA generation is memory-intensive\n\n\nDisk Space\n15 GB\n100+ GB\nModel weights + optional databases\n\n\nCUDA\n11.1+\n12.1+\nCheck compatibility",
    "crumbs": [
      "Monday",
      "2. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/2-localcolabfold.html#preparation",
    "href": "monday/2-localcolabfold.html#preparation",
    "title": "2. LocalColabFold",
    "section": "Preparation",
    "text": "Preparation\n Mark as complete\n\n\n\n\n\n\nImportantPrerequisites\n\n\n\n\nCompleted HPC Setup guide\nAccess to a GPU node for testing\n~15 GB disk space for installation\n\n\n\n\n\n\n\n\n\nNoteVerify your environment\n\n\n\nnvidia-smi          # Check GPU is available\nnvcc --version      # Check CUDA version",
    "crumbs": [
      "Monday",
      "2. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/2-localcolabfold.html#installation",
    "href": "monday/2-localcolabfold.html#installation",
    "title": "2. LocalColabFold",
    "section": "Installation",
    "text": "Installation\n Mark as complete\n\nDownload the installation script: Navigate to the directory where you want to install LocalColabFold (e.g., your scratch directory or apps folder).\n\nwget https://raw.githubusercontent.com/YoshitakaMo/localcolabfold/main/install_colabbatch_linux.sh\n\nMake the script executable and run it:\n\nchmod +x install_colabbatch_linux.sh\n./install_colabbatch_linux.sh\nThis creates a localcolabfold directory containing: - A conda environment (colabfold_batch) - ColabFold and all dependencies - Model weights (~10-15 GB, downloaded automatically)\nExpected install time: 15-30 minutes depending on network speed.\n\nAdd the environment to your PATH (add to ~/.bashrc for permanent access):\n\nexport PATH=\"/path/to/your/localcolabfold/colabfold-conda/bin:$PATH\"",
    "crumbs": [
      "Monday",
      "2. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/2-localcolabfold.html#testing-the-installation",
    "href": "monday/2-localcolabfold.html#testing-the-installation",
    "title": "2. LocalColabFold",
    "section": "Testing the Installation",
    "text": "Testing the Installation\n Mark as complete\n\nActivate the ColabFold environment:\n\nsource localcolabfold/colabfold-conda/bin/activate\n\nCreate a directory for testing and a test FASTA file:\n\nmkdir -p tests\necho \"&gt;test_protein\nMKFLKFSLLTAVLLSVVFAFSSCGDDDDTYPYDVPDYAGTCGDDDDTYPYDVPDYA\" &gt; tests/test.fasta\n\nRun prediction:\n\ncolabfold_batch tests/test.fasta tests/test_output/\nSuccess indicators:\n\nCommand completes without errors\ntests/test_output/ directory contains:\n\ntest_protein_relaxed_rank_001_*.pdb (predicted structure)\ntest_protein_scores_rank_001_*.json (confidence scores)\ntest_protein_coverage.png (MSA coverage plot)\n\n\nExpected runtime: 2-5 minutes for this small test protein.\nVerify GPU is being used:\n# In another terminal while prediction runs:\nnvidia-smi\n# Look for python process using GPU memory",
    "crumbs": [
      "Monday",
      "2. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/2-localcolabfold.html#hpc-job-script",
    "href": "monday/2-localcolabfold.html#hpc-job-script",
    "title": "2. LocalColabFold",
    "section": "HPC Job Script",
    "text": "HPC Job Script\n#!/bin/bash\n#SBATCH --job-name=colabfold\n#SBATCH --partition=gpu\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=64G\n#SBATCH --time=04:00:00\n#SBATCH --output=%x_%j.out\n\n# Activate environment\nsource /path/to/localcolabfold/colabfold-conda/bin/activate\n\n# Optional: Use shared database location\nexport COLABFOLD_DOWNLOAD_DIR=/shared/colabfold_db\n\n# Run prediction\ncolabfold_batch input.fasta output_dir/",
    "crumbs": [
      "Monday",
      "2. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/2-localcolabfold.html#usage-examples",
    "href": "monday/2-localcolabfold.html#usage-examples",
    "title": "2. LocalColabFold",
    "section": "Usage Examples",
    "text": "Usage Examples\nBasic prediction:\ncolabfold_batch sequences.fasta predictions/\nWith custom MSA server (if your HPC has one):\ncolabfold_batch --msa-server \"https://internal.server.edu\" sequences.fasta predictions/\nMultimer prediction (protein complexes):\n# Separate chains with : in the FASTA file\n# &gt;complex\n# SEQUENCEA:SEQUENCEB\ncolabfold_batch complex.fasta complex_output/\nBatch with templates:\ncolabfold_batch --templates sequences.fasta predictions/\nReduce memory usage (for large proteins):\ncolabfold_batch --amber --num-recycle 3 large_protein.fasta output/",
    "crumbs": [
      "Monday",
      "2. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/2-localcolabfold.html#understanding-the-output",
    "href": "monday/2-localcolabfold.html#understanding-the-output",
    "title": "2. LocalColabFold",
    "section": "Understanding the Output",
    "text": "Understanding the Output\n\n\n\n\n\n\n\nFile\nDescription\n\n\n\n\n*_relaxed_rank_001_*.pdb\nBest predicted structure (Amber-relaxed)\n\n\n*_unrelaxed_rank_001_*.pdb\nBest prediction before relaxation\n\n\n*_scores_rank_001_*.json\npLDDT and pTM scores\n\n\n*_coverage.png\nMSA coverage visualization\n\n\n*_pae.png\nPredicted Aligned Error heatmap\n\n\n\nConfidence scores:\n\npLDDT (per-residue): &gt;90 high confidence, 70-90 confident, 50-70 low, &lt;50 very low\npTM (overall): &gt;0.8 high confidence for whole structure\nPAE (pairwise): Lower is better, indicates domain organization confidence",
    "crumbs": [
      "Monday",
      "2. LocalColabFold"
    ]
  },
  {
    "objectID": "monday/2-localcolabfold.html#troubleshooting",
    "href": "monday/2-localcolabfold.html#troubleshooting",
    "title": "2. LocalColabFold",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n\n\n\n\n\nWarningCommon Issues\n\n\n\n“CUDA out of memory”:\n\nRequest GPU with more memory (#SBATCH --gpus=a100:1)\nUse --amber flag to reduce peak memory\nFor very large proteins (&gt;1000 aa), use Chai-1 or Boltz-2 instead\n\nMSA generation is slow:\n\nUse the MMseqs2 server option for faster MSA generation\nPre-compute MSAs for frequently used sequences\n\nDatabase location filling home directory:\n# Set in ~/.bashrc before running\nexport COLABFOLD_DOWNLOAD_DIR=/scratch/$USER/colabfold_db\nModel weights download fails:\n\nCheck network connectivity\nManually download from: https://storage.googleapis.com/alphafold/\nPlace in ~/.cache/colabfold/params/\n\nGPU not being used (slow prediction):\n# Verify CUDA is detected\npython -c \"import torch; print(torch.cuda.is_available())\"\n# Should print: True",
    "crumbs": [
      "Monday",
      "2. LocalColabFold"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This bootcamp covers machine learning tools for protein structure prediction and design.\nMore information coming soon."
  },
  {
    "objectID": "capstone/targets/beta-glucosidase.html",
    "href": "capstone/targets/beta-glucosidase.html",
    "title": "Beta-Glucosidase Deep Dive",
    "section": "",
    "text": "Beta-Glucosidase is an enzyme that breaks down complex sugars (glucosides) into glucose. It is relevant in both human metabolism (Gaucher’s disease) and industrial biofuel production.\nWhy it matters: In industrial applications, thermostable or stabilized versions of this enzyme are highly valuable. In health, chaperones (binders) that help fold unstable mutants can be therapeutic.\nThe Goal: Design a binder that stabilizes the enzyme without blocking its active site, or conversely, an inhibitor that blocks it.",
    "crumbs": [
      "Capstone",
      "Beta-Glucosidase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/beta-glucosidase.html#biological-context",
    "href": "capstone/targets/beta-glucosidase.html#biological-context",
    "title": "Beta-Glucosidase Deep Dive",
    "section": "",
    "text": "Beta-Glucosidase is an enzyme that breaks down complex sugars (glucosides) into glucose. It is relevant in both human metabolism (Gaucher’s disease) and industrial biofuel production.\nWhy it matters: In industrial applications, thermostable or stabilized versions of this enzyme are highly valuable. In health, chaperones (binders) that help fold unstable mutants can be therapeutic.\nThe Goal: Design a binder that stabilizes the enzyme without blocking its active site, or conversely, an inhibitor that blocks it.",
    "crumbs": [
      "Capstone",
      "Beta-Glucosidase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/beta-glucosidase.html#interactive-structure",
    "href": "capstone/targets/beta-glucosidase.html#interactive-structure",
    "title": "Beta-Glucosidase Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nThe viewer below shows Beta-Glucosidase A.\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "Beta-Glucosidase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/beta-glucosidase.html#design-mission",
    "href": "capstone/targets/beta-glucosidase.html#design-mission",
    "title": "Beta-Glucosidase Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nTarget a surface patch on Beta-Glucosidase.\n\nTarget Specifications\n\n\n\n\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nBeta-Glucosidase\n\n\nPDB ID\n2JIE\n\n\nTarget Chain\nChain A\n\n\nInterface / Hotspot\nSurface patches away from the active site (for stabilization) or the active site (for inhibition).\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 2JIE.\nDefine your Goal: Decide if you want to inhibit or stabilize.\nSelect Hotspot: Choose residues accordingly.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "Beta-Glucosidase Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/ifnar2.html",
    "href": "capstone/targets/ifnar2.html",
    "title": "IFNAR2 Target Deep Dive",
    "section": "",
    "text": "Interferon Alpha/Beta Receptor 2 (IFNAR2) is one of the two receptor subunits that recognize Type I interferons (like IFN-α and IFN-β). These cytokines are powerful antivirals and immune regulators.\nWhy it matters: Modulating this pathway is crucial for treating viral infections (activating it) or autoimmune diseases like Lupus (blocking it).\nThe Goal: Design a protein that binds to the extracellular domain of IFNAR2. Depending on the design goal, this could either compete with natural Interferon (blocking the signal) or potentially act as a synthetic agonist.",
    "crumbs": [
      "Capstone",
      "IFNAR2 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/ifnar2.html#biological-context",
    "href": "capstone/targets/ifnar2.html#biological-context",
    "title": "IFNAR2 Target Deep Dive",
    "section": "",
    "text": "Interferon Alpha/Beta Receptor 2 (IFNAR2) is one of the two receptor subunits that recognize Type I interferons (like IFN-α and IFN-β). These cytokines are powerful antivirals and immune regulators.\nWhy it matters: Modulating this pathway is crucial for treating viral infections (activating it) or autoimmune diseases like Lupus (blocking it).\nThe Goal: Design a protein that binds to the extracellular domain of IFNAR2. Depending on the design goal, this could either compete with natural Interferon (blocking the signal) or potentially act as a synthetic agonist.",
    "crumbs": [
      "Capstone",
      "IFNAR2 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/ifnar2.html#interactive-structure",
    "href": "capstone/targets/ifnar2.html#interactive-structure",
    "title": "IFNAR2 Target Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nThe viewer below shows IFNAR2 (Chain B) in complex with Interferon alpha-2 (Chain A) and IFNAR1 (Chain C).\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "IFNAR2 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/ifnar2.html#design-mission",
    "href": "capstone/targets/ifnar2.html#design-mission",
    "title": "IFNAR2 Target Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nCreate a binder that targets the Interferon-binding interface of IFNAR2.\n\nTarget Specifications\n\n\n\n\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nIFNAR2 (Interferon alpha/beta receptor 2)\n\n\nPDB ID\n3SE3\n\n\nTarget Chain\nChain B (IFNAR2)\n\n\nBinder to Mimic\nChain A (IFN-alpha2)\n\n\nInterface / Hotspot\nResidues interacting with Chain A (approx 40-50, 70-80 regions)\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 3SE3.\nClean the structure: Isolate Chain B (IFNAR2) as your target.\nAnalyze the Interface: Load the full complex in PyMOL. Select Chain B residues that are within 5Å of Chain A to define your hotspot for BindCraft or RFdiffusion.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "IFNAR2 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/bet-v-1.html",
    "href": "capstone/targets/bet-v-1.html",
    "title": "Bet v 1 Target Deep Dive",
    "section": "",
    "text": "Bet v 1 is the major allergen found in birch pollen. It is responsible for seasonal allergies in millions of people worldwide.\nWhy it matters: The allergic reaction is caused when the patient’s antibodies (IgE) recognize and bind to epitopes on the surface of Bet v 1.\nThe Goal: Design a “masking” protein (similar to a neutralizing antibody) that binds tightly to the surface of Bet v 1. If your designed binder covers the IgE binding sites, it could theoretically prevent the allergic reaction.",
    "crumbs": [
      "Capstone",
      "Bet v 1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/bet-v-1.html#biological-context",
    "href": "capstone/targets/bet-v-1.html#biological-context",
    "title": "Bet v 1 Target Deep Dive",
    "section": "",
    "text": "Bet v 1 is the major allergen found in birch pollen. It is responsible for seasonal allergies in millions of people worldwide.\nWhy it matters: The allergic reaction is caused when the patient’s antibodies (IgE) recognize and bind to epitopes on the surface of Bet v 1.\nThe Goal: Design a “masking” protein (similar to a neutralizing antibody) that binds tightly to the surface of Bet v 1. If your designed binder covers the IgE binding sites, it could theoretically prevent the allergic reaction.",
    "crumbs": [
      "Capstone",
      "Bet v 1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/bet-v-1.html#interactive-structure",
    "href": "capstone/targets/bet-v-1.html#interactive-structure",
    "title": "Bet v 1 Target Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nThe viewer below shows the structure of Bet v 1.\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "Bet v 1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/bet-v-1.html#design-mission",
    "href": "capstone/targets/bet-v-1.html#design-mission",
    "title": "Bet v 1 Target Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nCreate a protein that binds to a solvent-exposed patch on Bet v 1. Unlike the other targets, you aren’t necessarily mimicking a natural partner—you are creating a blocker.\n\nTarget Specifications\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nBet v 1\n\n\nPDB ID\n4A88\n\n\nTarget Chain\nChain A\n\n\nInterface / Hotspot\nChoose a large exposed surface patch.\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 4A88.\nSurface Mapping: Since there is no single “natural ligand” to mimic, you have more freedom. Pick a surface patch that looks accessible (convex or flat surfaces are easier than deep pockets for protein-protein interactions).\nPyligner/BindCraft: These tools are excellent for identifying bindable patches on an antigen.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "Bet v 1 Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/il-7r.html",
    "href": "capstone/targets/il-7r.html",
    "title": "IL-7R Target Deep Dive",
    "section": "",
    "text": "Interleukin-7 Receptor alpha (IL-7R) is crucial for the development and survival of T-cells.\nWhy it matters: Dysregulation of IL-7 signaling is implicated in T-cell acute lymphoblastic leukemia (T-ALL) and various autoimmune conditions like Multiple Sclerosis.\nThe Goal: Design a binder that blocks the interaction between IL-7 and IL-7R. A high-affinity binder could serve as a therapeutic antagonist to dampen the immune response in autoimmune settings.",
    "crumbs": [
      "Capstone",
      "IL-7R Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/il-7r.html#biological-context",
    "href": "capstone/targets/il-7r.html#biological-context",
    "title": "IL-7R Target Deep Dive",
    "section": "",
    "text": "Interleukin-7 Receptor alpha (IL-7R) is crucial for the development and survival of T-cells.\nWhy it matters: Dysregulation of IL-7 signaling is implicated in T-cell acute lymphoblastic leukemia (T-ALL) and various autoimmune conditions like Multiple Sclerosis.\nThe Goal: Design a binder that blocks the interaction between IL-7 and IL-7R. A high-affinity binder could serve as a therapeutic antagonist to dampen the immune response in autoimmune settings.",
    "crumbs": [
      "Capstone",
      "IL-7R Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/il-7r.html#interactive-structure",
    "href": "capstone/targets/il-7r.html#interactive-structure",
    "title": "IL-7R Target Deep Dive",
    "section": "Interactive Structure",
    "text": "Interactive Structure\nThe viewer below shows IL-7R alpha (Chain B) bound to its cytokine IL-7 (Chain A).\n\n\n\n\n\n\n\n\n\nTipViewer Controls\n\n\n\n\nRotate: Left-click and drag\nZoom: Scroll wheel\nPan: Right-click (or Ctrl+Left-click) and drag",
    "crumbs": [
      "Capstone",
      "IL-7R Target Deep Dive"
    ]
  },
  {
    "objectID": "capstone/targets/il-7r.html#design-mission",
    "href": "capstone/targets/il-7r.html#design-mission",
    "title": "IL-7R Target Deep Dive",
    "section": "Design Mission",
    "text": "Design Mission\nCreate a binder that occupies the ligand-binding groove of IL-7R, preventing native IL-7 from binding.\n\nTarget Specifications\n\n\n\nFeature\nDetail\n\n\n\n\nTarget Name\nIL-7R alpha\n\n\nPDB ID\n3DI2\n\n\nTarget Chain\nChain B\n\n\nBinder to Mimic\nChain A (IL-7)\n\n\nInterface / Hotspot\nThe groove where Chain A sits.\n\n\n\n\n\nStrategy Tips\n\nDownload PDB 3DI2.\nClean the structure: Keep Chain B.\nHotspot Definition: Use the residues on Chain B that contact Chain A to define the binding site.\n\n\n← Back to Capstone Overview",
    "crumbs": [
      "Capstone",
      "IL-7R Target Deep Dive"
    ]
  },
  {
    "objectID": "thursday/index.html",
    "href": "thursday/index.html",
    "title": "Thursday",
    "section": "",
    "text": "Thursday content coming soon.",
    "crumbs": [
      "Thursday"
    ]
  },
  {
    "objectID": "thursday/index.html#overview",
    "href": "thursday/index.html#overview",
    "title": "Thursday",
    "section": "",
    "text": "Thursday content coming soon.",
    "crumbs": [
      "Thursday"
    ]
  },
  {
    "objectID": "thursday/index.html#modules",
    "href": "thursday/index.html#modules",
    "title": "Thursday",
    "section": "Modules",
    "text": "Modules\n\n\n\nModule\nTopic\nStatus\n\n\n\n\n4.1\nPlaceholder Module\nComing soon\n\n\n\nMore modules will be added here.\n\n\n\n← Wednesday\n\n\nBack to Home\n\n\nCapstone →",
    "crumbs": [
      "Thursday"
    ]
  }
]